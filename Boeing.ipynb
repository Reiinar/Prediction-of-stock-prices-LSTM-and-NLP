{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Boeing\n",
    "Sentiment is built upon the Reuters titles dataset.\n",
    "Historical data is taken from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping historical data from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooFinanceHistory:\n",
    "    timeout = 2\n",
    "    crumb_link = 'https://finance.yahoo.com/quote/{0}/history?p={0}'\n",
    "    crumble_regex = r'CrumbStore\":{\"crumb\":\"(.*?)\"}'\n",
    "    quote_link = 'https://query1.finance.yahoo.com/v7/finance/download/{quote}?period1={dfrom}&period2={dto}&interval=1d&events=history&crumb={crumb}'\n",
    "\n",
    "    def __init__(self, symbol, days_back=7):\n",
    "        self.symbol = symbol\n",
    "        self.session = requests.Session()\n",
    "        self.dt = timedelta(days=days_back)\n",
    "\n",
    "#requesting crumb and cookie\n",
    "    def get_crumb(self):\n",
    "        response = self.session.get(self.crumb_link.format(self.symbol), timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        match = re.search(self.crumble_regex, response.text)\n",
    "        if not match:\n",
    "            raise ValueError('Could not get crumb from Yahoo Finance')\n",
    "        else:\n",
    "            self.crumb = match.group(1)\n",
    "\n",
    "#requesting data\n",
    "    def get_quote(self):\n",
    "        if not hasattr(self, 'crumb') or len(self.session.cookies) == 0:\n",
    "            self.get_crumb()\n",
    "        now = datetime.utcnow()\n",
    "        dateto = int(now.timestamp())\n",
    "        datefrom = int((now - self.dt).timestamp())\n",
    "        url = self.quote_link.format(quote=self.symbol, dfrom=datefrom, dto=dateto, crumb=self.crumb)\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(StringIO(response.text), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data about Boeing from 400 days back\n",
    "df_v = YahooFinanceHistory('BA', days_back=4000).get_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2009-05-15</td>\n",
       "      <td>43.270000</td>\n",
       "      <td>43.990002</td>\n",
       "      <td>42.709999</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>32.646542</td>\n",
       "      <td>5189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>44.529999</td>\n",
       "      <td>43.810001</td>\n",
       "      <td>44.369999</td>\n",
       "      <td>33.686687</td>\n",
       "      <td>7288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>44.360001</td>\n",
       "      <td>45.060001</td>\n",
       "      <td>43.900002</td>\n",
       "      <td>44.619999</td>\n",
       "      <td>33.876472</td>\n",
       "      <td>4792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2009-05-20</td>\n",
       "      <td>45.049999</td>\n",
       "      <td>45.740002</td>\n",
       "      <td>44.540001</td>\n",
       "      <td>44.580002</td>\n",
       "      <td>33.846115</td>\n",
       "      <td>4734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2009-05-21</td>\n",
       "      <td>43.990002</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>42.700001</td>\n",
       "      <td>43.290001</td>\n",
       "      <td>32.866718</td>\n",
       "      <td>5785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>147.210007</td>\n",
       "      <td>151.630005</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>143.610001</td>\n",
       "      <td>143.610001</td>\n",
       "      <td>34983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2751</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>139.009995</td>\n",
       "      <td>141.899994</td>\n",
       "      <td>135.449997</td>\n",
       "      <td>136.330002</td>\n",
       "      <td>136.330002</td>\n",
       "      <td>27043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2752</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>140.419998</td>\n",
       "      <td>140.820007</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>134.970001</td>\n",
       "      <td>134.970001</td>\n",
       "      <td>22842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2753</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>142.380005</td>\n",
       "      <td>136.050003</td>\n",
       "      <td>137.740005</td>\n",
       "      <td>137.740005</td>\n",
       "      <td>25936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2754</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.570007</td>\n",
       "      <td>128.330002</td>\n",
       "      <td>128.979996</td>\n",
       "      <td>128.979996</td>\n",
       "      <td>35884700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-05-15   43.270000   43.990002   42.709999   43.000000   32.646542   \n",
       "1    2009-05-18   44.250000   44.529999   43.810001   44.369999   33.686687   \n",
       "2    2009-05-19   44.360001   45.060001   43.900002   44.619999   33.876472   \n",
       "3    2009-05-20   45.049999   45.740002   44.540001   44.580002   33.846115   \n",
       "4    2009-05-21   43.990002   44.000000   42.700001   43.290001   32.866718   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2750 2020-04-20  147.210007  151.630005  142.500000  143.610001  143.610001   \n",
       "2751 2020-04-21  139.009995  141.899994  135.449997  136.330002  136.330002   \n",
       "2752 2020-04-22  140.419998  140.820007  134.520004  134.970001  134.970001   \n",
       "2753 2020-04-23  136.500000  142.380005  136.050003  137.740005  137.740005   \n",
       "2754 2020-04-24  137.000000  137.570007  128.330002  128.979996  128.979996   \n",
       "\n",
       "        Volume  \n",
       "0      5189300  \n",
       "1      7288000  \n",
       "2      4792100  \n",
       "3      4734200  \n",
       "4      5785000  \n",
       "...        ...  \n",
       "2750  34983800  \n",
       "2751  27043800  \n",
       "2752  22842600  \n",
       "2753  25936400  \n",
       "2754  35884700  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting dates from the latest to earliest\n",
    "df_v.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         datetime64[ns]\n",
       "Open                float64\n",
       "High                float64\n",
       "Low                 float64\n",
       "Close               float64\n",
       "Adj Close           float64\n",
       "Volume                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2009-05-15</td>\n",
       "      <td>43.270000</td>\n",
       "      <td>43.990002</td>\n",
       "      <td>42.709999</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>32.646542</td>\n",
       "      <td>5189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>44.529999</td>\n",
       "      <td>43.810001</td>\n",
       "      <td>44.369999</td>\n",
       "      <td>33.686687</td>\n",
       "      <td>7288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>44.360001</td>\n",
       "      <td>45.060001</td>\n",
       "      <td>43.900002</td>\n",
       "      <td>44.619999</td>\n",
       "      <td>33.876472</td>\n",
       "      <td>4792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2009-05-20</td>\n",
       "      <td>45.049999</td>\n",
       "      <td>45.740002</td>\n",
       "      <td>44.540001</td>\n",
       "      <td>44.580002</td>\n",
       "      <td>33.846115</td>\n",
       "      <td>4734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2009-05-21</td>\n",
       "      <td>43.990002</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>42.700001</td>\n",
       "      <td>43.290001</td>\n",
       "      <td>32.866718</td>\n",
       "      <td>5785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>147.210007</td>\n",
       "      <td>151.630005</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>143.610001</td>\n",
       "      <td>143.610001</td>\n",
       "      <td>34983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2751</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>139.009995</td>\n",
       "      <td>141.899994</td>\n",
       "      <td>135.449997</td>\n",
       "      <td>136.330002</td>\n",
       "      <td>136.330002</td>\n",
       "      <td>27043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2752</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>140.419998</td>\n",
       "      <td>140.820007</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>134.970001</td>\n",
       "      <td>134.970001</td>\n",
       "      <td>22842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2753</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>142.380005</td>\n",
       "      <td>136.050003</td>\n",
       "      <td>137.740005</td>\n",
       "      <td>137.740005</td>\n",
       "      <td>25936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2754</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.570007</td>\n",
       "      <td>128.330002</td>\n",
       "      <td>128.979996</td>\n",
       "      <td>128.979996</td>\n",
       "      <td>35884700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-05-15   43.270000   43.990002   42.709999   43.000000   32.646542   \n",
       "1    2009-05-18   44.250000   44.529999   43.810001   44.369999   33.686687   \n",
       "2    2009-05-19   44.360001   45.060001   43.900002   44.619999   33.876472   \n",
       "3    2009-05-20   45.049999   45.740002   44.540001   44.580002   33.846115   \n",
       "4    2009-05-21   43.990002   44.000000   42.700001   43.290001   32.866718   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2750 2020-04-20  147.210007  151.630005  142.500000  143.610001  143.610001   \n",
       "2751 2020-04-21  139.009995  141.899994  135.449997  136.330002  136.330002   \n",
       "2752 2020-04-22  140.419998  140.820007  134.520004  134.970001  134.970001   \n",
       "2753 2020-04-23  136.500000  142.380005  136.050003  137.740005  137.740005   \n",
       "2754 2020-04-24  137.000000  137.570007  128.330002  128.979996  128.979996   \n",
       "\n",
       "        Volume  \n",
       "0      5189300  \n",
       "1      7288000  \n",
       "2      4792100  \n",
       "3      4734200  \n",
       "4      5785000  \n",
       "...        ...  \n",
       "2750  34983800  \n",
       "2751  27043800  \n",
       "2752  22842600  \n",
       "2753  25936400  \n",
       "2754  35884700  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment for all the articles with \"Microsoft\" in the body of an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading file\n",
    "df2 = pd.read_csv(r'C:\\Users\\Ania Sikora\\Desktop\\Python - zaawansowane\\dfet\\sentymenty\\tytuÅ‚y i daty\\df_BA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>-0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-09</td>\n",
       "      <td>0.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-10</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>1528</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>1529</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>-0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1530</td>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>1531</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>-0.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>1532</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1533 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Date  compound_mean\n",
       "0              0  2011-07-06      -0.493900\n",
       "1              1  2011-07-07       0.074200\n",
       "2              2  2011-07-08       0.084700\n",
       "3              3  2011-07-09       0.153100\n",
       "4              4  2011-07-10       0.055833\n",
       "...          ...         ...            ...\n",
       "1528        1528  2017-02-06       0.055833\n",
       "1529        1529  2017-02-07      -0.273000\n",
       "1530        1530  2017-02-10       0.055833\n",
       "1531        1531  2017-02-13      -0.361200\n",
       "1532        1532  2017-02-14       0.055833\n",
       "\n",
       "[1533 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting column Unnamed\n",
    "df2 = df2.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>-0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-09</td>\n",
       "      <td>0.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-10</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>-0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>-0.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1533 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  compound_mean\n",
       "0     2011-07-06      -0.493900\n",
       "1     2011-07-07       0.074200\n",
       "2     2011-07-08       0.084700\n",
       "3     2011-07-09       0.153100\n",
       "4     2011-07-10       0.055833\n",
       "...          ...            ...\n",
       "1528  2017-02-06       0.055833\n",
       "1529  2017-02-07      -0.273000\n",
       "1530  2017-02-10       0.055833\n",
       "1531  2017-02-13      -0.361200\n",
       "1532  2017-02-14       0.055833\n",
       "\n",
       "[1533 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              object\n",
       "compound_mean    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column Date type to datetime type\n",
    "df2.Date=pd.to_datetime(df2['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging dataframe with historical data with dataframe with sentiments \n",
    "df3 = pd.merge(df_v,df2,on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>74.129997</td>\n",
       "      <td>75.160004</td>\n",
       "      <td>74.010002</td>\n",
       "      <td>74.739998</td>\n",
       "      <td>59.897026</td>\n",
       "      <td>3757800</td>\n",
       "      <td>-0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>75.330002</td>\n",
       "      <td>76.199997</td>\n",
       "      <td>74.849998</td>\n",
       "      <td>75.989998</td>\n",
       "      <td>60.898773</td>\n",
       "      <td>4976900</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>74.570000</td>\n",
       "      <td>75.070000</td>\n",
       "      <td>60.161488</td>\n",
       "      <td>4051200</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>74.169998</td>\n",
       "      <td>74.730003</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>73.349998</td>\n",
       "      <td>58.783066</td>\n",
       "      <td>4379000</td>\n",
       "      <td>-0.157993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>73.620003</td>\n",
       "      <td>73.860001</td>\n",
       "      <td>71.790001</td>\n",
       "      <td>71.930000</td>\n",
       "      <td>57.645065</td>\n",
       "      <td>5773000</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>162.419998</td>\n",
       "      <td>164.080002</td>\n",
       "      <td>162.380005</td>\n",
       "      <td>163.979996</td>\n",
       "      <td>152.032944</td>\n",
       "      <td>3110500</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>167.419998</td>\n",
       "      <td>164.869995</td>\n",
       "      <td>166.500000</td>\n",
       "      <td>154.369339</td>\n",
       "      <td>4243200</td>\n",
       "      <td>-0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>164.470001</td>\n",
       "      <td>166.229996</td>\n",
       "      <td>155.444748</td>\n",
       "      <td>2689700</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>169.070007</td>\n",
       "      <td>166.350006</td>\n",
       "      <td>168.029999</td>\n",
       "      <td>157.127945</td>\n",
       "      <td>3765000</td>\n",
       "      <td>-0.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>167.699997</td>\n",
       "      <td>168.800003</td>\n",
       "      <td>167.220001</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>157.567429</td>\n",
       "      <td>2580300</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1277 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2011-07-06   74.129997   75.160004   74.010002   74.739998   59.897026   \n",
       "1    2011-07-07   75.330002   76.199997   74.849998   75.989998   60.898773   \n",
       "2    2011-07-08   75.580002   75.580002   74.570000   75.070000   60.161488   \n",
       "3    2011-07-11   74.169998   74.730003   73.000000   73.349998   58.783066   \n",
       "4    2011-07-12   73.620003   73.860001   71.790001   71.930000   57.645065   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1272 2017-02-06  162.419998  164.080002  162.380005  163.979996  152.032944   \n",
       "1273 2017-02-07  165.000000  167.419998  164.869995  166.500000  154.369339   \n",
       "1274 2017-02-10  165.250000  166.449997  164.470001  166.229996  155.444748   \n",
       "1275 2017-02-13  166.449997  169.070007  166.350006  168.029999  157.127945   \n",
       "1276 2017-02-14  167.699997  168.800003  167.220001  168.500000  157.567429   \n",
       "\n",
       "       Volume  compound_mean  \n",
       "0     3757800      -0.493900  \n",
       "1     4976900       0.074200  \n",
       "2     4051200       0.084700  \n",
       "3     4379000      -0.157993  \n",
       "4     5773000       0.360000  \n",
       "...       ...            ...  \n",
       "1272  3110500       0.055833  \n",
       "1273  4243200      -0.273000  \n",
       "1274  2689700       0.055833  \n",
       "1275  3765000      -0.361200  \n",
       "1276  2580300       0.055833  \n",
       "\n",
       "[1277 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for prediction of label for the next day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepcopying dataframe, so there would be no need to run everything from the beggining\n",
    "df = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ania Sikora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\Ania Sikora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1277",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c0e93022ea5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1277"
     ]
    }
   ],
   "source": [
    "#label indicates wheter the price will go up(1) or down(0) next day\n",
    "m = 1\n",
    "for i in df.Close:\n",
    "    if i < df.Close[m]:\n",
    "        df['Label'][m-1] = 1\n",
    "    else:\n",
    "        df['Label'][m-1] = 0\n",
    "    m = m+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>74.129997</td>\n",
       "      <td>75.160004</td>\n",
       "      <td>74.010002</td>\n",
       "      <td>74.739998</td>\n",
       "      <td>59.897026</td>\n",
       "      <td>3757800</td>\n",
       "      <td>-0.493900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>75.330002</td>\n",
       "      <td>76.199997</td>\n",
       "      <td>74.849998</td>\n",
       "      <td>75.989998</td>\n",
       "      <td>60.898773</td>\n",
       "      <td>4976900</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>74.570000</td>\n",
       "      <td>75.070000</td>\n",
       "      <td>60.161488</td>\n",
       "      <td>4051200</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>74.169998</td>\n",
       "      <td>74.730003</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>73.349998</td>\n",
       "      <td>58.783066</td>\n",
       "      <td>4379000</td>\n",
       "      <td>-0.157993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>73.620003</td>\n",
       "      <td>73.860001</td>\n",
       "      <td>71.790001</td>\n",
       "      <td>71.930000</td>\n",
       "      <td>57.645065</td>\n",
       "      <td>5773000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>162.419998</td>\n",
       "      <td>164.080002</td>\n",
       "      <td>162.380005</td>\n",
       "      <td>163.979996</td>\n",
       "      <td>152.032944</td>\n",
       "      <td>3110500</td>\n",
       "      <td>0.055833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>167.419998</td>\n",
       "      <td>164.869995</td>\n",
       "      <td>166.500000</td>\n",
       "      <td>154.369339</td>\n",
       "      <td>4243200</td>\n",
       "      <td>-0.273000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>164.470001</td>\n",
       "      <td>166.229996</td>\n",
       "      <td>155.444748</td>\n",
       "      <td>2689700</td>\n",
       "      <td>0.055833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>169.070007</td>\n",
       "      <td>166.350006</td>\n",
       "      <td>168.029999</td>\n",
       "      <td>157.127945</td>\n",
       "      <td>3765000</td>\n",
       "      <td>-0.361200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>167.699997</td>\n",
       "      <td>168.800003</td>\n",
       "      <td>167.220001</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>157.567429</td>\n",
       "      <td>2580300</td>\n",
       "      <td>0.055833</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1277 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2011-07-06   74.129997   75.160004   74.010002   74.739998   59.897026   \n",
       "1    2011-07-07   75.330002   76.199997   74.849998   75.989998   60.898773   \n",
       "2    2011-07-08   75.580002   75.580002   74.570000   75.070000   60.161488   \n",
       "3    2011-07-11   74.169998   74.730003   73.000000   73.349998   58.783066   \n",
       "4    2011-07-12   73.620003   73.860001   71.790001   71.930000   57.645065   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1272 2017-02-06  162.419998  164.080002  162.380005  163.979996  152.032944   \n",
       "1273 2017-02-07  165.000000  167.419998  164.869995  166.500000  154.369339   \n",
       "1274 2017-02-10  165.250000  166.449997  164.470001  166.229996  155.444748   \n",
       "1275 2017-02-13  166.449997  169.070007  166.350006  168.029999  157.127945   \n",
       "1276 2017-02-14  167.699997  168.800003  167.220001  168.500000  157.567429   \n",
       "\n",
       "       Volume  compound_mean Label  \n",
       "0     3757800      -0.493900     1  \n",
       "1     4976900       0.074200     0  \n",
       "2     4051200       0.084700     0  \n",
       "3     4379000      -0.157993     0  \n",
       "4     5773000       0.360000     1  \n",
       "...       ...            ...   ...  \n",
       "1272  3110500       0.055833     1  \n",
       "1273  4243200      -0.273000     0  \n",
       "1274  2689700       0.055833     1  \n",
       "1275  3765000      -0.361200     1  \n",
       "1276  2580300       0.055833        \n",
       "\n",
       "[1277 rows x 9 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the index\n",
    "df.drop(index = 1276, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             datetime64[ns]\n",
       "Open                    float64\n",
       "High                    float64\n",
       "Low                     float64\n",
       "Close                   float64\n",
       "Adj Close               float64\n",
       "Volume                    int64\n",
       "compound_mean           float64\n",
       "Label                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting column Label to numeric\n",
    "df.Label=pd.to_numeric(df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             datetime64[ns]\n",
       "Open                    float64\n",
       "High                    float64\n",
       "Low                     float64\n",
       "Close                   float64\n",
       "Adj Close               float64\n",
       "Volume                    int64\n",
       "compound_mean           float64\n",
       "Label                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training and testing datasets\n",
    "X = array[:,1:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardising features, fitting and transforming X\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting Y to data type integer\n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15129418 0.14381154 0.16181233 0.15513382 0.12252169 0.06436869\n",
      " 0.1636901 ]\n",
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'compound_mean'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(df.columns[1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05  0.05  0.047 0.046 0.017 0.085 0.235]\n",
      "[[0.151 0.144 0.162 0.064 0.164]\n",
      " [0.162 0.153 0.169 0.092 0.521]\n",
      " [0.165 0.148 0.167 0.071 0.528]\n",
      " [0.152 0.14  0.153 0.079 0.375]\n",
      " [0.147 0.132 0.142 0.111 0.701]]\n"
     ]
    }
   ],
   "source": [
    "#choosing best features for the model\n",
    "test = SelectKBest(score_func=chi2, k=5)\n",
    "fit = test.fit(X, Y)\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.151, 0.144, 0.162, 0.064, 0.164],\n",
       "       [0.162, 0.153, 0.169, 0.092, 0.521],\n",
       "       [0.165, 0.148, 0.167, 0.071, 0.528],\n",
       "       ...,\n",
       "       [0.979, 0.977, 0.979, 0.076, 0.303],\n",
       "       [0.982, 0.968, 0.975, 0.04 , 0.509],\n",
       "       [0.993, 0.992, 0.992, 0.065, 0.247]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open, High, Low, Volume and compound mean give the most information\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 957 samples, validate on 319 samples\n",
      "Epoch 1/100\n",
      "957/957 [==============================] - ETA: 1:03 - loss: 0.6897 - accuracy: 0.46 - ETA: 32s - loss: 0.6769 - accuracy: 0.6094 - ETA: 15s - loss: 0.6847 - accuracy: 0.578 - ETA: 4s - loss: 0.6923 - accuracy: 0.521 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6949 - accuracy: 0.52 - ETA: 0s - loss: 0.6990 - accuracy: 0.53 - 3s 3ms/step - loss: 0.6977 - accuracy: 0.5298 - val_loss: 0.6936 - val_accuracy: 0.5047\n",
      "Epoch 2/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7032 - accuracy: 0.37 - ETA: 0s - loss: 0.7043 - accuracy: 0.44 - ETA: 0s - loss: 0.6987 - accuracy: 0.46 - ETA: 0s - loss: 0.6959 - accuracy: 0.47 - ETA: 0s - loss: 0.6953 - accuracy: 0.48 - ETA: 0s - loss: 0.6958 - accuracy: 0.49 - 0s 353us/step - loss: 0.6957 - accuracy: 0.4932 - val_loss: 0.6956 - val_accuracy: 0.5047\n",
      "Epoch 3/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.37 - ETA: 1s - loss: 0.7027 - accuracy: 0.45 - ETA: 1s - loss: 0.6959 - accuracy: 0.49 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 1s 524us/step - loss: 0.6921 - accuracy: 0.5319 - val_loss: 0.6951 - val_accuracy: 0.5047\n",
      "Epoch 4/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.43 - ETA: 0s - loss: 0.6991 - accuracy: 0.50 - ETA: 0s - loss: 0.6969 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - 0s 341us/step - loss: 0.6928 - accuracy: 0.5371 - val_loss: 0.6956 - val_accuracy: 0.5047\n",
      "Epoch 5/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.40 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6909 - accuracy: 0.55 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6940 - accuracy: 0.53 - ETA: 0s - loss: 0.6932 - accuracy: 0.54 - 0s 374us/step - loss: 0.6923 - accuracy: 0.5486 - val_loss: 0.6960 - val_accuracy: 0.5047\n",
      "Epoch 6/100\n",
      "957/957 [==============================] - ETA: 2s - loss: 0.6852 - accuracy: 0.53 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6893 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - 0s 470us/step - loss: 0.6892 - accuracy: 0.5329 - val_loss: 0.6963 - val_accuracy: 0.5047\n",
      "Epoch 7/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7038 - accuracy: 0.50 - ETA: 0s - loss: 0.6962 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - 0s 388us/step - loss: 0.6905 - accuracy: 0.5486 - val_loss: 0.6970 - val_accuracy: 0.5047\n",
      "Epoch 8/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - 1s 547us/step - loss: 0.6910 - accuracy: 0.5465 - val_loss: 0.6965 - val_accuracy: 0.5047\n",
      "Epoch 9/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - 0s 370us/step - loss: 0.6868 - accuracy: 0.5465 - val_loss: 0.6955 - val_accuracy: 0.4796\n",
      "Epoch 10/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.43 - ETA: 0s - loss: 0.6996 - accuracy: 0.47 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - 0s 359us/step - loss: 0.6922 - accuracy: 0.5340 - val_loss: 0.7010 - val_accuracy: 0.5047\n",
      "Epoch 11/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.53 - ETA: 0s - loss: 0.6980 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - 1s 581us/step - loss: 0.6914 - accuracy: 0.5381 - val_loss: 0.6979 - val_accuracy: 0.5047\n",
      "Epoch 12/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.62 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - 0s 377us/step - loss: 0.6922 - accuracy: 0.5434 - val_loss: 0.6981 - val_accuracy: 0.5047\n",
      "Epoch 13/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.62 - ETA: 0s - loss: 0.6959 - accuracy: 0.50 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - 0s 411us/step - loss: 0.6908 - accuracy: 0.5444 - val_loss: 0.6951 - val_accuracy: 0.5047\n",
      "Epoch 14/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.59 - ETA: 1s - loss: 0.6833 - accuracy: 0.64 - ETA: 1s - loss: 0.6816 - accuracy: 0.65 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6932 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - 1s 563us/step - loss: 0.6891 - accuracy: 0.5507 - val_loss: 0.6961 - val_accuracy: 0.5047\n",
      "Epoch 15/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6743 - accuracy: 0.68 - ETA: 0s - loss: 0.6954 - accuracy: 0.53 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6905 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - 0s 415us/step - loss: 0.6902 - accuracy: 0.5465 - val_loss: 0.6975 - val_accuracy: 0.5047\n",
      "Epoch 16/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - 1s 560us/step - loss: 0.6920 - accuracy: 0.5235 - val_loss: 0.6964 - val_accuracy: 0.5047\n",
      "Epoch 17/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.40 - ETA: 0s - loss: 0.6980 - accuracy: 0.52 - ETA: 0s - loss: 0.6972 - accuracy: 0.51 - ETA: 0s - loss: 0.6971 - accuracy: 0.52 - ETA: 0s - loss: 0.6954 - accuracy: 0.53 - ETA: 0s - loss: 0.6960 - accuracy: 0.52 - ETA: 0s - loss: 0.6952 - accuracy: 0.52 - 0s 435us/step - loss: 0.6948 - accuracy: 0.5246 - val_loss: 0.6974 - val_accuracy: 0.5047\n",
      "Epoch 18/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.53 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - 0s 428us/step - loss: 0.6899 - accuracy: 0.5392 - val_loss: 0.6965 - val_accuracy: 0.5047\n",
      "Epoch 19/100\n",
      "957/957 [==============================] - ETA: 2s - loss: 0.6833 - accuracy: 0.59 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - 1s 571us/step - loss: 0.6914 - accuracy: 0.5350 - val_loss: 0.6954 - val_accuracy: 0.5047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.34 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - 0s 406us/step - loss: 0.6900 - accuracy: 0.5392 - val_loss: 0.6964 - val_accuracy: 0.5047\n",
      "Epoch 21/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.46 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6955 - accuracy: 0.52 - ETA: 0s - loss: 0.6966 - accuracy: 0.51 - ETA: 0s - loss: 0.6951 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - 1s 686us/step - loss: 0.6901 - accuracy: 0.5444 - val_loss: 0.6968 - val_accuracy: 0.5047\n",
      "Epoch 22/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7046 - accuracy: 0.46 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.55 - 1s 539us/step - loss: 0.6891 - accuracy: 0.5434 - val_loss: 0.6983 - val_accuracy: 0.5047\n",
      "Epoch 23/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.43 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - 1s 657us/step - loss: 0.6907 - accuracy: 0.5246 - val_loss: 0.6978 - val_accuracy: 0.5047\n",
      "Epoch 24/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.59 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - 1s 620us/step - loss: 0.6911 - accuracy: 0.5413 - val_loss: 0.6968 - val_accuracy: 0.5047\n",
      "Epoch 25/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.46 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - 1s 753us/step - loss: 0.6878 - accuracy: 0.5455 - val_loss: 0.6978 - val_accuracy: 0.4984\n",
      "Epoch 26/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - 1s 553us/step - loss: 0.6936 - accuracy: 0.5277 - val_loss: 0.7005 - val_accuracy: 0.5047\n",
      "Epoch 27/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7129 - accuracy: 0.40 - ETA: 0s - loss: 0.6778 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - 1s 741us/step - loss: 0.6874 - accuracy: 0.5413 - val_loss: 0.6986 - val_accuracy: 0.5047\n",
      "Epoch 28/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.71 - ETA: 0s - loss: 0.6890 - accuracy: 0.55 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - 0s 522us/step - loss: 0.6901 - accuracy: 0.5350 - val_loss: 0.6988 - val_accuracy: 0.5047\n",
      "Epoch 29/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - 1s 675us/step - loss: 0.6859 - accuracy: 0.5569 - val_loss: 0.6974 - val_accuracy: 0.4984\n",
      "Epoch 30/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.46 - ETA: 0s - loss: 0.7020 - accuracy: 0.50 - ETA: 0s - loss: 0.6960 - accuracy: 0.54 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - 0s 496us/step - loss: 0.6884 - accuracy: 0.5361 - val_loss: 0.7003 - val_accuracy: 0.5047\n",
      "Epoch 31/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.46 - ETA: 0s - loss: 0.6778 - accuracy: 0.59 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6956 - accuracy: 0.52 - ETA: 0s - loss: 0.6959 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - 1s 717us/step - loss: 0.6931 - accuracy: 0.5225 - val_loss: 0.6992 - val_accuracy: 0.5047\n",
      "Epoch 32/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - 0s 485us/step - loss: 0.6893 - accuracy: 0.5423 - val_loss: 0.6993 - val_accuracy: 0.5047\n",
      "Epoch 33/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.53 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - 1s 699us/step - loss: 0.6891 - accuracy: 0.5423 - val_loss: 0.6964 - val_accuracy: 0.5047\n",
      "Epoch 34/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7104 - accuracy: 0.43 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - 0s 463us/step - loss: 0.6907 - accuracy: 0.5361 - val_loss: 0.6996 - val_accuracy: 0.5047\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957/957 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.37 - ETA: 0s - loss: 0.6992 - accuracy: 0.50 - ETA: 0s - loss: 0.6911 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - 1s 725us/step - loss: 0.6896 - accuracy: 0.5381 - val_loss: 0.6969 - val_accuracy: 0.4984\n",
      "Epoch 36/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6806 - accuracy: 0.59 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - 1s 568us/step - loss: 0.6875 - accuracy: 0.5402 - val_loss: 0.6994 - val_accuracy: 0.4984\n",
      "Epoch 37/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.53 - ETA: 0s - loss: 0.6985 - accuracy: 0.55 - ETA: 0s - loss: 0.6967 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.56 - ETA: 0s - loss: 0.6944 - accuracy: 0.54 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - 1s 792us/step - loss: 0.6889 - accuracy: 0.5475 - val_loss: 0.6994 - val_accuracy: 0.5016\n",
      "Epoch 38/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.59 - ETA: 0s - loss: 0.7016 - accuracy: 0.50 - ETA: 0s - loss: 0.6981 - accuracy: 0.50 - ETA: 0s - loss: 0.6971 - accuracy: 0.50 - ETA: 0s - loss: 0.6975 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - 0s 517us/step - loss: 0.6911 - accuracy: 0.5371 - val_loss: 0.6999 - val_accuracy: 0.5047\n",
      "Epoch 39/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.56 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - 1s 712us/step - loss: 0.6896 - accuracy: 0.5423 - val_loss: 0.6977 - val_accuracy: 0.5016\n",
      "Epoch 40/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.46 - ETA: 0s - loss: 0.6957 - accuracy: 0.55 - ETA: 0s - loss: 0.6988 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - 0s 510us/step - loss: 0.6921 - accuracy: 0.5350 - val_loss: 0.6974 - val_accuracy: 0.5047\n",
      "Epoch 41/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.59 - ETA: 0s - loss: 0.7039 - accuracy: 0.48 - ETA: 0s - loss: 0.6954 - accuracy: 0.52 - ETA: 0s - loss: 0.6947 - accuracy: 0.52 - ETA: 0s - loss: 0.6956 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - 1s 618us/step - loss: 0.6887 - accuracy: 0.5549 - val_loss: 0.6972 - val_accuracy: 0.5047\n",
      "Epoch 42/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.50 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - 0s 394us/step - loss: 0.6884 - accuracy: 0.5517 - val_loss: 0.6982 - val_accuracy: 0.5016\n",
      "Epoch 43/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.56 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - 0s 386us/step - loss: 0.6890 - accuracy: 0.5340 - val_loss: 0.6974 - val_accuracy: 0.5047\n",
      "Epoch 44/100\n",
      "957/957 [==============================] - ETA: 2s - loss: 0.6655 - accuracy: 0.65 - ETA: 1s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - 0s 517us/step - loss: 0.6878 - accuracy: 0.5580 - val_loss: 0.6963 - val_accuracy: 0.4922\n",
      "Epoch 45/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.53 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - 0s 404us/step - loss: 0.6900 - accuracy: 0.5319 - val_loss: 0.7012 - val_accuracy: 0.5016\n",
      "Epoch 46/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6638 - accuracy: 0.68 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - 1s 600us/step - loss: 0.6871 - accuracy: 0.5465 - val_loss: 0.6968 - val_accuracy: 0.4922\n",
      "Epoch 47/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.59 - ETA: 0s - loss: 0.6834 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - 0s 445us/step - loss: 0.6843 - accuracy: 0.5423 - val_loss: 0.7004 - val_accuracy: 0.5047\n",
      "Epoch 48/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.43 - ETA: 0s - loss: 0.6975 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.57 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - 0s 424us/step - loss: 0.6888 - accuracy: 0.5611 - val_loss: 0.6986 - val_accuracy: 0.4859\n",
      "Epoch 49/100\n",
      "957/957 [==============================] - ETA: 1s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6933 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - 0s 490us/step - loss: 0.6906 - accuracy: 0.5319 - val_loss: 0.6977 - val_accuracy: 0.4953\n",
      "Epoch 50/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.62 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.54 - 0s 336us/step - loss: 0.6853 - accuracy: 0.5507 - val_loss: 0.6978 - val_accuracy: 0.4984\n",
      "Epoch 51/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.62 - ETA: 0s - loss: 0.6976 - accuracy: 0.51 - ETA: 0s - loss: 0.6965 - accuracy: 0.53 - ETA: 0s - loss: 0.6965 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - 1s 564us/step - loss: 0.6892 - accuracy: 0.5664 - val_loss: 0.6978 - val_accuracy: 0.4953\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957/957 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - 0s 379us/step - loss: 0.6879 - accuracy: 0.5381 - val_loss: 0.6985 - val_accuracy: 0.4953\n",
      "Epoch 53/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.46 - ETA: 0s - loss: 0.6987 - accuracy: 0.48 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6953 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - 0s 420us/step - loss: 0.6890 - accuracy: 0.5434 - val_loss: 0.6999 - val_accuracy: 0.4890\n",
      "Epoch 54/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.46 - ETA: 0s - loss: 0.6962 - accuracy: 0.50 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - 1s 556us/step - loss: 0.6868 - accuracy: 0.5517 - val_loss: 0.7018 - val_accuracy: 0.5110\n",
      "Epoch 55/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.68 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - 0s 406us/step - loss: 0.6883 - accuracy: 0.5371 - val_loss: 0.6974 - val_accuracy: 0.5110\n",
      "Epoch 56/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.34 - ETA: 0s - loss: 0.6949 - accuracy: 0.45 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6910 - accuracy: 0.50 - ETA: 0s - loss: 0.6903 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.51 - 0s 374us/step - loss: 0.6901 - accuracy: 0.5193 - val_loss: 0.6976 - val_accuracy: 0.5016\n",
      "Epoch 57/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.62 - ETA: 1s - loss: 0.6900 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.60 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - 1s 552us/step - loss: 0.6883 - accuracy: 0.5559 - val_loss: 0.6967 - val_accuracy: 0.4953\n",
      "Epoch 58/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.68 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - 0s 431us/step - loss: 0.6901 - accuracy: 0.5381 - val_loss: 0.6960 - val_accuracy: 0.4953\n",
      "Epoch 59/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6732 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - 1s 569us/step - loss: 0.6871 - accuracy: 0.5486 - val_loss: 0.6967 - val_accuracy: 0.4922\n",
      "Epoch 60/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.50 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - 0s 356us/step - loss: 0.6860 - accuracy: 0.5517 - val_loss: 0.6983 - val_accuracy: 0.4922\n",
      "Epoch 61/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.62 - ETA: 0s - loss: 0.6737 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - 0s 360us/step - loss: 0.6924 - accuracy: 0.5319 - val_loss: 0.6978 - val_accuracy: 0.4953\n",
      "Epoch 62/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - 1s 549us/step - loss: 0.6866 - accuracy: 0.5455 - val_loss: 0.6981 - val_accuracy: 0.4953\n",
      "Epoch 63/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.56 - ETA: 0s - loss: 0.7007 - accuracy: 0.48 - ETA: 0s - loss: 0.6965 - accuracy: 0.51 - ETA: 0s - loss: 0.6952 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 0s 436us/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6955 - val_accuracy: 0.5047\n",
      "Epoch 64/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6847 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - 0s 348us/step - loss: 0.6881 - accuracy: 0.5329 - val_loss: 0.6981 - val_accuracy: 0.5047\n",
      "Epoch 65/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.46 - ETA: 0s - loss: 0.6977 - accuracy: 0.50 - ETA: 0s - loss: 0.6957 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - 0s 509us/step - loss: 0.6876 - accuracy: 0.5423 - val_loss: 0.6976 - val_accuracy: 0.4984\n",
      "Epoch 66/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.56 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.62 - ETA: 0s - loss: 0.6812 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - 0s 449us/step - loss: 0.6847 - accuracy: 0.5747 - val_loss: 0.6985 - val_accuracy: 0.5266\n",
      "Epoch 67/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7038 - accuracy: 0.43 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - 1s 541us/step - loss: 0.6892 - accuracy: 0.5340 - val_loss: 0.6985 - val_accuracy: 0.4953\n",
      "Epoch 68/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.62 - ETA: 0s - loss: 0.6945 - accuracy: 0.54 - ETA: 0s - loss: 0.6931 - accuracy: 0.54 - ETA: 0s - loss: 0.6943 - accuracy: 0.53 - ETA: 0s - loss: 0.6945 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - 0s 369us/step - loss: 0.6914 - accuracy: 0.5392 - val_loss: 0.6959 - val_accuracy: 0.4890\n",
      "Epoch 69/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.43 - ETA: 0s - loss: 0.6925 - accuracy: 0.48 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - 0s 401us/step - loss: 0.6881 - accuracy: 0.5465 - val_loss: 0.6979 - val_accuracy: 0.5016\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957/957 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.43 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6817 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - 1s 651us/step - loss: 0.6892 - accuracy: 0.5392 - val_loss: 0.6982 - val_accuracy: 0.4984\n",
      "Epoch 71/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.56 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6928 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - 1s 562us/step - loss: 0.6884 - accuracy: 0.5496 - val_loss: 0.6969 - val_accuracy: 0.4922\n",
      "Epoch 72/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - 1s 742us/step - loss: 0.6894 - accuracy: 0.5423 - val_loss: 0.6970 - val_accuracy: 0.4922\n",
      "Epoch 73/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.53 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - 1s 710us/step - loss: 0.6845 - accuracy: 0.5601 - val_loss: 0.6986 - val_accuracy: 0.4984\n",
      "Epoch 74/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.50 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6806 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - 1s 805us/step - loss: 0.6884 - accuracy: 0.5361 - val_loss: 0.6993 - val_accuracy: 0.4922\n",
      "Epoch 75/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.46 - ETA: 0s - loss: 0.6857 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - 1s 858us/step - loss: 0.6871 - accuracy: 0.5517 - val_loss: 0.6991 - val_accuracy: 0.4984\n",
      "Epoch 76/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.78 - ETA: 0s - loss: 0.6614 - accuracy: 0.65 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - 1s 533us/step - loss: 0.6850 - accuracy: 0.5549 - val_loss: 0.6985 - val_accuracy: 0.4922\n",
      "Epoch 77/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.43 - ETA: 0s - loss: 0.6789 - accuracy: 0.54 - ETA: 0s - loss: 0.6781 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.51 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - 1s 591us/step - loss: 0.6872 - accuracy: 0.5434 - val_loss: 0.6999 - val_accuracy: 0.4984\n",
      "Epoch 78/100\n",
      "957/957 [==============================] - ETA: 1s - loss: 0.6945 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - 1s 638us/step - loss: 0.6871 - accuracy: 0.5486 - val_loss: 0.6983 - val_accuracy: 0.5172\n",
      "Epoch 79/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.50 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - 1s 932us/step - loss: 0.6888 - accuracy: 0.5371 - val_loss: 0.6991 - val_accuracy: 0.4922\n",
      "Epoch 80/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7334 - accuracy: 0.43 - ETA: 0s - loss: 0.6974 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - 1s 693us/step - loss: 0.6890 - accuracy: 0.5319 - val_loss: 0.6958 - val_accuracy: 0.5078\n",
      "Epoch 81/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 1s - loss: 0.6709 - accuracy: 0.60 - ETA: 1s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - 1s 823us/step - loss: 0.6886 - accuracy: 0.5350 - val_loss: 0.6956 - val_accuracy: 0.5078\n",
      "Epoch 82/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.50 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - 1s 997us/step - loss: 0.6889 - accuracy: 0.5350 - val_loss: 0.6949 - val_accuracy: 0.5266\n",
      "Epoch 83/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - 1s 556us/step - loss: 0.6832 - accuracy: 0.5580 - val_loss: 0.7014 - val_accuracy: 0.4953\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957/957 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.46 - ETA: 0s - loss: 0.6784 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.53 - ETA: 0s - loss: 0.6812 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.50 - ETA: 0s - loss: 0.6893 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - 1s 892us/step - loss: 0.6884 - accuracy: 0.5496 - val_loss: 0.6967 - val_accuracy: 0.4953\n",
      "Epoch 85/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.71 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - 1s 794us/step - loss: 0.6857 - accuracy: 0.5455 - val_loss: 0.7020 - val_accuracy: 0.4922\n",
      "Epoch 86/100\n",
      "957/957 [==============================] - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6772 - accuracy: 0.57 - ETA: 0s - loss: 0.6876 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - 1s 843us/step - loss: 0.6903 - accuracy: 0.5465 - val_loss: 0.6961 - val_accuracy: 0.5141\n",
      "Epoch 87/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6439 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.53 - ETA: 0s - loss: 0.6776 - accuracy: 0.55 - ETA: 0s - loss: 0.6768 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.53 - ETA: 0s - loss: 0.6807 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - 1s 919us/step - loss: 0.6855 - accuracy: 0.5319 - val_loss: 0.6977 - val_accuracy: 0.4953\n",
      "Epoch 88/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6826 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.59 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6868 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - 1s 1ms/step - loss: 0.6890 - accuracy: 0.5413 - val_loss: 0.6970 - val_accuracy: 0.5078\n",
      "Epoch 89/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.59 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - 1s 631us/step - loss: 0.6851 - accuracy: 0.5538 - val_loss: 0.6985 - val_accuracy: 0.4953\n",
      "Epoch 90/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6955 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - 1s 891us/step - loss: 0.6832 - accuracy: 0.5559 - val_loss: 0.6998 - val_accuracy: 0.4984\n",
      "Epoch 91/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.59 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6905 - accuracy: 0.55 - ETA: 0s - loss: 0.6918 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - 1s 1ms/step - loss: 0.6908 - accuracy: 0.5580 - val_loss: 0.6970 - val_accuracy: 0.5016\n",
      "Epoch 92/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - 1s 591us/step - loss: 0.6871 - accuracy: 0.5507 - val_loss: 0.6976 - val_accuracy: 0.4953\n",
      "Epoch 93/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - 1s 811us/step - loss: 0.6883 - accuracy: 0.5423 - val_loss: 0.6963 - val_accuracy: 0.4984\n",
      "Epoch 94/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - 1s 639us/step - loss: 0.6885 - accuracy: 0.5475 - val_loss: 0.6970 - val_accuracy: 0.4953\n",
      "Epoch 95/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.37 - ETA: 0s - loss: 0.7020 - accuracy: 0.46 - ETA: 0s - loss: 0.6838 - accuracy: 0.53 - ETA: 1s - loss: 0.6971 - accuracy: 0.49 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - 1s 857us/step - loss: 0.6858 - accuracy: 0.5528 - val_loss: 0.6969 - val_accuracy: 0.5016\n",
      "Epoch 96/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.50 - ETA: 0s - loss: 0.6901 - accuracy: 0.51 - ETA: 0s - loss: 0.7000 - accuracy: 0.46 - ETA: 0s - loss: 0.6963 - accuracy: 0.48 - ETA: 0s - loss: 0.6912 - accuracy: 0.50 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - 1s 607us/step - loss: 0.6868 - accuracy: 0.5444 - val_loss: 0.6984 - val_accuracy: 0.4953\n",
      "Epoch 97/100\n",
      "957/957 [==============================] - ETA: 3s - loss: 0.6817 - accuracy: 0.56 - ETA: 3s - loss: 0.6760 - accuracy: 0.62 - ETA: 1s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - 1s 647us/step - loss: 0.6882 - accuracy: 0.5371 - val_loss: 0.6990 - val_accuracy: 0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - 0s 490us/step - loss: 0.6872 - accuracy: 0.5381 - val_loss: 0.6966 - val_accuracy: 0.4922\n",
      "Epoch 99/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.40 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6819 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - 1s 684us/step - loss: 0.6885 - accuracy: 0.5371 - val_loss: 0.6975 - val_accuracy: 0.4859\n",
      "Epoch 100/100\n",
      "957/957 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.46 - ETA: 0s - loss: 0.6808 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.52 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - 0s 481us/step - loss: 0.6851 - accuracy: 0.5434 - val_loss: 0.6955 - val_accuracy: 0.4953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#building and training a model\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, Y, test_size=0.25)\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(5,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data= (X_test,y_test),\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - ETA:  - 0s 129us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6955296273889213, 0.4952978193759918]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM AND GRU METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1[['Open','High','Low','Volume', 'Close','compound_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing function\n",
    "def normalized_df(df):\n",
    "    normalized_df=(df-df.mean())/df.std()\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for later\n",
    "normalized_df2 = copy.deepcopy(normalized_df1)\n",
    "normalized_df3 = copy.deepcopy(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>74.129997</td>\n",
       "      <td>75.160004</td>\n",
       "      <td>74.010002</td>\n",
       "      <td>3757800</td>\n",
       "      <td>74.739998</td>\n",
       "      <td>-0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>75.330002</td>\n",
       "      <td>76.199997</td>\n",
       "      <td>74.849998</td>\n",
       "      <td>4976900</td>\n",
       "      <td>75.989998</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>75.580002</td>\n",
       "      <td>74.570000</td>\n",
       "      <td>4051200</td>\n",
       "      <td>75.070000</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>74.169998</td>\n",
       "      <td>74.730003</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>4379000</td>\n",
       "      <td>73.349998</td>\n",
       "      <td>-0.157993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>73.620003</td>\n",
       "      <td>73.860001</td>\n",
       "      <td>71.790001</td>\n",
       "      <td>5773000</td>\n",
       "      <td>71.930000</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>162.419998</td>\n",
       "      <td>164.080002</td>\n",
       "      <td>162.380005</td>\n",
       "      <td>3110500</td>\n",
       "      <td>163.979996</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>167.419998</td>\n",
       "      <td>164.869995</td>\n",
       "      <td>4243200</td>\n",
       "      <td>166.500000</td>\n",
       "      <td>-0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>164.470001</td>\n",
       "      <td>2689700</td>\n",
       "      <td>166.229996</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>169.070007</td>\n",
       "      <td>166.350006</td>\n",
       "      <td>3765000</td>\n",
       "      <td>168.029999</td>\n",
       "      <td>-0.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>167.699997</td>\n",
       "      <td>168.800003</td>\n",
       "      <td>167.220001</td>\n",
       "      <td>2580300</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1277 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low   Volume       Close  compound_mean\n",
       "0      74.129997   75.160004   74.010002  3757800   74.739998      -0.493900\n",
       "1      75.330002   76.199997   74.849998  4976900   75.989998       0.074200\n",
       "2      75.580002   75.580002   74.570000  4051200   75.070000       0.084700\n",
       "3      74.169998   74.730003   73.000000  4379000   73.349998      -0.157993\n",
       "4      73.620003   73.860001   71.790001  5773000   71.930000       0.360000\n",
       "...          ...         ...         ...      ...         ...            ...\n",
       "1272  162.419998  164.080002  162.380005  3110500  163.979996       0.055833\n",
       "1273  165.000000  167.419998  164.869995  4243200  166.500000      -0.273000\n",
       "1274  165.250000  166.449997  164.470001  2689700  166.229996       0.055833\n",
       "1275  166.449997  169.070007  166.350006  3765000  168.029999      -0.361200\n",
       "1276  167.699997  168.800003  167.220001  2580300  168.500000       0.055833\n",
       "\n",
       "[1277 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding label: up or down or steady\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.289, -1.278, -1.268, ..., -1.269, -1.929,  0.   ],\n",
       "       [-1.249, -1.243, -1.24 , ..., -1.228,  0.029,  1.   ],\n",
       "       [-1.241, -1.264, -1.25 , ..., -1.258,  0.065,  0.   ],\n",
       "       ...,\n",
       "       [ 1.743,  1.739,  1.758, ...,  1.771, -0.035,  0.   ],\n",
       "       [ 1.783,  1.825,  1.821, ...,  1.831, -1.472,  1.   ],\n",
       "       [ 1.824,  1.816,  1.85 , ...,  1.846, -0.035,  1.   ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#defining our generator\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=32, step=5):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][-1]\n",
    "        yield samples, to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30\n",
    "step = 10\n",
    "delay = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train, test and validation set\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2/2 [==============================] - ETA: 3s - loss: 0.2503 - accuracy: 0.50 - 6s 3s/step - loss: 0.2490 - accuracy: 0.5625 - val_loss: 0.2507 - val_accuracy: 0.4911\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.56 - 1s 674ms/step - loss: 0.2482 - accuracy: 0.5469 - val_loss: 0.2482 - val_accuracy: 0.4915\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.40 - 1s 692ms/step - loss: 0.2538 - accuracy: 0.4062 - val_loss: 0.2502 - val_accuracy: 0.4281\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.53 - 1s 682ms/step - loss: 0.2506 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.4416\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.59 - 1s 730ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2502 - val_accuracy: 0.4648\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.68 - 1s 669ms/step - loss: 0.2431 - accuracy: 0.6562 - val_loss: 0.2506 - val_accuracy: 0.4908\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.53 - 1s 703ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2511 - val_accuracy: 0.4736\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.65 - 1s 676ms/step - loss: 0.2459 - accuracy: 0.6094 - val_loss: 0.2508 - val_accuracy: 0.4911\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.59 - 1s 673ms/step - loss: 0.2516 - accuracy: 0.5000 - val_loss: 0.2458 - val_accuracy: 0.4915\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.50 - 2s 794ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2498 - val_accuracy: 0.4909\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.40 - 1s 666ms/step - loss: 0.2502 - accuracy: 0.5000 - val_loss: 0.2528 - val_accuracy: 0.4904\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.53 - 1s 655ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2497 - val_accuracy: 0.4917\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.56 - 1s 684ms/step - loss: 0.2467 - accuracy: 0.5469 - val_loss: 0.2514 - val_accuracy: 0.4908\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.40 - 2s 767ms/step - loss: 0.2589 - accuracy: 0.4062 - val_loss: 0.2528 - val_accuracy: 0.4912\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 731ms/step - loss: 0.2482 - accuracy: 0.5469 - val_loss: 0.2516 - val_accuracy: 0.4911\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.56 - 1s 705ms/step - loss: 0.2473 - accuracy: 0.5312 - val_loss: 0.2453 - val_accuracy: 0.4915\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.65 - 1s 676ms/step - loss: 0.2401 - accuracy: 0.6406 - val_loss: 0.2497 - val_accuracy: 0.4909\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.53 - 1s 740ms/step - loss: 0.2479 - accuracy: 0.5312 - val_loss: 0.2529 - val_accuracy: 0.4904\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.65 - 1s 666ms/step - loss: 0.2449 - accuracy: 0.6094 - val_loss: 0.2500 - val_accuracy: 0.4917\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.59 - 1s 677ms/step - loss: 0.2506 - accuracy: 0.5000 - val_loss: 0.2515 - val_accuracy: 0.4908\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.50 - 1s 686ms/step - loss: 0.2485 - accuracy: 0.5625 - val_loss: 0.2548 - val_accuracy: 0.4912\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.40 - 1s 730ms/step - loss: 0.2504 - accuracy: 0.5000 - val_loss: 0.2523 - val_accuracy: 0.4911\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.53 - 1s 692ms/step - loss: 0.2450 - accuracy: 0.5625 - val_loss: 0.2441 - val_accuracy: 0.4915\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.56 - 1s 659ms/step - loss: 0.2442 - accuracy: 0.5469 - val_loss: 0.2497 - val_accuracy: 0.4909\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.40 - 1s 688ms/step - loss: 0.2567 - accuracy: 0.4062 - val_loss: 0.2527 - val_accuracy: 0.4904\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.53 - 1s 669ms/step - loss: 0.2494 - accuracy: 0.5938 - val_loss: 0.2507 - val_accuracy: 0.4917\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.68 - 1s 749ms/step - loss: 0.2452 - accuracy: 0.5938 - val_loss: 0.2517 - val_accuracy: 0.4908\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.65 - 1s 672ms/step - loss: 0.2384 - accuracy: 0.6406 - val_loss: 0.2548 - val_accuracy: 0.4912\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.53 - 1s 669ms/step - loss: 0.2478 - accuracy: 0.5312 - val_loss: 0.2524 - val_accuracy: 0.4911\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.62 - 1s 673ms/step - loss: 0.2441 - accuracy: 0.5938 - val_loss: 0.2429 - val_accuracy: 0.4915\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.59 - 1s 666ms/step - loss: 0.2508 - accuracy: 0.5000 - val_loss: 0.2496 - val_accuracy: 0.4909\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 1s 742ms/step - loss: 0.2486 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4904\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.40 - 1s 676ms/step - loss: 0.2511 - accuracy: 0.4844 - val_loss: 0.2512 - val_accuracy: 0.4917\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.53 - 1s 681ms/step - loss: 0.2467 - accuracy: 0.5625 - val_loss: 0.2524 - val_accuracy: 0.4908\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.56 - 1s 669ms/step - loss: 0.2435 - accuracy: 0.5469 - val_loss: 0.2562 - val_accuracy: 0.4912\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.40 - 2s 761ms/step - loss: 0.2505 - accuracy: 0.4219 - val_loss: 0.2531 - val_accuracy: 0.4911\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.50 - 2s 760ms/step - loss: 0.2476 - accuracy: 0.6094 - val_loss: 0.2439 - val_accuracy: 0.4915\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.59 - 1s 737ms/step - loss: 0.2405 - accuracy: 0.5938 - val_loss: 0.2495 - val_accuracy: 0.4864\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.62 - 2s 817ms/step - loss: 0.2363 - accuracy: 0.6250 - val_loss: 0.2545 - val_accuracy: 0.4904\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.53 - 2s 775ms/step - loss: 0.2475 - accuracy: 0.5469 - val_loss: 0.2521 - val_accuracy: 0.4872\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.62 - 2s 769ms/step - loss: 0.2438 - accuracy: 0.5938 - val_loss: 0.2529 - val_accuracy: 0.4908\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.56 - 2s 773ms/step - loss: 0.2510 - accuracy: 0.4844 - val_loss: 0.2564 - val_accuracy: 0.4912\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.50 - 1s 671ms/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2536 - val_accuracy: 0.4911\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.43 - 1s 681ms/step - loss: 0.2505 - accuracy: 0.5000 - val_loss: 0.2421 - val_accuracy: 0.4915\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2488 - accuracy: 0.5938 - val_loss: 0.2493 - val_accuracy: 0.4909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.56 - 1s 749ms/step - loss: 0.2429 - accuracy: 0.5625 - val_loss: 0.2546 - val_accuracy: 0.4904\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.37 - 1s 721ms/step - loss: 0.2546 - accuracy: 0.4219 - val_loss: 0.2522 - val_accuracy: 0.4872\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.46 - 1s 736ms/step - loss: 0.2505 - accuracy: 0.5469 - val_loss: 0.2531 - val_accuracy: 0.4820\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.62 - 1s 727ms/step - loss: 0.2421 - accuracy: 0.5781 - val_loss: 0.2570 - val_accuracy: 0.4824\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.65 - 2s 806ms/step - loss: 0.2338 - accuracy: 0.6406 - val_loss: 0.2532 - val_accuracy: 0.4865\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.56 - 1s 689ms/step - loss: 0.2468 - accuracy: 0.5625 - val_loss: 0.2432 - val_accuracy: 0.4827\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.65 - 1s 723ms/step - loss: 0.2421 - accuracy: 0.6250 - val_loss: 0.2489 - val_accuracy: 0.4864\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.59 - 2s 788ms/step - loss: 0.2517 - accuracy: 0.5000 - val_loss: 0.2542 - val_accuracy: 0.4860\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.50 - 1s 731ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2529 - val_accuracy: 0.4917\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.43 - 1s 729ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2532 - val_accuracy: 0.4864\n",
      "Epoch 56/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.53 - 2s 794ms/step - loss: 0.2435 - accuracy: 0.5781 - val_loss: 0.2578 - val_accuracy: 0.4822\n",
      "Epoch 57/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.56 - 1s 722ms/step - loss: 0.2389 - accuracy: 0.5469 - val_loss: 0.2533 - val_accuracy: 0.4776\n",
      "Epoch 58/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.43 - 1s 689ms/step - loss: 0.2553 - accuracy: 0.4531 - val_loss: 0.2441 - val_accuracy: 0.4555\n",
      "Epoch 59/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.53 - 1s 677ms/step - loss: 0.2534 - accuracy: 0.5938 - val_loss: 0.2487 - val_accuracy: 0.4509\n",
      "Epoch 60/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.71 - 1s 748ms/step - loss: 0.2412 - accuracy: 0.6562 - val_loss: 0.2550 - val_accuracy: 0.4502\n",
      "Epoch 61/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.59 - 1s 694ms/step - loss: 0.2351 - accuracy: 0.6094 - val_loss: 0.2542 - val_accuracy: 0.4604\n",
      "Epoch 62/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2454 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4510\n",
      "Epoch 63/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.65 - 1s 689ms/step - loss: 0.2437 - accuracy: 0.5938 - val_loss: 0.2592 - val_accuracy: 0.4643\n",
      "Epoch 64/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.46 - 2s 801ms/step - loss: 0.2518 - accuracy: 0.4688 - val_loss: 0.2532 - val_accuracy: 0.4688\n",
      "Epoch 65/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.53 - 2s 756ms/step - loss: 0.2495 - accuracy: 0.5469 - val_loss: 0.2415 - val_accuracy: 0.4824\n",
      "Epoch 66/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.50 - 2s 755ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2485 - val_accuracy: 0.4554\n",
      "Epoch 67/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.59 - 2s 800ms/step - loss: 0.2473 - accuracy: 0.6250 - val_loss: 0.2554 - val_accuracy: 0.4591\n",
      "Epoch 68/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.62 - 1s 724ms/step - loss: 0.2356 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4604\n",
      "Epoch 69/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.40 - 1s 724ms/step - loss: 0.2492 - accuracy: 0.5156 - val_loss: 0.2552 - val_accuracy: 0.4420\n",
      "Epoch 70/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.53 - 2s 786ms/step - loss: 0.2513 - accuracy: 0.5781 - val_loss: 0.2588 - val_accuracy: 0.4420\n",
      "Epoch 71/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.65 - 1s 734ms/step - loss: 0.2406 - accuracy: 0.6562 - val_loss: 0.2536 - val_accuracy: 0.4420\n",
      "Epoch 72/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.56 - 1s 677ms/step - loss: 0.2318 - accuracy: 0.5781 - val_loss: 0.2431 - val_accuracy: 0.4420\n",
      "Epoch 73/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.50 - 1s 695ms/step - loss: 0.2465 - accuracy: 0.5625 - val_loss: 0.2482 - val_accuracy: 0.4374\n",
      "Epoch 74/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.62 - 1s 744ms/step - loss: 0.2409 - accuracy: 0.6250 - val_loss: 0.2565 - val_accuracy: 0.4635\n",
      "Epoch 75/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.46 - 1s 737ms/step - loss: 0.2527 - accuracy: 0.4375 - val_loss: 0.2554 - val_accuracy: 0.4605\n",
      "Epoch 76/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.53 - 1s 719ms/step - loss: 0.2505 - accuracy: 0.4844 - val_loss: 0.2548 - val_accuracy: 0.4463\n",
      "Epoch 77/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.56 - 2s 792ms/step - loss: 0.2499 - accuracy: 0.5469 - val_loss: 0.2592 - val_accuracy: 0.4642\n",
      "Epoch 78/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.56 - 1s 658ms/step - loss: 0.2416 - accuracy: 0.5938 - val_loss: 0.2534 - val_accuracy: 0.4509\n",
      "Epoch 79/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.62 - 1s 668ms/step - loss: 0.2343 - accuracy: 0.5781 - val_loss: 0.2433 - val_accuracy: 0.4422\n",
      "Epoch 80/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.59 - 1s 677ms/step - loss: 0.2443 - accuracy: 0.6719 - val_loss: 0.2481 - val_accuracy: 0.4240\n",
      "Epoch 81/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.46 - 1s 748ms/step - loss: 0.2519 - accuracy: 0.5156 - val_loss: 0.2559 - val_accuracy: 0.4369\n",
      "Epoch 82/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.65 - 2s 761ms/step - loss: 0.2390 - accuracy: 0.6250 - val_loss: 0.2561 - val_accuracy: 0.4287\n",
      "Epoch 83/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.56 - 1s 684ms/step - loss: 0.2342 - accuracy: 0.6406 - val_loss: 0.2548 - val_accuracy: 0.4378\n",
      "Epoch 84/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.46 - 1s 667ms/step - loss: 0.2468 - accuracy: 0.5469 - val_loss: 0.2591 - val_accuracy: 0.4374\n",
      "Epoch 85/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.62 - 1s 670ms/step - loss: 0.2410 - accuracy: 0.5781 - val_loss: 0.2541 - val_accuracy: 0.4554\n",
      "Epoch 86/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.53 - 1s 729ms/step - loss: 0.2524 - accuracy: 0.4688 - val_loss: 0.2424 - val_accuracy: 0.4555\n",
      "Epoch 87/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.56 - 1s 689ms/step - loss: 0.2506 - accuracy: 0.5312 - val_loss: 0.2487 - val_accuracy: 0.4598\n",
      "Epoch 88/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.50 - 1s 674ms/step - loss: 0.2509 - accuracy: 0.5312 - val_loss: 0.2561 - val_accuracy: 0.4591\n",
      "Epoch 89/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.53 - 1s 676ms/step - loss: 0.2431 - accuracy: 0.5938 - val_loss: 0.2547 - val_accuracy: 0.4559\n",
      "Epoch 90/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.65 - 1s 697ms/step - loss: 0.2394 - accuracy: 0.6250 - val_loss: 0.2539 - val_accuracy: 0.4510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.53 - 2s 753ms/step - loss: 0.2449 - accuracy: 0.6094 - val_loss: 0.2585 - val_accuracy: 0.4375\n",
      "Epoch 92/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.40 - 1s 708ms/step - loss: 0.2552 - accuracy: 0.4844 - val_loss: 0.2537 - val_accuracy: 0.4375\n",
      "Epoch 93/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.65 - 1s 680ms/step - loss: 0.2408 - accuracy: 0.6406 - val_loss: 0.2442 - val_accuracy: 0.4331\n",
      "Epoch 94/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.50 - 1s 678ms/step - loss: 0.2359 - accuracy: 0.5625 - val_loss: 0.2488 - val_accuracy: 0.4330\n",
      "Epoch 95/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.53 - 1s 730ms/step - loss: 0.2431 - accuracy: 0.5625 - val_loss: 0.2565 - val_accuracy: 0.4369\n",
      "Epoch 96/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.62 - 1s 679ms/step - loss: 0.2437 - accuracy: 0.5781 - val_loss: 0.2554 - val_accuracy: 0.4559\n",
      "Epoch 97/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.46 - 1s 669ms/step - loss: 0.2523 - accuracy: 0.4688 - val_loss: 0.2542 - val_accuracy: 0.4553\n",
      "Epoch 98/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2594 - val_accuracy: 0.4777\n",
      "Epoch 99/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.50 - 1s 675ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2540 - val_accuracy: 0.4597\n",
      "Epoch 100/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.59 - 1s 741ms/step - loss: 0.2486 - accuracy: 0.6094 - val_loss: 0.2420 - val_accuracy: 0.4601\n",
      "Epoch 101/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.65 - 2s 773ms/step - loss: 0.2297 - accuracy: 0.5781 - val_loss: 0.2490 - val_accuracy: 0.4554\n",
      "Epoch 102/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.46 - 2s 759ms/step - loss: 0.2393 - accuracy: 0.5625 - val_loss: 0.2570 - val_accuracy: 0.4503\n",
      "Epoch 103/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.40 - 2s 909ms/step - loss: 0.2529 - accuracy: 0.5000 - val_loss: 0.2557 - val_accuracy: 0.4469\n",
      "Epoch 104/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.68 - 1s 696ms/step - loss: 0.2348 - accuracy: 0.6406 - val_loss: 0.2542 - val_accuracy: 0.4420\n",
      "Epoch 105/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.59 - 1s 686ms/step - loss: 0.2349 - accuracy: 0.6406 - val_loss: 0.2594 - val_accuracy: 0.4509\n",
      "Epoch 106/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.53 - 1s 674ms/step - loss: 0.2491 - accuracy: 0.5781 - val_loss: 0.2542 - val_accuracy: 0.4463\n",
      "Epoch 107/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.65 - 2s 757ms/step - loss: 0.2395 - accuracy: 0.6250 - val_loss: 0.2404 - val_accuracy: 0.4643\n",
      "Epoch 108/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 723ms/step - loss: 0.2526 - accuracy: 0.4688 - val_loss: 0.2490 - val_accuracy: 0.4777\n",
      "Epoch 109/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 686ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2582 - val_accuracy: 0.4858\n",
      "Epoch 110/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.53 - 1s 699ms/step - loss: 0.2499 - accuracy: 0.5625 - val_loss: 0.2550 - val_accuracy: 0.4738\n",
      "Epoch 111/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.50 - 1s 749ms/step - loss: 0.2439 - accuracy: 0.5781 - val_loss: 0.2538 - val_accuracy: 0.4732\n",
      "Epoch 112/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.65 - 1s 679ms/step - loss: 0.2310 - accuracy: 0.5938 - val_loss: 0.2587 - val_accuracy: 0.4554\n",
      "Epoch 113/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.56 - 1s 686ms/step - loss: 0.2381 - accuracy: 0.6094 - val_loss: 0.2540 - val_accuracy: 0.4416\n",
      "Epoch 114/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.46 - 2s 1s/step - loss: 0.2582 - accuracy: 0.5469 - val_loss: 0.2430 - val_accuracy: 0.4511\n",
      "Epoch 115/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.59 - 2s 1s/step - loss: 0.2370 - accuracy: 0.5938 - val_loss: 0.2491 - val_accuracy: 0.4509\n",
      "Epoch 116/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.65 - 2s 935ms/step - loss: 0.2330 - accuracy: 0.6875 - val_loss: 0.2578 - val_accuracy: 0.4547\n",
      "Epoch 117/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.46 - 2s 952ms/step - loss: 0.2507 - accuracy: 0.5469 - val_loss: 0.2555 - val_accuracy: 0.4559\n",
      "Epoch 118/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.65 - 1s 689ms/step - loss: 0.2431 - accuracy: 0.5938 - val_loss: 0.2541 - val_accuracy: 0.4776\n",
      "Epoch 119/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.53 - 1s 663ms/step - loss: 0.2526 - accuracy: 0.4844 - val_loss: 0.2578 - val_accuracy: 0.4821\n",
      "Epoch 120/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.59 - 1s 665ms/step - loss: 0.2488 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4865\n",
      "Epoch 121/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.53 - 2s 752ms/step - loss: 0.2481 - accuracy: 0.5781 - val_loss: 0.2409 - val_accuracy: 0.4778\n",
      "Epoch 122/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.56 - 1s 686ms/step - loss: 0.2440 - accuracy: 0.6094 - val_loss: 0.2496 - val_accuracy: 0.4777\n",
      "Epoch 123/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.59 - 1s 679ms/step - loss: 0.2322 - accuracy: 0.5625 - val_loss: 0.2574 - val_accuracy: 0.4769\n",
      "Epoch 124/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.53 - 1s 670ms/step - loss: 0.2455 - accuracy: 0.5781 - val_loss: 0.2553 - val_accuracy: 0.4559\n",
      "Epoch 125/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.46 - 1s 732ms/step - loss: 0.2518 - accuracy: 0.5312 - val_loss: 0.2530 - val_accuracy: 0.4643\n",
      "Epoch 126/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.62 - 1s 677ms/step - loss: 0.2393 - accuracy: 0.6094 - val_loss: 0.2570 - val_accuracy: 0.4642\n",
      "Epoch 127/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.56 - 1s 675ms/step - loss: 0.2369 - accuracy: 0.5938 - val_loss: 0.2544 - val_accuracy: 0.4688\n",
      "Epoch 128/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.50 - 1s 674ms/step - loss: 0.2455 - accuracy: 0.5312 - val_loss: 0.2419 - val_accuracy: 0.4689\n",
      "Epoch 129/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.62 - 1s 669ms/step - loss: 0.2416 - accuracy: 0.6094 - val_loss: 0.2495 - val_accuracy: 0.4822\n",
      "Epoch 130/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.50 - 1s 723ms/step - loss: 0.2512 - accuracy: 0.4688 - val_loss: 0.2575 - val_accuracy: 0.4769\n",
      "Epoch 131/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.50 - 1s 670ms/step - loss: 0.2493 - accuracy: 0.5156 - val_loss: 0.2536 - val_accuracy: 0.4872\n",
      "Epoch 132/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.46 - 1s 665ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2530 - val_accuracy: 0.4862\n",
      "Epoch 133/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.56 - 1s 660ms/step - loss: 0.2427 - accuracy: 0.6094 - val_loss: 0.2576 - val_accuracy: 0.4824\n",
      "Epoch 134/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.65 - 1s 671ms/step - loss: 0.2274 - accuracy: 0.5781 - val_loss: 0.2544 - val_accuracy: 0.4911\n",
      "Epoch 135/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.56 - 2s 759ms/step - loss: 0.2443 - accuracy: 0.5938 - val_loss: 0.2424 - val_accuracy: 0.4689\n",
      "Epoch 136/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.46 - 1s 664ms/step - loss: 0.2489 - accuracy: 0.5312 - val_loss: 0.2495 - val_accuracy: 0.4733\n",
      "Epoch 137/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.75 - 1s 677ms/step - loss: 0.2296 - accuracy: 0.6875 - val_loss: 0.2574 - val_accuracy: 0.4681\n",
      "Epoch 138/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.56 - 1s 668ms/step - loss: 0.2308 - accuracy: 0.5938 - val_loss: 0.2548 - val_accuracy: 0.4738\n",
      "Epoch 139/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.46 - 1s 666ms/step - loss: 0.2452 - accuracy: 0.5469 - val_loss: 0.2527 - val_accuracy: 0.4776\n",
      "Epoch 140/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.62 - 1s 715ms/step - loss: 0.2422 - accuracy: 0.6094 - val_loss: 0.2575 - val_accuracy: 0.4915\n",
      "Epoch 141/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.50 - 1s 672ms/step - loss: 0.2527 - accuracy: 0.4531 - val_loss: 0.2550 - val_accuracy: 0.4909\n",
      "Epoch 142/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.59 - 1s 671ms/step - loss: 0.2482 - accuracy: 0.5781 - val_loss: 0.2390 - val_accuracy: 0.4871\n",
      "Epoch 143/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.40 - 1s 666ms/step - loss: 0.2493 - accuracy: 0.4844 - val_loss: 0.2500 - val_accuracy: 0.4908\n",
      "Epoch 144/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.53 - 1s 664ms/step - loss: 0.2452 - accuracy: 0.5781 - val_loss: 0.2581 - val_accuracy: 0.4860\n",
      "Epoch 145/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.65 - 1s 727ms/step - loss: 0.2272 - accuracy: 0.6094 - val_loss: 0.2533 - val_accuracy: 0.4872\n",
      "Epoch 146/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2488 - accuracy: 0.6250 - val_loss: 0.2524 - val_accuracy: 0.4864\n",
      "Epoch 147/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.50 - 1s 668ms/step - loss: 0.2543 - accuracy: 0.5156 - val_loss: 0.2563 - val_accuracy: 0.4866\n",
      "Epoch 148/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.62 - 1s 747ms/step - loss: 0.2380 - accuracy: 0.6250 - val_loss: 0.2546 - val_accuracy: 0.4866\n",
      "Epoch 149/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.53 - 2s 810ms/step - loss: 0.2361 - accuracy: 0.5469 - val_loss: 0.2407 - val_accuracy: 0.4780\n",
      "Epoch 150/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2478 - accuracy: 0.5625 - val_loss: 0.2496 - val_accuracy: 0.4776\n",
      "Epoch 151/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.68 - 1s 715ms/step - loss: 0.2397 - accuracy: 0.6250 - val_loss: 0.2586 - val_accuracy: 0.4860\n",
      "Epoch 152/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.56 - 2s 762ms/step - loss: 0.2520 - accuracy: 0.4844 - val_loss: 0.2532 - val_accuracy: 0.4917\n",
      "Epoch 153/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 679ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2532 - val_accuracy: 0.4864\n",
      "Epoch 154/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.40 - 1s 664ms/step - loss: 0.2489 - accuracy: 0.5000 - val_loss: 0.2578 - val_accuracy: 0.4913\n",
      "Epoch 155/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 671ms/step - loss: 0.2448 - accuracy: 0.5625 - val_loss: 0.2553 - val_accuracy: 0.4909\n",
      "Epoch 156/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.65 - 1s 671ms/step - loss: 0.2296 - accuracy: 0.5938 - val_loss: 0.2401 - val_accuracy: 0.4872\n",
      "Epoch 157/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.46 - 1s 735ms/step - loss: 0.2490 - accuracy: 0.5469 - val_loss: 0.2496 - val_accuracy: 0.4866\n",
      "Epoch 158/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.46 - 1s 654ms/step - loss: 0.2564 - accuracy: 0.5312 - val_loss: 0.2584 - val_accuracy: 0.4813\n",
      "Epoch 159/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.62 - 1s 662ms/step - loss: 0.2356 - accuracy: 0.6250 - val_loss: 0.2548 - val_accuracy: 0.4784\n",
      "Epoch 160/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.59 - 1s 693ms/step - loss: 0.2336 - accuracy: 0.6094 - val_loss: 0.2533 - val_accuracy: 0.4864\n",
      "Epoch 161/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.43 - 1s 660ms/step - loss: 0.2454 - accuracy: 0.5312 - val_loss: 0.2578 - val_accuracy: 0.4821\n",
      "Epoch 162/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.59 - 1s 730ms/step - loss: 0.2428 - accuracy: 0.5938 - val_loss: 0.2557 - val_accuracy: 0.4822\n",
      "Epoch 163/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.56 - 1s 659ms/step - loss: 0.2537 - accuracy: 0.5000 - val_loss: 0.2401 - val_accuracy: 0.4780\n",
      "Epoch 164/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.56 - 1s 668ms/step - loss: 0.2460 - accuracy: 0.5625 - val_loss: 0.2500 - val_accuracy: 0.4908\n",
      "Epoch 165/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.46 - 1s 669ms/step - loss: 0.2473 - accuracy: 0.5312 - val_loss: 0.2600 - val_accuracy: 0.4770\n",
      "Epoch 166/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.50 - 1s 672ms/step - loss: 0.2439 - accuracy: 0.5625 - val_loss: 0.2540 - val_accuracy: 0.4782\n",
      "Epoch 167/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.62 - 1s 734ms/step - loss: 0.2261 - accuracy: 0.5938 - val_loss: 0.2533 - val_accuracy: 0.4864\n",
      "Epoch 168/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.46 - 1s 665ms/step - loss: 0.2482 - accuracy: 0.5312 - val_loss: 0.2576 - val_accuracy: 0.4643\n",
      "Epoch 169/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.50 - 1s 659ms/step - loss: 0.2479 - accuracy: 0.5781 - val_loss: 0.2553 - val_accuracy: 0.4551\n",
      "Epoch 170/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.59 - 1s 666ms/step - loss: 0.2322 - accuracy: 0.6250 - val_loss: 0.2415 - val_accuracy: 0.4645\n",
      "Epoch 171/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.56 - 1s 661ms/step - loss: 0.2350 - accuracy: 0.5781 - val_loss: 0.2495 - val_accuracy: 0.4822\n",
      "Epoch 172/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.46 - 1s 735ms/step - loss: 0.2433 - accuracy: 0.5469 - val_loss: 0.2601 - val_accuracy: 0.4769\n",
      "Epoch 173/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.62 - 1s 673ms/step - loss: 0.2405 - accuracy: 0.5781 - val_loss: 0.2543 - val_accuracy: 0.4872\n",
      "Epoch 174/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.46 - 1s 670ms/step - loss: 0.2545 - accuracy: 0.4531 - val_loss: 0.2535 - val_accuracy: 0.4908\n",
      "Epoch 175/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.50 - 1s 666ms/step - loss: 0.2473 - accuracy: 0.5156 - val_loss: 0.2602 - val_accuracy: 0.4913\n",
      "Epoch 176/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.43 - 1s 664ms/step - loss: 0.2471 - accuracy: 0.5469 - val_loss: 0.2563 - val_accuracy: 0.4866\n",
      "Epoch 177/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.40 - 1s 735ms/step - loss: 0.2480 - accuracy: 0.4844 - val_loss: 0.2398 - val_accuracy: 0.4778\n",
      "Epoch 178/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.62 - 1s 661ms/step - loss: 0.2242 - accuracy: 0.5938 - val_loss: 0.2492 - val_accuracy: 0.4780\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.50 - 1s 663ms/step - loss: 0.2487 - accuracy: 0.5625 - val_loss: 0.2599 - val_accuracy: 0.4547\n",
      "Epoch 180/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.53 - 1s 666ms/step - loss: 0.2462 - accuracy: 0.5781 - val_loss: 0.2561 - val_accuracy: 0.4561\n",
      "Epoch 181/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.62 - 1s 674ms/step - loss: 0.2344 - accuracy: 0.6250 - val_loss: 0.2538 - val_accuracy: 0.4551\n",
      "Epoch 182/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.56 - 1s 730ms/step - loss: 0.2396 - accuracy: 0.5781 - val_loss: 0.2594 - val_accuracy: 0.4599\n",
      "Epoch 183/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.46 - 1s 667ms/step - loss: 0.2433 - accuracy: 0.5469 - val_loss: 0.2563 - val_accuracy: 0.4642\n",
      "Epoch 184/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.62 - 1s 665ms/step - loss: 0.2429 - accuracy: 0.5781 - val_loss: 0.2382 - val_accuracy: 0.4824\n",
      "Epoch 185/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.53 - 1s 667ms/step - loss: 0.2542 - accuracy: 0.4688 - val_loss: 0.2493 - val_accuracy: 0.4866\n",
      "Epoch 186/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.56 - 1s 671ms/step - loss: 0.2460 - accuracy: 0.5938 - val_loss: 0.2615 - val_accuracy: 0.4904\n",
      "Epoch 187/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.46 - 1s 736ms/step - loss: 0.2465 - accuracy: 0.5625 - val_loss: 0.2548 - val_accuracy: 0.4873\n",
      "Epoch 188/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.50 - 1s 673ms/step - loss: 0.2434 - accuracy: 0.5469 - val_loss: 0.2538 - val_accuracy: 0.4820\n",
      "Epoch 189/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.62 - 1s 673ms/step - loss: 0.2291 - accuracy: 0.5781 - val_loss: 0.2591 - val_accuracy: 0.4821\n",
      "Epoch 190/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.53 - 1s 667ms/step - loss: 0.2443 - accuracy: 0.5938 - val_loss: 0.2563 - val_accuracy: 0.4642\n",
      "Epoch 191/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.46 - 1s 669ms/step - loss: 0.2475 - accuracy: 0.5469 - val_loss: 0.2413 - val_accuracy: 0.4557\n",
      "Epoch 192/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.68 - 2s 757ms/step - loss: 0.2370 - accuracy: 0.6562 - val_loss: 0.2495 - val_accuracy: 0.4598\n",
      "Epoch 193/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.62 - 1s 726ms/step - loss: 0.2330 - accuracy: 0.6406 - val_loss: 0.2619 - val_accuracy: 0.4769\n",
      "Epoch 194/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.53 - 1s 725ms/step - loss: 0.2416 - accuracy: 0.5781 - val_loss: 0.2559 - val_accuracy: 0.4784\n",
      "Epoch 195/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.62 - 2s 782ms/step - loss: 0.2413 - accuracy: 0.5938 - val_loss: 0.2543 - val_accuracy: 0.4864\n",
      "Epoch 196/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.62 - 1s 727ms/step - loss: 0.2541 - accuracy: 0.5312 - val_loss: 0.2600 - val_accuracy: 0.4866\n",
      "Epoch 197/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.43 - 1s 724ms/step - loss: 0.2483 - accuracy: 0.5000 - val_loss: 0.2577 - val_accuracy: 0.4732\n",
      "Epoch 198/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.43 - 2s 795ms/step - loss: 0.2464 - accuracy: 0.5000 - val_loss: 0.2397 - val_accuracy: 0.4689\n",
      "Epoch 199/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.50 - 1s 732ms/step - loss: 0.2451 - accuracy: 0.5000 - val_loss: 0.2491 - val_accuracy: 0.4689\n",
      "Epoch 200/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.68 - 1s 683ms/step - loss: 0.2278 - accuracy: 0.6250 - val_loss: 0.2625 - val_accuracy: 0.4591\n",
      "Epoch 201/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.50 - 1s 668ms/step - loss: 0.2484 - accuracy: 0.5469 - val_loss: 0.2578 - val_accuracy: 0.4515\n",
      "Epoch 202/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.53 - 2s 768ms/step - loss: 0.2443 - accuracy: 0.5938 - val_loss: 0.2539 - val_accuracy: 0.4551\n",
      "Epoch 203/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.62 - 1s 666ms/step - loss: 0.2289 - accuracy: 0.6250 - val_loss: 0.2595 - val_accuracy: 0.4555\n",
      "Epoch 204/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.65 - 1s 664ms/step - loss: 0.2269 - accuracy: 0.6406 - val_loss: 0.2578 - val_accuracy: 0.4597\n",
      "Epoch 205/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.46 - 1s 660ms/step - loss: 0.2456 - accuracy: 0.5469 - val_loss: 0.2406 - val_accuracy: 0.4557\n",
      "Epoch 206/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.65 - 2s 791ms/step - loss: 0.2384 - accuracy: 0.6250 - val_loss: 0.2496 - val_accuracy: 0.4778\n",
      "Epoch 207/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.53 - 1s 729ms/step - loss: 0.2520 - accuracy: 0.5000 - val_loss: 0.2625 - val_accuracy: 0.4725\n",
      "Epoch 208/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.62 - 1s 716ms/step - loss: 0.2450 - accuracy: 0.6094 - val_loss: 0.2544 - val_accuracy: 0.4872\n",
      "Epoch 209/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.46 - 2s 781ms/step - loss: 0.2445 - accuracy: 0.5781 - val_loss: 0.2541 - val_accuracy: 0.4820\n",
      "Epoch 210/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.53 - 1s 716ms/step - loss: 0.2462 - accuracy: 0.5625 - val_loss: 0.2616 - val_accuracy: 0.4732\n",
      "Epoch 211/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.65 - 1s 733ms/step - loss: 0.2263 - accuracy: 0.6250 - val_loss: 0.2588 - val_accuracy: 0.4733\n",
      "Epoch 212/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.46 - 1s 722ms/step - loss: 0.2534 - accuracy: 0.5000 - val_loss: 0.2406 - val_accuracy: 0.4601\n",
      "Epoch 213/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.56 - 2s 751ms/step - loss: 0.2497 - accuracy: 0.5781 - val_loss: 0.2496 - val_accuracy: 0.4553\n",
      "Epoch 214/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.62 - 1s 661ms/step - loss: 0.2384 - accuracy: 0.6250 - val_loss: 0.2634 - val_accuracy: 0.4591\n",
      "Epoch 215/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.59 - 1s 668ms/step - loss: 0.2422 - accuracy: 0.5938 - val_loss: 0.2564 - val_accuracy: 0.4696\n",
      "Epoch 216/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.53 - 1s 673ms/step - loss: 0.2466 - accuracy: 0.5781 - val_loss: 0.2546 - val_accuracy: 0.4597\n",
      "Epoch 217/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.71 - 1s 662ms/step - loss: 0.2387 - accuracy: 0.6094 - val_loss: 0.2621 - val_accuracy: 0.4866\n",
      "Epoch 218/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.59 - 1s 747ms/step - loss: 0.2555 - accuracy: 0.5156 - val_loss: 0.2587 - val_accuracy: 0.4777\n",
      "Epoch 219/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.56 - 1s 698ms/step - loss: 0.2442 - accuracy: 0.5625 - val_loss: 0.2359 - val_accuracy: 0.4916\n",
      "Epoch 220/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.43 - 1s 679ms/step - loss: 0.2462 - accuracy: 0.4844 - val_loss: 0.2498 - val_accuracy: 0.4820\n",
      "Epoch 221/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.50 - 1s 663ms/step - loss: 0.2454 - accuracy: 0.5156 - val_loss: 0.2650 - val_accuracy: 0.4770\n",
      "Epoch 222/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.62 - 1s 724ms/step - loss: 0.2258 - accuracy: 0.6094 - val_loss: 0.2557 - val_accuracy: 0.4829\n",
      "Epoch 223/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.46 - 1s 664ms/step - loss: 0.2495 - accuracy: 0.5156 - val_loss: 0.2542 - val_accuracy: 0.4730\n",
      "Epoch 224/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.53 - 1s 713ms/step - loss: 0.2511 - accuracy: 0.6094 - val_loss: 0.2608 - val_accuracy: 0.4688\n",
      "Epoch 225/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.68 - 1s 706ms/step - loss: 0.2311 - accuracy: 0.6562 - val_loss: 0.2591 - val_accuracy: 0.4688\n",
      "Epoch 226/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.56 - 1s 737ms/step - loss: 0.2307 - accuracy: 0.6094 - val_loss: 0.2376 - val_accuracy: 0.4822\n",
      "Epoch 227/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.50 - 1s 662ms/step - loss: 0.2444 - accuracy: 0.5781 - val_loss: 0.2497 - val_accuracy: 0.4866\n",
      "Epoch 228/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.68 - 1s 674ms/step - loss: 0.2404 - accuracy: 0.6094 - val_loss: 0.2653 - val_accuracy: 0.4816\n",
      "Epoch 229/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.59 - 1s 668ms/step - loss: 0.2570 - accuracy: 0.5000 - val_loss: 0.2541 - val_accuracy: 0.4872\n",
      "Epoch 230/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.56 - 1s 663ms/step - loss: 0.2481 - accuracy: 0.5781 - val_loss: 0.2541 - val_accuracy: 0.4908\n",
      "Epoch 231/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.43 - 1s 734ms/step - loss: 0.2485 - accuracy: 0.5469 - val_loss: 0.2598 - val_accuracy: 0.4776\n",
      "Epoch 232/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 664ms/step - loss: 0.2426 - accuracy: 0.5469 - val_loss: 0.2572 - val_accuracy: 0.4688\n",
      "Epoch 233/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.65 - 1s 729ms/step - loss: 0.2250 - accuracy: 0.6094 - val_loss: 0.2398 - val_accuracy: 0.4645\n",
      "Epoch 234/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.56 - 1s 706ms/step - loss: 0.2451 - accuracy: 0.5938 - val_loss: 0.2496 - val_accuracy: 0.4553\n",
      "Epoch 235/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.50 - 2s 764ms/step - loss: 0.2516 - accuracy: 0.5625 - val_loss: 0.2624 - val_accuracy: 0.4547\n",
      "Epoch 236/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.68 - 1s 663ms/step - loss: 0.2387 - accuracy: 0.6562 - val_loss: 0.2566 - val_accuracy: 0.4605\n",
      "Epoch 237/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.59 - 1s 663ms/step - loss: 0.2390 - accuracy: 0.5625 - val_loss: 0.2536 - val_accuracy: 0.4685\n",
      "Epoch 238/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.46 - 1s 670ms/step - loss: 0.2440 - accuracy: 0.5469 - val_loss: 0.2594 - val_accuracy: 0.4688\n",
      "Epoch 239/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.62 - 1s 674ms/step - loss: 0.2374 - accuracy: 0.6094 - val_loss: 0.2579 - val_accuracy: 0.4777\n",
      "Epoch 240/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 725ms/step - loss: 0.2546 - accuracy: 0.4688 - val_loss: 0.2383 - val_accuracy: 0.4778\n",
      "Epoch 241/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.62 - 1s 669ms/step - loss: 0.2415 - accuracy: 0.5938 - val_loss: 0.2497 - val_accuracy: 0.4865\n",
      "Epoch 242/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.43 - 1s 662ms/step - loss: 0.2457 - accuracy: 0.5156 - val_loss: 0.2644 - val_accuracy: 0.4679\n",
      "Epoch 243/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.50 - 1s 666ms/step - loss: 0.2439 - accuracy: 0.5469 - val_loss: 0.2556 - val_accuracy: 0.4696\n",
      "Epoch 244/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.62 - 1s 671ms/step - loss: 0.2272 - accuracy: 0.5938 - val_loss: 0.2543 - val_accuracy: 0.4685\n",
      "Epoch 245/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.46 - 2s 761ms/step - loss: 0.2468 - accuracy: 0.4844 - val_loss: 0.2605 - val_accuracy: 0.4599\n",
      "Epoch 246/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.50 - 1s 665ms/step - loss: 0.2539 - accuracy: 0.5781 - val_loss: 0.2587 - val_accuracy: 0.4597\n",
      "Epoch 247/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.71 - 1s 662ms/step - loss: 0.2325 - accuracy: 0.6719 - val_loss: 0.2396 - val_accuracy: 0.4601\n",
      "Epoch 248/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.59 - 1s 658ms/step - loss: 0.2296 - accuracy: 0.5938 - val_loss: 0.2496 - val_accuracy: 0.4689\n",
      "Epoch 249/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.46 - 2s 857ms/step - loss: 0.2433 - accuracy: 0.5312 - val_loss: 0.2647 - val_accuracy: 0.4635\n",
      "Epoch 250/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.56 - 1s 668ms/step - loss: 0.2417 - accuracy: 0.5625 - val_loss: 0.2563 - val_accuracy: 0.4696\n"
     ]
    }
   ],
   "source": [
    "#Building and training LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=250,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.6145833134651184\n",
      "test_loss: 0.243784099817276\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2/2 [==============================] - ETA: 4s - loss: 0.2543 - accuracy: 0.50 - 6s 3s/step - loss: 0.2470 - accuracy: 0.6094 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.46 - 1s 633ms/step - loss: 0.2481 - accuracy: 0.5000 - val_loss: 0.2541 - val_accuracy: 0.4908\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.53 - 1s 664ms/step - loss: 0.2459 - accuracy: 0.5625 - val_loss: 0.2546 - val_accuracy: 0.4913\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.59 - 1s 689ms/step - loss: 0.2443 - accuracy: 0.5781 - val_loss: 0.2556 - val_accuracy: 0.4820\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.53 - 1s 603ms/step - loss: 0.2426 - accuracy: 0.5938 - val_loss: 0.2412 - val_accuracy: 0.4915\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.50 - 1s 605ms/step - loss: 0.2502 - accuracy: 0.5469 - val_loss: 0.2500 - val_accuracy: 0.4909\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.43 - 1s 613ms/step - loss: 0.2531 - accuracy: 0.4375 - val_loss: 0.2548 - val_accuracy: 0.4904\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.62 - 1s 602ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2504 - val_accuracy: 0.4917\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.59 - 1s 609ms/step - loss: 0.2546 - accuracy: 0.5469 - val_loss: 0.2529 - val_accuracy: 0.4864\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.59 - 1s 612ms/step - loss: 0.2522 - accuracy: 0.5625 - val_loss: 0.2557 - val_accuracy: 0.4868\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.59 - 1s 608ms/step - loss: 0.2546 - accuracy: 0.5312 - val_loss: 0.2559 - val_accuracy: 0.4820\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.40 - 1s 610ms/step - loss: 0.2544 - accuracy: 0.4531 - val_loss: 0.2442 - val_accuracy: 0.4827\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.56 - 1s 607ms/step - loss: 0.2460 - accuracy: 0.6250 - val_loss: 0.2509 - val_accuracy: 0.4818\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.50 - 1s 612ms/step - loss: 0.2488 - accuracy: 0.5469 - val_loss: 0.2583 - val_accuracy: 0.4816\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.59 - 1s 685ms/step - loss: 0.2445 - accuracy: 0.5625 - val_loss: 0.2512 - val_accuracy: 0.4827\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.59 - 1s 606ms/step - loss: 0.2432 - accuracy: 0.6094 - val_loss: 0.2546 - val_accuracy: 0.4864\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.53 - 1s 605ms/step - loss: 0.2476 - accuracy: 0.5312 - val_loss: 0.2617 - val_accuracy: 0.4912\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.37 - 1s 607ms/step - loss: 0.2541 - accuracy: 0.4375 - val_loss: 0.2568 - val_accuracy: 0.4865\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.65 - 1s 601ms/step - loss: 0.2519 - accuracy: 0.5469 - val_loss: 0.2428 - val_accuracy: 0.4871\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.56 - 1s 621ms/step - loss: 0.2433 - accuracy: 0.5312 - val_loss: 0.2494 - val_accuracy: 0.4864\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.59 - 1s 610ms/step - loss: 0.2386 - accuracy: 0.5938 - val_loss: 0.2591 - val_accuracy: 0.4860\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.62 - 1s 610ms/step - loss: 0.2521 - accuracy: 0.5312 - val_loss: 0.2525 - val_accuracy: 0.4917\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.40 - 1s 606ms/step - loss: 0.2553 - accuracy: 0.4375 - val_loss: 0.2538 - val_accuracy: 0.4996\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.56 - 1s 618ms/step - loss: 0.2505 - accuracy: 0.5156 - val_loss: 0.2589 - val_accuracy: 0.4915\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.53 - 1s 617ms/step - loss: 0.2426 - accuracy: 0.5312 - val_loss: 0.2580 - val_accuracy: 0.4952\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.56 - 1s 677ms/step - loss: 0.2462 - accuracy: 0.5469 - val_loss: 0.2427 - val_accuracy: 0.4869\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.65 - 1s 606ms/step - loss: 0.2456 - accuracy: 0.6562 - val_loss: 0.2491 - val_accuracy: 0.4865\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.50 - 1s 611ms/step - loss: 0.2489 - accuracy: 0.5312 - val_loss: 0.2632 - val_accuracy: 0.4860\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.43 - 1s 607ms/step - loss: 0.2553 - accuracy: 0.4375 - val_loss: 0.2518 - val_accuracy: 0.4829\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.56 - 1s 600ms/step - loss: 0.2548 - accuracy: 0.4844 - val_loss: 0.2528 - val_accuracy: 0.4598\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.59 - 1s 605ms/step - loss: 0.2415 - accuracy: 0.5938 - val_loss: 0.2563 - val_accuracy: 0.4732\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.56 - 1s 632ms/step - loss: 0.2388 - accuracy: 0.5469 - val_loss: 0.2539 - val_accuracy: 0.4602\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.56 - 1s 614ms/step - loss: 0.2484 - accuracy: 0.4844 - val_loss: 0.2457 - val_accuracy: 0.4733\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.37 - 1s 607ms/step - loss: 0.2529 - accuracy: 0.4688 - val_loss: 0.2496 - val_accuracy: 0.4778\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.59 - 1s 611ms/step - loss: 0.2571 - accuracy: 0.5156 - val_loss: 0.2568 - val_accuracy: 0.4769\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.53 - 1s 648ms/step - loss: 0.2448 - accuracy: 0.5469 - val_loss: 0.2510 - val_accuracy: 0.4829\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.59 - 1s 663ms/step - loss: 0.2447 - accuracy: 0.5781 - val_loss: 0.2526 - val_accuracy: 0.4643\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.59 - 1s 686ms/step - loss: 0.2447 - accuracy: 0.6094 - val_loss: 0.2556 - val_accuracy: 0.4551\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.50 - 1s 633ms/step - loss: 0.2451 - accuracy: 0.5312 - val_loss: 0.2557 - val_accuracy: 0.4864\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.46 - 1s 616ms/step - loss: 0.2505 - accuracy: 0.4688 - val_loss: 0.2458 - val_accuracy: 0.4643\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.65 - 1s 631ms/step - loss: 0.2500 - accuracy: 0.5312 - val_loss: 0.2498 - val_accuracy: 0.4466\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.62 - 1s 625ms/step - loss: 0.2452 - accuracy: 0.5781 - val_loss: 0.2553 - val_accuracy: 0.4544\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.62 - 1s 612ms/step - loss: 0.2392 - accuracy: 0.6094 - val_loss: 0.2514 - val_accuracy: 0.4517\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.53 - 1s 614ms/step - loss: 0.2475 - accuracy: 0.4844 - val_loss: 0.2528 - val_accuracy: 0.4332\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.46 - 1s 641ms/step - loss: 0.2555 - accuracy: 0.4844 - val_loss: 0.2553 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.46 - 1s 620ms/step - loss: 0.2449 - accuracy: 0.5312 - val_loss: 0.2525 - val_accuracy: 0.4375\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.50 - 1s 619ms/step - loss: 0.2399 - accuracy: 0.5781 - val_loss: 0.2468 - val_accuracy: 0.4420\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.53 - 1s 613ms/step - loss: 0.2461 - accuracy: 0.5000 - val_loss: 0.2482 - val_accuracy: 0.4511\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.56 - 1s 614ms/step - loss: 0.2456 - accuracy: 0.5938 - val_loss: 0.2557 - val_accuracy: 0.4590\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.62 - 1s 627ms/step - loss: 0.2467 - accuracy: 0.5625 - val_loss: 0.2518 - val_accuracy: 0.4873\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.46 - 1s 629ms/step - loss: 0.2598 - accuracy: 0.4219 - val_loss: 0.2525 - val_accuracy: 0.4688\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.56 - 1s 621ms/step - loss: 0.2488 - accuracy: 0.4844 - val_loss: 0.2554 - val_accuracy: 0.4463\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.53 - 1s 618ms/step - loss: 0.2474 - accuracy: 0.5156 - val_loss: 0.2525 - val_accuracy: 0.4820\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.53 - 1s 694ms/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2446 - val_accuracy: 0.4824\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.50 - 1s 615ms/step - loss: 0.2538 - accuracy: 0.5000 - val_loss: 0.2495 - val_accuracy: 0.4866\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00055: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Building and training a model with GRU\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.1))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=250,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5416666865348816\n",
      "test_loss: 0.2535386383533478\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "normalized_df1 = normalized_df2\n",
    "\n",
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std\n",
    "#adding label: up/down or steady\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying function \n",
    "del normalized_df1['compound_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.288893</td>\n",
       "      <td>-1.277830</td>\n",
       "      <td>-1.268443</td>\n",
       "      <td>-0.336991</td>\n",
       "      <td>-1.269058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.248970</td>\n",
       "      <td>-1.243467</td>\n",
       "      <td>-1.240343</td>\n",
       "      <td>0.130291</td>\n",
       "      <td>-1.227522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.240652</td>\n",
       "      <td>-1.263952</td>\n",
       "      <td>-1.249709</td>\n",
       "      <td>-0.224531</td>\n",
       "      <td>-1.258092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.287562</td>\n",
       "      <td>-1.292038</td>\n",
       "      <td>-1.302230</td>\n",
       "      <td>-0.098885</td>\n",
       "      <td>-1.315245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.305861</td>\n",
       "      <td>-1.320784</td>\n",
       "      <td>-1.342708</td>\n",
       "      <td>0.435436</td>\n",
       "      <td>-1.362429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>1.648477</td>\n",
       "      <td>1.660242</td>\n",
       "      <td>1.687779</td>\n",
       "      <td>-0.585101</td>\n",
       "      <td>1.696228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>1.734312</td>\n",
       "      <td>1.770602</td>\n",
       "      <td>1.771076</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>1.779963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>1.742630</td>\n",
       "      <td>1.738551</td>\n",
       "      <td>1.757695</td>\n",
       "      <td>-0.746394</td>\n",
       "      <td>1.770992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.782553</td>\n",
       "      <td>1.825121</td>\n",
       "      <td>1.820587</td>\n",
       "      <td>-0.334231</td>\n",
       "      <td>1.830803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>1.824140</td>\n",
       "      <td>1.816199</td>\n",
       "      <td>1.849690</td>\n",
       "      <td>-0.788327</td>\n",
       "      <td>1.846420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1277 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low    Volume     Close  Label\n",
       "0    -1.288893 -1.277830 -1.268443 -0.336991 -1.269058      0\n",
       "1    -1.248970 -1.243467 -1.240343  0.130291 -1.227522      1\n",
       "2    -1.240652 -1.263952 -1.249709 -0.224531 -1.258092      0\n",
       "3    -1.287562 -1.292038 -1.302230 -0.098885 -1.315245      0\n",
       "4    -1.305861 -1.320784 -1.342708  0.435436 -1.362429      0\n",
       "...        ...       ...       ...       ...       ...    ...\n",
       "1272  1.648477  1.660242  1.687779 -0.585101  1.696228      1\n",
       "1273  1.734312  1.770602  1.771076 -0.150937  1.779963      1\n",
       "1274  1.742630  1.738551  1.757695 -0.746394  1.770992      0\n",
       "1275  1.782553  1.825121  1.820587 -0.334231  1.830803      1\n",
       "1276  1.824140  1.816199  1.849690 -0.788327  1.846420      1\n",
       "\n",
       "[1277 rows x 6 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into tain validation and test set\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - ETA: 3s - loss: 0.2506 - accuracy: 0.53 - 6s 3s/step - loss: 0.2504 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4911\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.50 - 1s 720ms/step - loss: 0.2485 - accuracy: 0.5156 - val_loss: 0.2464 - val_accuracy: 0.4915\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.40 - 1s 677ms/step - loss: 0.2528 - accuracy: 0.3906 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.34 - 1s 670ms/step - loss: 0.2503 - accuracy: 0.4219 - val_loss: 0.2514 - val_accuracy: 0.4904\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.59 - 1s 664ms/step - loss: 0.2491 - accuracy: 0.5469 - val_loss: 0.2496 - val_accuracy: 0.4917\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.62 - 1s 663ms/step - loss: 0.2463 - accuracy: 0.6250 - val_loss: 0.2509 - val_accuracy: 0.4908\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.53 - 1s 742ms/step - loss: 0.2480 - accuracy: 0.5312 - val_loss: 0.2526 - val_accuracy: 0.4912\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.65 - 1s 671ms/step - loss: 0.2463 - accuracy: 0.6094 - val_loss: 0.2522 - val_accuracy: 0.4911\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.59 - 1s 668ms/step - loss: 0.2508 - accuracy: 0.5000 - val_loss: 0.2442 - val_accuracy: 0.4915\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.50 - 1s 663ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.40 - 1s 660ms/step - loss: 0.2508 - accuracy: 0.5000 - val_loss: 0.2532 - val_accuracy: 0.4904\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.53 - 1s 725ms/step - loss: 0.2455 - accuracy: 0.5625 - val_loss: 0.2492 - val_accuracy: 0.4917\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.56 - 1s 649ms/step - loss: 0.2469 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.40 - 1s 662ms/step - loss: 0.2578 - accuracy: 0.4062 - val_loss: 0.2526 - val_accuracy: 0.4912\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.53 - 1s 662ms/step - loss: 0.2477 - accuracy: 0.5469 - val_loss: 0.2517 - val_accuracy: 0.4911\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.56 - 1s 654ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2452 - val_accuracy: 0.4915\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.65 - 1s 659ms/step - loss: 0.2432 - accuracy: 0.6406 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.53 - 1s 728ms/step - loss: 0.2467 - accuracy: 0.5312 - val_loss: 0.2529 - val_accuracy: 0.4904\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.65 - 1s 664ms/step - loss: 0.2465 - accuracy: 0.6094 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.59 - 1s 669ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.50 - 1s 659ms/step - loss: 0.2481 - accuracy: 0.5625 - val_loss: 0.2544 - val_accuracy: 0.4912\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.40 - 1s 665ms/step - loss: 0.2507 - accuracy: 0.5000 - val_loss: 0.2524 - val_accuracy: 0.4911\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.53 - 1s 747ms/step - loss: 0.2464 - accuracy: 0.5625 - val_loss: 0.2431 - val_accuracy: 0.4915\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.56 - 1s 673ms/step - loss: 0.2482 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.40 - 1s 713ms/step - loss: 0.2572 - accuracy: 0.4219 - val_loss: 0.2531 - val_accuracy: 0.4904\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 669ms/step - loss: 0.2514 - accuracy: 0.5312 - val_loss: 0.2492 - val_accuracy: 0.4917\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.46 - 2s 751ms/step - loss: 0.2503 - accuracy: 0.4531 - val_loss: 0.2511 - val_accuracy: 0.4908\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.65 - 1s 653ms/step - loss: 0.2444 - accuracy: 0.6406 - val_loss: 0.2541 - val_accuracy: 0.4912\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.53 - 1s 656ms/step - loss: 0.2491 - accuracy: 0.5312 - val_loss: 0.2525 - val_accuracy: 0.4911\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.65 - 1s 661ms/step - loss: 0.2450 - accuracy: 0.6094 - val_loss: 0.2421 - val_accuracy: 0.4915\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.59 - 1s 663ms/step - loss: 0.2512 - accuracy: 0.5000 - val_loss: 0.2500 - val_accuracy: 0.4909\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.50 - 1s 727ms/step - loss: 0.2476 - accuracy: 0.5625 - val_loss: 0.2545 - val_accuracy: 0.4904\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.40 - 1s 688ms/step - loss: 0.2509 - accuracy: 0.5000 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.53 - 1s 658ms/step - loss: 0.2458 - accuracy: 0.5625 - val_loss: 0.2513 - val_accuracy: 0.4908\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2434 - accuracy: 0.5469 - val_loss: 0.2540 - val_accuracy: 0.4912\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.40 - 1s 669ms/step - loss: 0.2572 - accuracy: 0.4062 - val_loss: 0.2518 - val_accuracy: 0.4911\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.62 - 1s 727ms/step - loss: 0.2496 - accuracy: 0.5781 - val_loss: 0.2439 - val_accuracy: 0.4915\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.46 - 1s 668ms/step - loss: 0.2505 - accuracy: 0.5156 - val_loss: 0.2498 - val_accuracy: 0.4909\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.65 - 1s 663ms/step - loss: 0.2398 - accuracy: 0.6406 - val_loss: 0.2535 - val_accuracy: 0.4904\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.53 - 1s 669ms/step - loss: 0.2475 - accuracy: 0.5312 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.65 - 1s 667ms/step - loss: 0.2436 - accuracy: 0.6094 - val_loss: 0.2516 - val_accuracy: 0.4908\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.59 - 1s 736ms/step - loss: 0.2516 - accuracy: 0.5000 - val_loss: 0.2547 - val_accuracy: 0.4912\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 663ms/step - loss: 0.2470 - accuracy: 0.5625 - val_loss: 0.2532 - val_accuracy: 0.4911\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.40 - 1s 668ms/step - loss: 0.2514 - accuracy: 0.5000 - val_loss: 0.2418 - val_accuracy: 0.4915\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.53 - 1s 680ms/step - loss: 0.2512 - accuracy: 0.5625 - val_loss: 0.2497 - val_accuracy: 0.4909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.56 - 1s 668ms/step - loss: 0.2474 - accuracy: 0.5469 - val_loss: 0.2541 - val_accuracy: 0.4904\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.40 - 1s 747ms/step - loss: 0.2642 - accuracy: 0.3594 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2505 - accuracy: 0.5156 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 672ms/step - loss: 0.2470 - accuracy: 0.5312 - val_loss: 0.2539 - val_accuracy: 0.4912\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.62 - 1s 686ms/step - loss: 0.2419 - accuracy: 0.6250 - val_loss: 0.2526 - val_accuracy: 0.4911\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.53 - 2s 776ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2423 - val_accuracy: 0.4915\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.65 - 1s 671ms/step - loss: 0.2429 - accuracy: 0.6094 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.59 - 1s 671ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2540 - val_accuracy: 0.4904\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 698ms/step - loss: 0.2469 - accuracy: 0.5625 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.40 - 1s 672ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2514 - val_accuracy: 0.4908\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.53 - 1s 748ms/step - loss: 0.2490 - accuracy: 0.5625 - val_loss: 0.2545 - val_accuracy: 0.4912\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.56 - 1s 666ms/step - loss: 0.2524 - accuracy: 0.5469 - val_loss: 0.2522 - val_accuracy: 0.4911\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.40 - 1s 663ms/step - loss: 0.2602 - accuracy: 0.4062 - val_loss: 0.2434 - val_accuracy: 0.4915\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.56 - 1s 664ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2497 - val_accuracy: 0.4909\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2533 - val_accuracy: 0.4904\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.65 - 2s 752ms/step - loss: 0.2416 - accuracy: 0.6406 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 664ms/step - loss: 0.2505 - accuracy: 0.5312 - val_loss: 0.2514 - val_accuracy: 0.4908\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.65 - 1s 662ms/step - loss: 0.2433 - accuracy: 0.6094 - val_loss: 0.2552 - val_accuracy: 0.4912\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.59 - 1s 721ms/step - loss: 0.2515 - accuracy: 0.5000 - val_loss: 0.2526 - val_accuracy: 0.4911\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.50 - 2s 794ms/step - loss: 0.2477 - accuracy: 0.5625 - val_loss: 0.2409 - val_accuracy: 0.4915\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.40 - 1s 682ms/step - loss: 0.2509 - accuracy: 0.5000 - val_loss: 0.2498 - val_accuracy: 0.4909\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.53 - 1s 661ms/step - loss: 0.2442 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4904\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.56 - 1s 663ms/step - loss: 0.2462 - accuracy: 0.5469 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.40 - 1s 669ms/step - loss: 0.2634 - accuracy: 0.4062 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.53 - 1s 728ms/step - loss: 0.2468 - accuracy: 0.5469 - val_loss: 0.2540 - val_accuracy: 0.4912\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.56 - 1s 683ms/step - loss: 0.2491 - accuracy: 0.5312 - val_loss: 0.2521 - val_accuracy: 0.4911\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.65 - 1s 673ms/step - loss: 0.2414 - accuracy: 0.6406 - val_loss: 0.2421 - val_accuracy: 0.4915\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.53 - 1s 664ms/step - loss: 0.2494 - accuracy: 0.5312 - val_loss: 0.2499 - val_accuracy: 0.4909\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.65 - 1s 661ms/step - loss: 0.2426 - accuracy: 0.6094 - val_loss: 0.2547 - val_accuracy: 0.4904\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.59 - 1s 730ms/step - loss: 0.2514 - accuracy: 0.5000 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.50 - 1s 692ms/step - loss: 0.2473 - accuracy: 0.5625 - val_loss: 0.2519 - val_accuracy: 0.4908\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.40 - 1s 669ms/step - loss: 0.2506 - accuracy: 0.5000 - val_loss: 0.2550 - val_accuracy: 0.4912\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.53 - 1s 665ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2528 - val_accuracy: 0.4911\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.56 - 1s 658ms/step - loss: 0.2474 - accuracy: 0.5469 - val_loss: 0.2415 - val_accuracy: 0.4915\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.40 - 1s 738ms/step - loss: 0.2606 - accuracy: 0.4062 - val_loss: 0.2496 - val_accuracy: 0.4909\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.53 - 1s 666ms/step - loss: 0.2478 - accuracy: 0.5469 - val_loss: 0.2538 - val_accuracy: 0.4904\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.56 - 1s 674ms/step - loss: 0.2508 - accuracy: 0.5312 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.65 - 1s 662ms/step - loss: 0.2396 - accuracy: 0.6406 - val_loss: 0.2515 - val_accuracy: 0.4908\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2492 - accuracy: 0.5312 - val_loss: 0.2552 - val_accuracy: 0.4912\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.65 - 1s 737ms/step - loss: 0.2426 - accuracy: 0.6094 - val_loss: 0.2532 - val_accuracy: 0.4911\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.59 - 1s 665ms/step - loss: 0.2521 - accuracy: 0.5000 - val_loss: 0.2413 - val_accuracy: 0.4915\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 687ms/step - loss: 0.2467 - accuracy: 0.5625 - val_loss: 0.2494 - val_accuracy: 0.4909\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.40 - 1s 662ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2541 - val_accuracy: 0.4904\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 654ms/step - loss: 0.2471 - accuracy: 0.5625 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.56 - 1s 738ms/step - loss: 0.2471 - accuracy: 0.5469 - val_loss: 0.2513 - val_accuracy: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.40 - 1s 658ms/step - loss: 0.2609 - accuracy: 0.4062 - val_loss: 0.2541 - val_accuracy: 0.4912\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 665ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2521 - val_accuracy: 0.4911\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2425 - val_accuracy: 0.4915\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.65 - 1s 669ms/step - loss: 0.2431 - accuracy: 0.6406 - val_loss: 0.2493 - val_accuracy: 0.4909\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 732ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2544 - val_accuracy: 0.4904\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.65 - 1s 685ms/step - loss: 0.2422 - accuracy: 0.6094 - val_loss: 0.2488 - val_accuracy: 0.4917\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.59 - 1s 679ms/step - loss: 0.2526 - accuracy: 0.5000 - val_loss: 0.2510 - val_accuracy: 0.4908\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.50 - 1s 664ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2558 - val_accuracy: 0.4912\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.40 - 1s 677ms/step - loss: 0.2507 - accuracy: 0.5000 - val_loss: 0.2522 - val_accuracy: 0.4911\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.53 - 2s 781ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2422 - val_accuracy: 0.4915\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.56 - 1s 745ms/step - loss: 0.2460 - accuracy: 0.5469 - val_loss: 0.2496 - val_accuracy: 0.4909\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.40 - 1s 678ms/step - loss: 0.2571 - accuracy: 0.4062 - val_loss: 0.2535 - val_accuracy: 0.4904\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.53 - 1s 676ms/step - loss: 0.2482 - accuracy: 0.5469 - val_loss: 0.2492 - val_accuracy: 0.4917\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.56 - 1s 719ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2503 - val_accuracy: 0.4908\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.65 - 1s 672ms/step - loss: 0.2389 - accuracy: 0.6406 - val_loss: 0.2550 - val_accuracy: 0.4912\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.53 - 1s 666ms/step - loss: 0.2477 - accuracy: 0.5312 - val_loss: 0.2522 - val_accuracy: 0.4911\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.65 - 1s 679ms/step - loss: 0.2415 - accuracy: 0.6094 - val_loss: 0.2407 - val_accuracy: 0.4915\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.59 - 1s 669ms/step - loss: 0.2516 - accuracy: 0.5000 - val_loss: 0.2497 - val_accuracy: 0.4909\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.50 - 2s 786ms/step - loss: 0.2465 - accuracy: 0.5625 - val_loss: 0.2564 - val_accuracy: 0.4904\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.40 - 1s 667ms/step - loss: 0.2509 - accuracy: 0.5000 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2510 - val_accuracy: 0.4908\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.56 - 1s 669ms/step - loss: 0.2464 - accuracy: 0.5469 - val_loss: 0.2567 - val_accuracy: 0.4912\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.40 - 1s 725ms/step - loss: 0.2570 - accuracy: 0.4062 - val_loss: 0.2527 - val_accuracy: 0.4911\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 670ms/step - loss: 0.2484 - accuracy: 0.5469 - val_loss: 0.2414 - val_accuracy: 0.4915\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.56 - 1s 660ms/step - loss: 0.2482 - accuracy: 0.5312 - val_loss: 0.2491 - val_accuracy: 0.4909\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.65 - 1s 686ms/step - loss: 0.2427 - accuracy: 0.6406 - val_loss: 0.2557 - val_accuracy: 0.4904\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.53 - 1s 675ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2491 - val_accuracy: 0.4917\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.65 - 1s 735ms/step - loss: 0.2409 - accuracy: 0.6094 - val_loss: 0.2517 - val_accuracy: 0.4908\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.59 - 1s 683ms/step - loss: 0.2519 - accuracy: 0.5000 - val_loss: 0.2567 - val_accuracy: 0.4912\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 668ms/step - loss: 0.2469 - accuracy: 0.5625 - val_loss: 0.2543 - val_accuracy: 0.4911\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.40 - 1s 659ms/step - loss: 0.2508 - accuracy: 0.5000 - val_loss: 0.2396 - val_accuracy: 0.4915\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.53 - 1s 668ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2494 - val_accuracy: 0.4909\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.56 - 1s 730ms/step - loss: 0.2452 - accuracy: 0.5469 - val_loss: 0.2563 - val_accuracy: 0.4904\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.40 - 1s 659ms/step - loss: 0.2587 - accuracy: 0.4062 - val_loss: 0.2490 - val_accuracy: 0.4917\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 658ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.56 - 1s 661ms/step - loss: 0.2500 - accuracy: 0.5312 - val_loss: 0.2565 - val_accuracy: 0.4912\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.65 - 1s 677ms/step - loss: 0.2390 - accuracy: 0.6406 - val_loss: 0.2539 - val_accuracy: 0.4911\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 2s 760ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2394 - val_accuracy: 0.4915\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.65 - 1s 665ms/step - loss: 0.2425 - accuracy: 0.6094 - val_loss: 0.2495 - val_accuracy: 0.4909\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.59 - 1s 663ms/step - loss: 0.2527 - accuracy: 0.5000 - val_loss: 0.2556 - val_accuracy: 0.4904\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 682ms/step - loss: 0.2459 - accuracy: 0.5625 - val_loss: 0.2492 - val_accuracy: 0.4917\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.40 - 1s 674ms/step - loss: 0.2506 - accuracy: 0.5000 - val_loss: 0.2505 - val_accuracy: 0.4908\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.53 - 1s 726ms/step - loss: 0.2503 - accuracy: 0.5625 - val_loss: 0.2559 - val_accuracy: 0.4912\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.56 - 1s 672ms/step - loss: 0.2438 - accuracy: 0.5469 - val_loss: 0.2528 - val_accuracy: 0.4911\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.40 - 1s 656ms/step - loss: 0.2581 - accuracy: 0.4062 - val_loss: 0.2421 - val_accuracy: 0.4915\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.53 - 1s 655ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2493 - val_accuracy: 0.4909\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.56 - 1s 680ms/step - loss: 0.2503 - accuracy: 0.5312 - val_loss: 0.2545 - val_accuracy: 0.4904\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.65 - 1s 740ms/step - loss: 0.2381 - accuracy: 0.6406 - val_loss: 0.2494 - val_accuracy: 0.4917\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.53 - 1s 662ms/step - loss: 0.2480 - accuracy: 0.5312 - val_loss: 0.2508 - val_accuracy: 0.4908\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.65 - 1s 670ms/step - loss: 0.2419 - accuracy: 0.6094 - val_loss: 0.2573 - val_accuracy: 0.4912\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.59 - 1s 670ms/step - loss: 0.2521 - accuracy: 0.5000 - val_loss: 0.2531 - val_accuracy: 0.4911\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 674ms/step - loss: 0.2464 - accuracy: 0.5625 - val_loss: 0.2398 - val_accuracy: 0.4915\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.40 - 1s 744ms/step - loss: 0.2500 - accuracy: 0.5000 - val_loss: 0.2495 - val_accuracy: 0.4909\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.53 - 1s 671ms/step - loss: 0.2489 - accuracy: 0.5625 - val_loss: 0.2550 - val_accuracy: 0.4904\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2422 - accuracy: 0.5469 - val_loss: 0.2495 - val_accuracy: 0.4917\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.40 - 1s 660ms/step - loss: 0.2582 - accuracy: 0.4062 - val_loss: 0.2500 - val_accuracy: 0.4908\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.53 - 1s 674ms/step - loss: 0.2499 - accuracy: 0.5469 - val_loss: 0.2554 - val_accuracy: 0.4912\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.56 - 1s 749ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2521 - val_accuracy: 0.4911\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.65 - 1s 666ms/step - loss: 0.2385 - accuracy: 0.6406 - val_loss: 0.2410 - val_accuracy: 0.4915\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 667ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2496 - val_accuracy: 0.4909\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.65 - 2s 782ms/step - loss: 0.2399 - accuracy: 0.6094 - val_loss: 0.2556 - val_accuracy: 0.4904\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.59 - 2s 810ms/step - loss: 0.2523 - accuracy: 0.5000 - val_loss: 0.2495 - val_accuracy: 0.4917\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.50 - 1s 693ms/step - loss: 0.2451 - accuracy: 0.5625 - val_loss: 0.2504 - val_accuracy: 0.4908\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.40 - 1s 672ms/step - loss: 0.2519 - accuracy: 0.5000 - val_loss: 0.2561 - val_accuracy: 0.4912\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.53 - 1s 712ms/step - loss: 0.2492 - accuracy: 0.5625 - val_loss: 0.2526 - val_accuracy: 0.4911\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.56 - 2s 793ms/step - loss: 0.2442 - accuracy: 0.5469 - val_loss: 0.2411 - val_accuracy: 0.4915\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.40 - 1s 717ms/step - loss: 0.2572 - accuracy: 0.4062 - val_loss: 0.2493 - val_accuracy: 0.4909\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.53 - 1s 674ms/step - loss: 0.2476 - accuracy: 0.5469 - val_loss: 0.2541 - val_accuracy: 0.4904\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.56 - 1s 696ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2494 - val_accuracy: 0.4917\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.65 - 1s 682ms/step - loss: 0.2390 - accuracy: 0.6406 - val_loss: 0.2498 - val_accuracy: 0.4908\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.53 - 1s 678ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2558 - val_accuracy: 0.4912\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.65 - 1s 691ms/step - loss: 0.2403 - accuracy: 0.6094 - val_loss: 0.2526 - val_accuracy: 0.4911\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.59 - 1s 668ms/step - loss: 0.2514 - accuracy: 0.5000 - val_loss: 0.2416 - val_accuracy: 0.4871\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.50 - 1s 726ms/step - loss: 0.2457 - accuracy: 0.5625 - val_loss: 0.2491 - val_accuracy: 0.4909\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.40 - 1s 706ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2548 - val_accuracy: 0.4905\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.53 - 1s 680ms/step - loss: 0.2487 - accuracy: 0.5625 - val_loss: 0.2495 - val_accuracy: 0.4873\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.56 - 1s 659ms/step - loss: 0.2445 - accuracy: 0.5469 - val_loss: 0.2495 - val_accuracy: 0.4908\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.40 - 1s 668ms/step - loss: 0.2574 - accuracy: 0.4062 - val_loss: 0.2554 - val_accuracy: 0.4912\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.53 - 1s 730ms/step - loss: 0.2475 - accuracy: 0.5469 - val_loss: 0.2519 - val_accuracy: 0.4909\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.56 - 1s 670ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2423 - val_accuracy: 0.5004\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.65 - 1s 690ms/step - loss: 0.2378 - accuracy: 0.6406 - val_loss: 0.2491 - val_accuracy: 0.4864\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.53 - 1s 664ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2547 - val_accuracy: 0.4817\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.65 - 1s 668ms/step - loss: 0.2399 - accuracy: 0.6094 - val_loss: 0.2496 - val_accuracy: 0.4917\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.59 - 1s 741ms/step - loss: 0.2518 - accuracy: 0.5000 - val_loss: 0.2491 - val_accuracy: 0.4862\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.50 - 1s 666ms/step - loss: 0.2442 - accuracy: 0.5625 - val_loss: 0.2591 - val_accuracy: 0.4956\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.40 - 1s 691ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2526 - val_accuracy: 0.4820\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.53 - 1s 696ms/step - loss: 0.2499 - accuracy: 0.5625 - val_loss: 0.2415 - val_accuracy: 0.4828\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.56 - 1s 732ms/step - loss: 0.2413 - accuracy: 0.5469 - val_loss: 0.2489 - val_accuracy: 0.4862\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00178: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Bilding and training LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=200,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5859375\n",
      "test_loss: 0.24482347071170807\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - ETA: 4s - loss: 0.2520 - accuracy: 0.43 - 6s 3s/step - loss: 0.2598 - accuracy: 0.3906 - val_loss: 0.2517 - val_accuracy: 0.4773\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.43 - 1s 632ms/step - loss: 0.2541 - accuracy: 0.4688 - val_loss: 0.2481 - val_accuracy: 0.5271\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.65 - 1s 669ms/step - loss: 0.2441 - accuracy: 0.5938 - val_loss: 0.2504 - val_accuracy: 0.4864\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.43 - 1s 600ms/step - loss: 0.2595 - accuracy: 0.4844 - val_loss: 0.2510 - val_accuracy: 0.5395\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.56 - 1s 612ms/step - loss: 0.2495 - accuracy: 0.5312 - val_loss: 0.2483 - val_accuracy: 0.5501\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.40 - 1s 626ms/step - loss: 0.2559 - accuracy: 0.4688 - val_loss: 0.2491 - val_accuracy: 0.5399\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.56 - 1s 600ms/step - loss: 0.2498 - accuracy: 0.5625 - val_loss: 0.2478 - val_accuracy: 0.5354\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.53 - 1s 602ms/step - loss: 0.2545 - accuracy: 0.5000 - val_loss: 0.2501 - val_accuracy: 0.5491\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.46 - 1s 602ms/step - loss: 0.2471 - accuracy: 0.5000 - val_loss: 0.2486 - val_accuracy: 0.5531\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.59 - 1s 609ms/step - loss: 0.2449 - accuracy: 0.5938 - val_loss: 0.2487 - val_accuracy: 0.5491\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.56 - 1s 615ms/step - loss: 0.2452 - accuracy: 0.6250 - val_loss: 0.2509 - val_accuracy: 0.5622\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.53 - 1s 618ms/step - loss: 0.2459 - accuracy: 0.5625 - val_loss: 0.2504 - val_accuracy: 0.4957\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.50 - 1s 613ms/step - loss: 0.2476 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.5757\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.46 - 1s 693ms/step - loss: 0.2579 - accuracy: 0.3750 - val_loss: 0.2466 - val_accuracy: 0.5577\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.46 - 1s 610ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2493 - val_accuracy: 0.4997\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.56 - 1s 603ms/step - loss: 0.2498 - accuracy: 0.5625 - val_loss: 0.2506 - val_accuracy: 0.5037\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.53 - 1s 595ms/step - loss: 0.2530 - accuracy: 0.4688 - val_loss: 0.2485 - val_accuracy: 0.5048\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.43 - 1s 648ms/step - loss: 0.2541 - accuracy: 0.4844 - val_loss: 0.2522 - val_accuracy: 0.4908\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.50 - 1s 644ms/step - loss: 0.2505 - accuracy: 0.4844 - val_loss: 0.2515 - val_accuracy: 0.4912\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.59 - 1s 583ms/step - loss: 0.2443 - accuracy: 0.6562 - val_loss: 0.2513 - val_accuracy: 0.4911\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.53 - 1s 646ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2428 - val_accuracy: 0.4961\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.56 - 1s 624ms/step - loss: 0.2442 - accuracy: 0.5938 - val_loss: 0.2493 - val_accuracy: 0.4953\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.62 - 1s 723ms/step - loss: 0.2518 - accuracy: 0.5156 - val_loss: 0.2559 - val_accuracy: 0.4904\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.50 - 1s 709ms/step - loss: 0.2521 - accuracy: 0.4844 - val_loss: 0.2484 - val_accuracy: 0.4917\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.56 - 1s 620ms/step - loss: 0.2565 - accuracy: 0.4375 - val_loss: 0.2523 - val_accuracy: 0.4908\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.59 - 1s 608ms/step - loss: 0.2548 - accuracy: 0.5625 - val_loss: 0.2536 - val_accuracy: 0.4912\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.62 - 1s 622ms/step - loss: 0.2410 - accuracy: 0.6094 - val_loss: 0.2531 - val_accuracy: 0.4911\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.46 - 1s 627ms/step - loss: 0.2615 - accuracy: 0.4219 - val_loss: 0.2401 - val_accuracy: 0.4915\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.46 - 1s 634ms/step - loss: 0.2547 - accuracy: 0.4844 - val_loss: 0.2506 - val_accuracy: 0.4909\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.62 - 1s 670ms/step - loss: 0.2538 - accuracy: 0.5469 - val_loss: 0.2545 - val_accuracy: 0.4904\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.50 - 1s 636ms/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2483 - val_accuracy: 0.4917\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.53 - 1s 609ms/step - loss: 0.2532 - accuracy: 0.5312 - val_loss: 0.2532 - val_accuracy: 0.4908\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.53 - 1s 639ms/step - loss: 0.2453 - accuracy: 0.6094 - val_loss: 0.2546 - val_accuracy: 0.4912\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.56 - 1s 595ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2567 - val_accuracy: 0.4911\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.43 - 1s 616ms/step - loss: 0.2560 - accuracy: 0.4844 - val_loss: 0.2425 - val_accuracy: 0.4915\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.56 - 1s 620ms/step - loss: 0.2518 - accuracy: 0.5000 - val_loss: 0.2482 - val_accuracy: 0.4909\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.56 - 1s 616ms/step - loss: 0.2467 - accuracy: 0.5625 - val_loss: 0.2535 - val_accuracy: 0.4904\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.56 - 1s 644ms/step - loss: 0.2469 - accuracy: 0.5781 - val_loss: 0.2487 - val_accuracy: 0.4917\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.53 - 1s 625ms/step - loss: 0.2530 - accuracy: 0.5000 - val_loss: 0.2519 - val_accuracy: 0.4908\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.46 - 1s 612ms/step - loss: 0.2540 - accuracy: 0.4844 - val_loss: 0.2535 - val_accuracy: 0.4912\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.43 - 1s 711ms/step - loss: 0.2453 - accuracy: 0.5312 - val_loss: 0.2512 - val_accuracy: 0.4911\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 1s 644ms/step - loss: 0.2453 - accuracy: 0.5781 - val_loss: 0.2414 - val_accuracy: 0.4915\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.62 - 1s 601ms/step - loss: 0.2441 - accuracy: 0.5625 - val_loss: 0.2481 - val_accuracy: 0.4909\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 1s 616ms/step - loss: 0.2456 - accuracy: 0.5781 - val_loss: 0.2527 - val_accuracy: 0.4904\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.56 - 1s 718ms/step - loss: 0.2493 - accuracy: 0.5469 - val_loss: 0.2491 - val_accuracy: 0.4917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.40 - 1s 711ms/step - loss: 0.2556 - accuracy: 0.4688 - val_loss: 0.2505 - val_accuracy: 0.5538\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.46 - 1s 677ms/step - loss: 0.2512 - accuracy: 0.4531 - val_loss: 0.2519 - val_accuracy: 0.5175\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.62 - 1s 605ms/step - loss: 0.2458 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.5001\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.62 - 1s 593ms/step - loss: 0.2419 - accuracy: 0.5781 - val_loss: 0.2429 - val_accuracy: 0.5003\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.50 - 1s 598ms/step - loss: 0.2628 - accuracy: 0.4688 - val_loss: 0.2481 - val_accuracy: 0.4909\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.43 - 1s 603ms/step - loss: 0.2488 - accuracy: 0.5156 - val_loss: 0.2521 - val_accuracy: 0.4904\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.50 - 1s 601ms/step - loss: 0.2479 - accuracy: 0.5469 - val_loss: 0.2488 - val_accuracy: 0.4917\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.50 - 1s 601ms/step - loss: 0.2557 - accuracy: 0.5000 - val_loss: 0.2514 - val_accuracy: 0.4908\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.68 - 1s 605ms/step - loss: 0.2463 - accuracy: 0.6250 - val_loss: 0.2533 - val_accuracy: 0.4912\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.53 - 1s 619ms/step - loss: 0.2464 - accuracy: 0.5938 - val_loss: 0.2507 - val_accuracy: 0.4911\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.56 - 1s 633ms/step - loss: 0.2507 - accuracy: 0.5312 - val_loss: 0.2379 - val_accuracy: 0.4915\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.40 - 1s 643ms/step - loss: 0.2547 - accuracy: 0.4219 - val_loss: 0.2483 - val_accuracy: 0.4909\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.65 - 1s 696ms/step - loss: 0.2531 - accuracy: 0.4844 - val_loss: 0.2525 - val_accuracy: 0.4904\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.59 - 1s 603ms/step - loss: 0.2494 - accuracy: 0.5469 - val_loss: 0.2487 - val_accuracy: 0.4917\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.65 - 1s 592ms/step - loss: 0.2460 - accuracy: 0.6094 - val_loss: 0.2524 - val_accuracy: 0.4908\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.53 - 1s 594ms/step - loss: 0.2536 - accuracy: 0.4688 - val_loss: 0.2555 - val_accuracy: 0.4912\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.50 - 1s 591ms/step - loss: 0.2548 - accuracy: 0.4844 - val_loss: 0.2532 - val_accuracy: 0.4911\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.59 - 1s 597ms/step - loss: 0.2466 - accuracy: 0.5312 - val_loss: 0.2391 - val_accuracy: 0.4915\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.50 - 1s 595ms/step - loss: 0.2470 - accuracy: 0.6094 - val_loss: 0.2489 - val_accuracy: 0.4909\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.53 - 1s 604ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2557 - val_accuracy: 0.4904\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.53 - 1s 597ms/step - loss: 0.2447 - accuracy: 0.5938 - val_loss: 0.2486 - val_accuracy: 0.4917\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.53 - 1s 588ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2536 - val_accuracy: 0.4908\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.37 - 1s 598ms/step - loss: 0.2570 - accuracy: 0.4375 - val_loss: 0.2539 - val_accuracy: 0.4912\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 595ms/step - loss: 0.2537 - accuracy: 0.4688 - val_loss: 0.2507 - val_accuracy: 0.4911\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.59 - 1s 593ms/step - loss: 0.2468 - accuracy: 0.5469 - val_loss: 0.2406 - val_accuracy: 0.4915\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.53 - 1s 623ms/step - loss: 0.2438 - accuracy: 0.5469 - val_loss: 0.2488 - val_accuracy: 0.4909\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.50 - 1s 605ms/step - loss: 0.2554 - accuracy: 0.4531 - val_loss: 0.2543 - val_accuracy: 0.4904\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.56 - 1s 657ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2494 - val_accuracy: 0.4917\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.56 - 1s 595ms/step - loss: 0.2503 - accuracy: 0.5781 - val_loss: 0.2516 - val_accuracy: 0.4908\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.50 - 1s 587ms/step - loss: 0.2502 - accuracy: 0.5312 - val_loss: 0.2557 - val_accuracy: 0.4912\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.56 - 1s 596ms/step - loss: 0.2490 - accuracy: 0.5469 - val_loss: 0.2519 - val_accuracy: 0.4911\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.53 - 1s 600ms/step - loss: 0.2458 - accuracy: 0.5938 - val_loss: 0.2392 - val_accuracy: 0.4915\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.56 - 1s 593ms/step - loss: 0.2447 - accuracy: 0.5625 - val_loss: 0.2491 - val_accuracy: 0.4909\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.40 - 1s 594ms/step - loss: 0.2481 - accuracy: 0.4375 - val_loss: 0.2517 - val_accuracy: 0.5043\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.65 - 1s 594ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2505 - val_accuracy: 0.5135\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.56 - 1s 601ms/step - loss: 0.2508 - accuracy: 0.5469 - val_loss: 0.2498 - val_accuracy: 0.4956\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.62 - 1s 589ms/step - loss: 0.2480 - accuracy: 0.5781 - val_loss: 0.2539 - val_accuracy: 0.4996\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 607ms/step - loss: 0.2535 - accuracy: 0.4844 - val_loss: 0.2497 - val_accuracy: 0.5003\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.43 - 1s 603ms/step - loss: 0.2557 - accuracy: 0.4688 - val_loss: 0.2431 - val_accuracy: 0.4913\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.46 - 1s 603ms/step - loss: 0.2557 - accuracy: 0.4531 - val_loss: 0.2492 - val_accuracy: 0.4905\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.40 - 1s 594ms/step - loss: 0.2535 - accuracy: 0.5156 - val_loss: 0.2519 - val_accuracy: 0.5087\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.62 - 1s 600ms/step - loss: 0.2447 - accuracy: 0.5938 - val_loss: 0.2503 - val_accuracy: 0.5091\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.53 - 1s 599ms/step - loss: 0.2441 - accuracy: 0.5938 - val_loss: 0.2498 - val_accuracy: 0.5183\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.59 - 1s 688ms/step - loss: 0.2442 - accuracy: 0.6250 - val_loss: 0.2563 - val_accuracy: 0.5044\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.43 - 1s 699ms/step - loss: 0.2539 - accuracy: 0.4531 - val_loss: 0.2496 - val_accuracy: 0.4912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.65 - 1s 663ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2440 - val_accuracy: 0.5136\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.59 - 1s 702ms/step - loss: 0.2504 - accuracy: 0.5781 - val_loss: 0.2495 - val_accuracy: 0.4911\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.59 - 1s 657ms/step - loss: 0.2415 - accuracy: 0.5781 - val_loss: 0.2530 - val_accuracy: 0.4858\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.46 - 1s 732ms/step - loss: 0.2626 - accuracy: 0.4531 - val_loss: 0.2501 - val_accuracy: 0.4961\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.56 - 1s 658ms/step - loss: 0.2532 - accuracy: 0.5469 - val_loss: 0.2507 - val_accuracy: 0.4953\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.50 - 1s 648ms/step - loss: 0.2527 - accuracy: 0.4688 - val_loss: 0.2548 - val_accuracy: 0.5000\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.59 - 1s 648ms/step - loss: 0.2494 - accuracy: 0.5781 - val_loss: 0.2504 - val_accuracy: 0.5001\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.62 - 1s 548ms/step - loss: 0.2478 - accuracy: 0.5781 - val_loss: 0.2415 - val_accuracy: 0.5003\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.53 - 1s 661ms/step - loss: 0.2437 - accuracy: 0.5938 - val_loss: 0.2491 - val_accuracy: 0.5000\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.56 - 1s 621ms/step - loss: 0.2488 - accuracy: 0.5625 - val_loss: 0.2574 - val_accuracy: 0.4904\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.40 - 1s 715ms/step - loss: 0.2567 - accuracy: 0.4375 - val_loss: 0.2501 - val_accuracy: 0.5094\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.62 - 1s 670ms/step - loss: 0.2499 - accuracy: 0.5156 - val_loss: 0.2505 - val_accuracy: 0.4953\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.59 - 1s 603ms/step - loss: 0.2462 - accuracy: 0.5625 - val_loss: 0.2558 - val_accuracy: 0.5132\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.59 - 1s 607ms/step - loss: 0.2458 - accuracy: 0.5625 - val_loss: 0.2509 - val_accuracy: 0.5047\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.50 - 1s 610ms/step - loss: 0.2545 - accuracy: 0.4062 - val_loss: 0.2402 - val_accuracy: 0.4915\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 599ms/step - loss: 0.2506 - accuracy: 0.5156 - val_loss: 0.2487 - val_accuracy: 0.4909\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.53 - 1s 593ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2548 - val_accuracy: 0.4904\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.59 - 1s 607ms/step - loss: 0.2446 - accuracy: 0.6250 - val_loss: 0.2496 - val_accuracy: 0.4917\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.53 - 1s 584ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2511 - val_accuracy: 0.4908\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 592ms/step - loss: 0.2445 - accuracy: 0.5938 - val_loss: 0.2572 - val_accuracy: 0.4912\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.56 - 1s 605ms/step - loss: 0.2472 - accuracy: 0.5469 - val_loss: 0.2540 - val_accuracy: 0.4911\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.43 - 1s 598ms/step - loss: 0.2533 - accuracy: 0.4688 - val_loss: 0.2388 - val_accuracy: 0.4915\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.62 - 1s 607ms/step - loss: 0.2489 - accuracy: 0.5156 - val_loss: 0.2479 - val_accuracy: 0.4909\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.56 - 1s 661ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2570 - val_accuracy: 0.4904\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.62 - 1s 596ms/step - loss: 0.2430 - accuracy: 0.6094 - val_loss: 0.2498 - val_accuracy: 0.4917\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.53 - 1s 589ms/step - loss: 0.2536 - accuracy: 0.4688 - val_loss: 0.2524 - val_accuracy: 0.4908\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.53 - 1s 594ms/step - loss: 0.2527 - accuracy: 0.5000 - val_loss: 0.2586 - val_accuracy: 0.4912\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.56 - 1s 630ms/step - loss: 0.2480 - accuracy: 0.5156 - val_loss: 0.2530 - val_accuracy: 0.4911\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.56 - 1s 594ms/step - loss: 0.2412 - accuracy: 0.6406 - val_loss: 0.2376 - val_accuracy: 0.4915\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.62 - 1s 590ms/step - loss: 0.2440 - accuracy: 0.5781 - val_loss: 0.2479 - val_accuracy: 0.4909\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.53 - 1s 595ms/step - loss: 0.2432 - accuracy: 0.5938 - val_loss: 0.2573 - val_accuracy: 0.4904\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.56 - 1s 602ms/step - loss: 0.2459 - accuracy: 0.5781 - val_loss: 0.2508 - val_accuracy: 0.4917\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.43 - 1s 587ms/step - loss: 0.2555 - accuracy: 0.4688 - val_loss: 0.2519 - val_accuracy: 0.4908\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.62 - 1s 585ms/step - loss: 0.2507 - accuracy: 0.5156 - val_loss: 0.2568 - val_accuracy: 0.5132\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.59 - 1s 590ms/step - loss: 0.2525 - accuracy: 0.5625 - val_loss: 0.2521 - val_accuracy: 0.4911\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.56 - 1s 584ms/step - loss: 0.2420 - accuracy: 0.5938 - val_loss: 0.2383 - val_accuracy: 0.4959\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 622ms/step - loss: 0.2515 - accuracy: 0.4844 - val_loss: 0.2475 - val_accuracy: 0.4909\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.46 - 1s 605ms/step - loss: 0.2534 - accuracy: 0.5000 - val_loss: 0.2555 - val_accuracy: 0.4904\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.62 - 1s 606ms/step - loss: 0.2455 - accuracy: 0.6250 - val_loss: 0.2504 - val_accuracy: 0.4917\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.53 - 1s 674ms/step - loss: 0.2409 - accuracy: 0.5938 - val_loss: 0.2511 - val_accuracy: 0.4908\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.62 - 1s 595ms/step - loss: 0.2470 - accuracy: 0.5781 - val_loss: 0.2580 - val_accuracy: 0.4956\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.53 - 1s 590ms/step - loss: 0.2439 - accuracy: 0.5938 - val_loss: 0.2513 - val_accuracy: 0.5092\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.56 - 1s 592ms/step - loss: 0.2463 - accuracy: 0.5312 - val_loss: 0.2367 - val_accuracy: 0.5003\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.40 - 1s 595ms/step - loss: 0.2578 - accuracy: 0.4531 - val_loss: 0.2473 - val_accuracy: 0.5088\n",
      "Epoch 135/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.59 - 1s 601ms/step - loss: 0.2529 - accuracy: 0.5312 - val_loss: 0.2527 - val_accuracy: 0.5127\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.65 - 1s 586ms/step - loss: 0.2527 - accuracy: 0.5781 - val_loss: 0.2512 - val_accuracy: 0.5358\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.59 - 1s 589ms/step - loss: 0.2509 - accuracy: 0.5781 - val_loss: 0.2503 - val_accuracy: 0.5317\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.53 - 1s 590ms/step - loss: 0.2604 - accuracy: 0.4688 - val_loss: 0.2578 - val_accuracy: 0.5176\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.43 - 1s 585ms/step - loss: 0.2527 - accuracy: 0.4844 - val_loss: 0.2512 - val_accuracy: 0.5229\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.50 - 1s 594ms/step - loss: 0.2457 - accuracy: 0.5312 - val_loss: 0.2394 - val_accuracy: 0.5179\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.50 - 1s 602ms/step - loss: 0.2447 - accuracy: 0.5781 - val_loss: 0.2474 - val_accuracy: 0.5226\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.65 - 1s 592ms/step - loss: 0.2449 - accuracy: 0.5938 - val_loss: 0.2547 - val_accuracy: 0.5257\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 593ms/step - loss: 0.2428 - accuracy: 0.5938 - val_loss: 0.2509 - val_accuracy: 0.5270\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.59 - 1s 586ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2530 - val_accuracy: 0.4908\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.40 - 1s 590ms/step - loss: 0.2560 - accuracy: 0.4531 - val_loss: 0.2592 - val_accuracy: 0.5000\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.62 - 1s 615ms/step - loss: 0.2502 - accuracy: 0.5312 - val_loss: 0.2506 - val_accuracy: 0.5050\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.59 - 1s 673ms/step - loss: 0.2433 - accuracy: 0.5625 - val_loss: 0.2389 - val_accuracy: 0.5135\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.56 - 1s 607ms/step - loss: 0.2512 - accuracy: 0.5469 - val_loss: 0.2479 - val_accuracy: 0.5135\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.53 - 1s 636ms/step - loss: 0.2590 - accuracy: 0.4688 - val_loss: 0.2561 - val_accuracy: 0.5125\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.43 - 1s 620ms/step - loss: 0.2567 - accuracy: 0.4531 - val_loss: 0.2506 - val_accuracy: 0.5094\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.56 - 1s 592ms/step - loss: 0.2450 - accuracy: 0.5938 - val_loss: 0.2507 - val_accuracy: 0.5180\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.50 - 1s 600ms/step - loss: 0.2488 - accuracy: 0.5781 - val_loss: 0.2586 - val_accuracy: 0.5132\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.62 - 1s 605ms/step - loss: 0.2455 - accuracy: 0.5781 - val_loss: 0.2518 - val_accuracy: 0.5092\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 631ms/step - loss: 0.2428 - accuracy: 0.5938 - val_loss: 0.2383 - val_accuracy: 0.4959\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.56 - 1s 614ms/step - loss: 0.2427 - accuracy: 0.5781 - val_loss: 0.2476 - val_accuracy: 0.5000\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.43 - 1s 627ms/step - loss: 0.2514 - accuracy: 0.4688 - val_loss: 0.2536 - val_accuracy: 0.4906\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.56 - 1s 702ms/step - loss: 0.2540 - accuracy: 0.5000 - val_loss: 0.2510 - val_accuracy: 0.4999\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.65 - 1s 596ms/step - loss: 0.2501 - accuracy: 0.5938 - val_loss: 0.2490 - val_accuracy: 0.4913\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.56 - 1s 587ms/step - loss: 0.2436 - accuracy: 0.5625 - val_loss: 0.2549 - val_accuracy: 0.4953\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.53 - 1s 679ms/step - loss: 0.2554 - accuracy: 0.4531 - val_loss: 0.2486 - val_accuracy: 0.5004\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.37 - 1s 619ms/step - loss: 0.2539 - accuracy: 0.4844 - val_loss: 0.2432 - val_accuracy: 0.5003\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.56 - 1s 608ms/step - loss: 0.2470 - accuracy: 0.5781 - val_loss: 0.2476 - val_accuracy: 0.4993\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.56 - 1s 589ms/step - loss: 0.2470 - accuracy: 0.5469 - val_loss: 0.2521 - val_accuracy: 0.4952\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.62 - 1s 591ms/step - loss: 0.2455 - accuracy: 0.5781 - val_loss: 0.2503 - val_accuracy: 0.5003\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.53 - 1s 600ms/step - loss: 0.2413 - accuracy: 0.5938 - val_loss: 0.2492 - val_accuracy: 0.5001\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.59 - 1s 607ms/step - loss: 0.2485 - accuracy: 0.5000 - val_loss: 0.2599 - val_accuracy: 0.4956\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.37 - 1s 626ms/step - loss: 0.2530 - accuracy: 0.4531 - val_loss: 0.2495 - val_accuracy: 0.5092\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.59 - 1s 592ms/step - loss: 0.2542 - accuracy: 0.5156 - val_loss: 0.2412 - val_accuracy: 0.5135\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.59 - 1s 597ms/step - loss: 0.2495 - accuracy: 0.5781 - val_loss: 0.2472 - val_accuracy: 0.5091\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.56 - 1s 679ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2553 - val_accuracy: 0.5036\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.53 - 1s 614ms/step - loss: 0.2544 - accuracy: 0.4531 - val_loss: 0.2502 - val_accuracy: 0.4917\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.37 - 1s 620ms/step - loss: 0.2581 - accuracy: 0.4219 - val_loss: 0.2511 - val_accuracy: 0.4908\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.50 - 1s 606ms/step - loss: 0.2495 - accuracy: 0.4844 - val_loss: 0.2586 - val_accuracy: 0.4912\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 604ms/step - loss: 0.2456 - accuracy: 0.5469 - val_loss: 0.2523 - val_accuracy: 0.4911\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.62 - 1s 597ms/step - loss: 0.2467 - accuracy: 0.5781 - val_loss: 0.2379 - val_accuracy: 0.4915\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 637ms/step - loss: 0.2434 - accuracy: 0.5938 - val_loss: 0.2475 - val_accuracy: 0.4909\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.56 - 1s 610ms/step - loss: 0.2454 - accuracy: 0.5938 - val_loss: 0.2600 - val_accuracy: 0.4904\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.37 - 1s 650ms/step - loss: 0.2581 - accuracy: 0.4375 - val_loss: 0.2499 - val_accuracy: 0.4917\n",
      "Epoch 179/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.62 - 1s 589ms/step - loss: 0.2523 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.4999\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.59 - 1s 673ms/step - loss: 0.2481 - accuracy: 0.5469 - val_loss: 0.2568 - val_accuracy: 0.4956\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.59 - 1s 595ms/step - loss: 0.2444 - accuracy: 0.5938 - val_loss: 0.2513 - val_accuracy: 0.4956\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.53 - 1s 591ms/step - loss: 0.2535 - accuracy: 0.4688 - val_loss: 0.2394 - val_accuracy: 0.4915\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.37 - 1s 593ms/step - loss: 0.2522 - accuracy: 0.4688 - val_loss: 0.2486 - val_accuracy: 0.4909\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.37 - 1s 591ms/step - loss: 0.2496 - accuracy: 0.4844 - val_loss: 0.2555 - val_accuracy: 0.4904\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.56 - 1s 588ms/step - loss: 0.2456 - accuracy: 0.6250 - val_loss: 0.2496 - val_accuracy: 0.4917\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.62 - 1s 603ms/step - loss: 0.2475 - accuracy: 0.5781 - val_loss: 0.2509 - val_accuracy: 0.4908\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 591ms/step - loss: 0.2442 - accuracy: 0.5938 - val_loss: 0.2583 - val_accuracy: 0.4912\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.56 - 1s 591ms/step - loss: 0.2461 - accuracy: 0.5781 - val_loss: 0.2566 - val_accuracy: 0.4911\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.37 - 1s 587ms/step - loss: 0.2619 - accuracy: 0.4219 - val_loss: 0.2388 - val_accuracy: 0.4915\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.62 - 1s 600ms/step - loss: 0.2540 - accuracy: 0.5000 - val_loss: 0.2486 - val_accuracy: 0.4909\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.59 - 1s 601ms/step - loss: 0.2535 - accuracy: 0.5625 - val_loss: 0.2574 - val_accuracy: 0.4904\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.59 - 1s 632ms/step - loss: 0.2462 - accuracy: 0.5781 - val_loss: 0.2495 - val_accuracy: 0.4917\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.53 - 1s 623ms/step - loss: 0.2532 - accuracy: 0.4844 - val_loss: 0.2512 - val_accuracy: 0.4908\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.43 - 1s 586ms/step - loss: 0.2513 - accuracy: 0.4688 - val_loss: 0.2587 - val_accuracy: 0.4912\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.59 - 1s 589ms/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2533 - val_accuracy: 0.4911\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.50 - 1s 668ms/step - loss: 0.2420 - accuracy: 0.5781 - val_loss: 0.2377 - val_accuracy: 0.4915\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.59 - 1s 600ms/step - loss: 0.2446 - accuracy: 0.5625 - val_loss: 0.2487 - val_accuracy: 0.4909\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 590ms/step - loss: 0.2444 - accuracy: 0.5938 - val_loss: 0.2575 - val_accuracy: 0.4904\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.56 - 1s 588ms/step - loss: 0.2474 - accuracy: 0.5781 - val_loss: 0.2494 - val_accuracy: 0.4917\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.40 - 1s 602ms/step - loss: 0.2562 - accuracy: 0.4531 - val_loss: 0.2507 - val_accuracy: 0.4908\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.62 - 1s 597ms/step - loss: 0.2506 - accuracy: 0.5156 - val_loss: 0.2568 - val_accuracy: 0.4912\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.59 - 1s 585ms/step - loss: 0.2465 - accuracy: 0.5625 - val_loss: 0.2529 - val_accuracy: 0.4911\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.59 - 1s 607ms/step - loss: 0.2393 - accuracy: 0.5781 - val_loss: 0.2391 - val_accuracy: 0.4915\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.53 - 1s 614ms/step - loss: 0.2599 - accuracy: 0.4688 - val_loss: 0.2488 - val_accuracy: 0.4909\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.37 - 1s 596ms/step - loss: 0.2530 - accuracy: 0.4531 - val_loss: 0.2554 - val_accuracy: 0.4904\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.56 - 1s 592ms/step - loss: 0.2528 - accuracy: 0.5312 - val_loss: 0.2488 - val_accuracy: 0.4917\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 591ms/step - loss: 0.2432 - accuracy: 0.6094 - val_loss: 0.2513 - val_accuracy: 0.4908\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.62 - 1s 586ms/step - loss: 0.2447 - accuracy: 0.5781 - val_loss: 0.2571 - val_accuracy: 0.4912\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.53 - 1s 591ms/step - loss: 0.2435 - accuracy: 0.5938 - val_loss: 0.2527 - val_accuracy: 0.4911\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.56 - 1s 594ms/step - loss: 0.2443 - accuracy: 0.5781 - val_loss: 0.2359 - val_accuracy: 0.4915\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.40 - 1s 597ms/step - loss: 0.2557 - accuracy: 0.4531 - val_loss: 0.2485 - val_accuracy: 0.4909\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.65 - 1s 657ms/step - loss: 0.2532 - accuracy: 0.5312 - val_loss: 0.2529 - val_accuracy: 0.5172\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.56 - 1s 639ms/step - loss: 0.2443 - accuracy: 0.5469 - val_loss: 0.2488 - val_accuracy: 0.4873\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.62 - 1s 585ms/step - loss: 0.2448 - accuracy: 0.6250 - val_loss: 0.2505 - val_accuracy: 0.4862\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.53 - 1s 590ms/step - loss: 0.2511 - accuracy: 0.4688 - val_loss: 0.2559 - val_accuracy: 0.4912\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.50 - 1s 622ms/step - loss: 0.2542 - accuracy: 0.4688 - val_loss: 0.2517 - val_accuracy: 0.4911\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.37 - 1s 585ms/step - loss: 0.2528 - accuracy: 0.4219 - val_loss: 0.2403 - val_accuracy: 0.4915\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.53 - 1s 577ms/step - loss: 0.2439 - accuracy: 0.5938 - val_loss: 0.2492 - val_accuracy: 0.4909\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.56 - 1s 582ms/step - loss: 0.2465 - accuracy: 0.5469 - val_loss: 0.2549 - val_accuracy: 0.4904\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 585ms/step - loss: 0.2437 - accuracy: 0.5938 - val_loss: 0.2488 - val_accuracy: 0.4917\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.56 - 1s 588ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2531 - val_accuracy: 0.4908\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.40 - 1s 590ms/step - loss: 0.2519 - accuracy: 0.4531 - val_loss: 0.2574 - val_accuracy: 0.4912\n",
      "Epoch 223/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.62 - 1s 578ms/step - loss: 0.2531 - accuracy: 0.5156 - val_loss: 0.2507 - val_accuracy: 0.5183\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.59 - 1s 586ms/step - loss: 0.2466 - accuracy: 0.5625 - val_loss: 0.2392 - val_accuracy: 0.4915\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.56 - 1s 604ms/step - loss: 0.2435 - accuracy: 0.5781 - val_loss: 0.2488 - val_accuracy: 0.4909\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 595ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2555 - val_accuracy: 0.4904\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.50 - 1s 594ms/step - loss: 0.2477 - accuracy: 0.5625 - val_loss: 0.2489 - val_accuracy: 0.4917\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.59 - 1s 599ms/step - loss: 0.2464 - accuracy: 0.5625 - val_loss: 0.2508 - val_accuracy: 0.4908\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.53 - 1s 605ms/step - loss: 0.2454 - accuracy: 0.5781 - val_loss: 0.2569 - val_accuracy: 0.4912\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.62 - 1s 672ms/step - loss: 0.2448 - accuracy: 0.5781 - val_loss: 0.2514 - val_accuracy: 0.4911\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.53 - 1s 623ms/step - loss: 0.2471 - accuracy: 0.5938 - val_loss: 0.2394 - val_accuracy: 0.4915\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 605ms/step - loss: 0.2471 - accuracy: 0.5312 - val_loss: 0.2482 - val_accuracy: 0.4909\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.37 - 2s 776ms/step - loss: 0.2557 - accuracy: 0.4375 - val_loss: 0.2521 - val_accuracy: 0.5176\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 609ms/step - loss: 0.2566 - accuracy: 0.4844 - val_loss: 0.2495 - val_accuracy: 0.5091\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.59 - 1s 642ms/step - loss: 0.2505 - accuracy: 0.5469 - val_loss: 0.2494 - val_accuracy: 0.5182\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.59 - 1s 607ms/step - loss: 0.2481 - accuracy: 0.5781 - val_loss: 0.2558 - val_accuracy: 0.5263\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.53 - 1s 604ms/step - loss: 0.2595 - accuracy: 0.4531 - val_loss: 0.2500 - val_accuracy: 0.5275\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.37 - 1s 670ms/step - loss: 0.2587 - accuracy: 0.4062 - val_loss: 0.2418 - val_accuracy: 0.5312\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.56 - 1s 615ms/step - loss: 0.2457 - accuracy: 0.5625 - val_loss: 0.2480 - val_accuracy: 0.5311\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.46 - 1s 623ms/step - loss: 0.2424 - accuracy: 0.5938 - val_loss: 0.2532 - val_accuracy: 0.5262\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.62 - 1s 622ms/step - loss: 0.2453 - accuracy: 0.5781 - val_loss: 0.2492 - val_accuracy: 0.5268\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.53 - 1s 605ms/step - loss: 0.2411 - accuracy: 0.5938 - val_loss: 0.2497 - val_accuracy: 0.5273\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.56 - 1s 607ms/step - loss: 0.2464 - accuracy: 0.5781 - val_loss: 0.2602 - val_accuracy: 0.4912\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.43 - 1s 616ms/step - loss: 0.2496 - accuracy: 0.4688 - val_loss: 0.2502 - val_accuracy: 0.5229\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.56 - 1s 611ms/step - loss: 0.2528 - accuracy: 0.5000 - val_loss: 0.2409 - val_accuracy: 0.5402\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.65 - 1s 618ms/step - loss: 0.2474 - accuracy: 0.5781 - val_loss: 0.2465 - val_accuracy: 0.5401\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.53 - 1s 641ms/step - loss: 0.2476 - accuracy: 0.5469 - val_loss: 0.2529 - val_accuracy: 0.5443\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.53 - 1s 692ms/step - loss: 0.2591 - accuracy: 0.4219 - val_loss: 0.2494 - val_accuracy: 0.5401\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.43 - 1s 601ms/step - loss: 0.2518 - accuracy: 0.4688 - val_loss: 0.2496 - val_accuracy: 0.5363\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.46 - 1s 601ms/step - loss: 0.2544 - accuracy: 0.5000 - val_loss: 0.2559 - val_accuracy: 0.5351\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.50 - 1s 637ms/step - loss: 0.2428 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.5274\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.62 - 1s 625ms/step - loss: 0.2465 - accuracy: 0.5781 - val_loss: 0.2417 - val_accuracy: 0.5224\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.53 - 1s 606ms/step - loss: 0.2428 - accuracy: 0.5938 - val_loss: 0.2468 - val_accuracy: 0.5224\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.56 - 1s 621ms/step - loss: 0.2479 - accuracy: 0.5469 - val_loss: 0.2554 - val_accuracy: 0.4904\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.40 - 1s 602ms/step - loss: 0.2548 - accuracy: 0.4375 - val_loss: 0.2499 - val_accuracy: 0.5401\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.65 - 1s 599ms/step - loss: 0.2554 - accuracy: 0.5156 - val_loss: 0.2506 - val_accuracy: 0.4999\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.56 - 1s 597ms/step - loss: 0.2467 - accuracy: 0.5469 - val_loss: 0.2596 - val_accuracy: 0.4912\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.59 - 1s 602ms/step - loss: 0.2420 - accuracy: 0.5781 - val_loss: 0.2528 - val_accuracy: 0.4911\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.53 - 1s 663ms/step - loss: 0.2562 - accuracy: 0.4688 - val_loss: 0.2385 - val_accuracy: 0.4915\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.43 - 1s 594ms/step - loss: 0.2508 - accuracy: 0.4844 - val_loss: 0.2475 - val_accuracy: 0.4909\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.56 - 1s 596ms/step - loss: 0.2499 - accuracy: 0.5469 - val_loss: 0.2567 - val_accuracy: 0.4904\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.53 - 1s 598ms/step - loss: 0.2485 - accuracy: 0.5938 - val_loss: 0.2493 - val_accuracy: 0.4917\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.62 - 1s 604ms/step - loss: 0.2490 - accuracy: 0.5781 - val_loss: 0.2516 - val_accuracy: 0.4908\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.53 - 1s 600ms/step - loss: 0.2444 - accuracy: 0.5938 - val_loss: 0.2606 - val_accuracy: 0.4912\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.56 - 1s 624ms/step - loss: 0.2454 - accuracy: 0.5625 - val_loss: 0.2548 - val_accuracy: 0.4911\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.43 - 1s 603ms/step - loss: 0.2530 - accuracy: 0.4375 - val_loss: 0.2390 - val_accuracy: 0.5047\n",
      "Epoch 267/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.62 - 1s 598ms/step - loss: 0.2478 - accuracy: 0.5312 - val_loss: 0.2468 - val_accuracy: 0.5135\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.59 - 1s 598ms/step - loss: 0.2492 - accuracy: 0.5625 - val_loss: 0.2552 - val_accuracy: 0.5080\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.59 - 1s 604ms/step - loss: 0.2459 - accuracy: 0.5938 - val_loss: 0.2497 - val_accuracy: 0.5094\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.53 - 1s 598ms/step - loss: 0.2535 - accuracy: 0.4688 - val_loss: 0.2506 - val_accuracy: 0.5089\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.43 - 1s 597ms/step - loss: 0.2526 - accuracy: 0.4844 - val_loss: 0.2594 - val_accuracy: 0.5088\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 600ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2520 - val_accuracy: 0.5092\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.43 - 1s 684ms/step - loss: 0.2452 - accuracy: 0.5469 - val_loss: 0.2386 - val_accuracy: 0.5003\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.62 - 1s 601ms/step - loss: 0.2456 - accuracy: 0.5781 - val_loss: 0.2464 - val_accuracy: 0.5045\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.53 - 1s 599ms/step - loss: 0.2438 - accuracy: 0.5938 - val_loss: 0.2554 - val_accuracy: 0.5036\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.53 - 1s 619ms/step - loss: 0.2455 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.4917\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.43 - 1s 609ms/step - loss: 0.2541 - accuracy: 0.4688 - val_loss: 0.2504 - val_accuracy: 0.5089\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.62 - 1s 599ms/step - loss: 0.2498 - accuracy: 0.5469 - val_loss: 0.2586 - val_accuracy: 0.5439\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.59 - 1s 611ms/step - loss: 0.2491 - accuracy: 0.5469 - val_loss: 0.2517 - val_accuracy: 0.5184\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.62 - 1s 604ms/step - loss: 0.2437 - accuracy: 0.6094 - val_loss: 0.2391 - val_accuracy: 0.5179\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.53 - 1s 623ms/step - loss: 0.2487 - accuracy: 0.4688 - val_loss: 0.2463 - val_accuracy: 0.5180\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.34 - 1s 615ms/step - loss: 0.2517 - accuracy: 0.4375 - val_loss: 0.2548 - val_accuracy: 0.5169\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.56 - 1s 698ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2497 - val_accuracy: 0.5180\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.46 - 1s 602ms/step - loss: 0.2430 - accuracy: 0.5625 - val_loss: 0.2499 - val_accuracy: 0.5182\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.65 - 1s 604ms/step - loss: 0.2437 - accuracy: 0.5938 - val_loss: 0.2594 - val_accuracy: 0.5307\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.53 - 1s 598ms/step - loss: 0.2456 - accuracy: 0.5938 - val_loss: 0.2512 - val_accuracy: 0.5366\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.56 - 1s 600ms/step - loss: 0.2447 - accuracy: 0.5625 - val_loss: 0.2383 - val_accuracy: 0.5267\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.43 - 1s 598ms/step - loss: 0.2554 - accuracy: 0.4375 - val_loss: 0.2456 - val_accuracy: 0.5127\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.62 - 1s 602ms/step - loss: 0.2527 - accuracy: 0.5156 - val_loss: 0.2508 - val_accuracy: 0.5180\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.59 - 1s 600ms/step - loss: 0.2433 - accuracy: 0.5625 - val_loss: 0.2492 - val_accuracy: 0.4957\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.59 - 1s 594ms/step - loss: 0.2399 - accuracy: 0.5938 - val_loss: 0.2489 - val_accuracy: 0.5138\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 594ms/step - loss: 0.2498 - accuracy: 0.5000 - val_loss: 0.2574 - val_accuracy: 0.5307\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.43 - 1s 599ms/step - loss: 0.2568 - accuracy: 0.3438 - val_loss: 0.2500 - val_accuracy: 0.5318\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.53 - 1s 611ms/step - loss: 0.2497 - accuracy: 0.5625 - val_loss: 0.2415 - val_accuracy: 0.5136\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.50 - 1s 600ms/step - loss: 0.2464 - accuracy: 0.5781 - val_loss: 0.2457 - val_accuracy: 0.5045\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.59 - 1s 601ms/step - loss: 0.2474 - accuracy: 0.5625 - val_loss: 0.2516 - val_accuracy: 0.4902\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.53 - 1s 674ms/step - loss: 0.2395 - accuracy: 0.5938 - val_loss: 0.2492 - val_accuracy: 0.5179\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.56 - 1s 599ms/step - loss: 0.2484 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.5318\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.53 - 1s 600ms/step - loss: 0.2478 - accuracy: 0.5312 - val_loss: 0.2563 - val_accuracy: 0.5176\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.68 - 1s 602ms/step - loss: 0.2530 - accuracy: 0.5312 - val_loss: 0.2481 - val_accuracy: 0.5315\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.53 - 1s 623ms/step - loss: 0.2451 - accuracy: 0.5469 - val_loss: 0.2416 - val_accuracy: 0.5045\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.53 - 1s 632ms/step - loss: 0.2459 - accuracy: 0.5625 - val_loss: 0.2458 - val_accuracy: 0.4689\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.53 - 1s 605ms/step - loss: 0.2530 - accuracy: 0.4844 - val_loss: 0.2517 - val_accuracy: 0.4857\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.37 - 1s 602ms/step - loss: 0.2535 - accuracy: 0.4219 - val_loss: 0.2488 - val_accuracy: 0.4649\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.59 - 1s 615ms/step - loss: 0.2436 - accuracy: 0.6094 - val_loss: 0.2485 - val_accuracy: 0.4776\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.56 - 1s 633ms/step - loss: 0.2431 - accuracy: 0.5312 - val_loss: 0.2557 - val_accuracy: 0.4908\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.62 - 1s 598ms/step - loss: 0.2494 - accuracy: 0.5781 - val_loss: 0.2490 - val_accuracy: 0.4827\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.53 - 1s 598ms/step - loss: 0.2494 - accuracy: 0.5938 - val_loss: 0.2417 - val_accuracy: 0.4733\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.56 - 1s 665ms/step - loss: 0.2483 - accuracy: 0.5312 - val_loss: 0.2469 - val_accuracy: 0.4955\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.40 - 1s 597ms/step - loss: 0.2539 - accuracy: 0.5000 - val_loss: 0.2533 - val_accuracy: 0.5125\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00310: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Bilding and training GRU model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=500,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5546875\n",
      "test_loss: 0.2419198900461197\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
