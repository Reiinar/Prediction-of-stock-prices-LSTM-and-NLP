{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Microsoft\n",
    "Sentiment is built upon the Reuters titles dataset.\n",
    "Historical data is taken from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping historical data from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooFinanceHistory:\n",
    "    timeout = 2\n",
    "    crumb_link = 'https://finance.yahoo.com/quote/{0}/history?p={0}'\n",
    "    crumble_regex = r'CrumbStore\":{\"crumb\":\"(.*?)\"}'\n",
    "    quote_link = 'https://query1.finance.yahoo.com/v7/finance/download/{quote}?period1={dfrom}&period2={dto}&interval=1d&events=history&crumb={crumb}'\n",
    "\n",
    "    def __init__(self, symbol, days_back=7):\n",
    "        self.symbol = symbol\n",
    "        self.session = requests.Session()\n",
    "        self.dt = timedelta(days=days_back)\n",
    "\n",
    "#requesting crumb and cookie\n",
    "    def get_crumb(self):\n",
    "        response = self.session.get(self.crumb_link.format(self.symbol), timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        match = re.search(self.crumble_regex, response.text)\n",
    "        if not match:\n",
    "            raise ValueError('Could not get crumb from Yahoo Finance')\n",
    "        else:\n",
    "            self.crumb = match.group(1)\n",
    "\n",
    "#requesting data\n",
    "    def get_quote(self):\n",
    "        if not hasattr(self, 'crumb') or len(self.session.cookies) == 0:\n",
    "            self.get_crumb()\n",
    "        now = datetime.utcnow()\n",
    "        dateto = int(now.timestamp())\n",
    "        datefrom = int((now - self.dt).timestamp())\n",
    "        url = self.quote_link.format(quote=self.symbol, dfrom=datefrom, dto=dateto, crumb=self.crumb)\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(StringIO(response.text), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting data about Apple from 4000 days back\n",
    "df_v = YahooFinanceHistory('AAPL', days_back=4000).get_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-04</td>\n",
       "      <td>20.018572</td>\n",
       "      <td>20.597143</td>\n",
       "      <td>20.005714</td>\n",
       "      <td>20.534286</td>\n",
       "      <td>17.776474</td>\n",
       "      <td>137658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>20.758572</td>\n",
       "      <td>20.914286</td>\n",
       "      <td>20.458570</td>\n",
       "      <td>20.667143</td>\n",
       "      <td>17.891493</td>\n",
       "      <td>158179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-08</td>\n",
       "      <td>20.545713</td>\n",
       "      <td>20.604286</td>\n",
       "      <td>19.918571</td>\n",
       "      <td>20.549999</td>\n",
       "      <td>17.790079</td>\n",
       "      <td>232913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-09</td>\n",
       "      <td>20.544285</td>\n",
       "      <td>20.651428</td>\n",
       "      <td>20.078571</td>\n",
       "      <td>20.388571</td>\n",
       "      <td>17.650331</td>\n",
       "      <td>169241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-06-10</td>\n",
       "      <td>20.325714</td>\n",
       "      <td>20.335714</td>\n",
       "      <td>19.757143</td>\n",
       "      <td>20.035715</td>\n",
       "      <td>17.344868</td>\n",
       "      <td>172155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>308.100006</td>\n",
       "      <td>317.049988</td>\n",
       "      <td>307.239990</td>\n",
       "      <td>315.010010</td>\n",
       "      <td>315.010010</td>\n",
       "      <td>36405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>317.829987</td>\n",
       "      <td>319.690002</td>\n",
       "      <td>310.910004</td>\n",
       "      <td>311.410004</td>\n",
       "      <td>311.410004</td>\n",
       "      <td>40575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>312.149994</td>\n",
       "      <td>315.950012</td>\n",
       "      <td>303.209991</td>\n",
       "      <td>307.649994</td>\n",
       "      <td>307.649994</td>\n",
       "      <td>50155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>304.510010</td>\n",
       "      <td>309.790009</td>\n",
       "      <td>301.529999</td>\n",
       "      <td>309.540009</td>\n",
       "      <td>309.540009</td>\n",
       "      <td>39732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>300.350006</td>\n",
       "      <td>307.899994</td>\n",
       "      <td>300.209991</td>\n",
       "      <td>307.709991</td>\n",
       "      <td>307.709991</td>\n",
       "      <td>41561200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2757 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-06-04   20.018572   20.597143   20.005714   20.534286   17.776474   \n",
       "1    2009-06-05   20.758572   20.914286   20.458570   20.667143   17.891493   \n",
       "2    2009-06-08   20.545713   20.604286   19.918571   20.549999   17.790079   \n",
       "3    2009-06-09   20.544285   20.651428   20.078571   20.388571   17.650331   \n",
       "4    2009-06-10   20.325714   20.335714   19.757143   20.035715   17.344868   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2752 2020-05-11  308.100006  317.049988  307.239990  315.010010  315.010010   \n",
       "2753 2020-05-12  317.829987  319.690002  310.910004  311.410004  311.410004   \n",
       "2754 2020-05-13  312.149994  315.950012  303.209991  307.649994  307.649994   \n",
       "2755 2020-05-14  304.510010  309.790009  301.529999  309.540009  309.540009   \n",
       "2756 2020-05-15  300.350006  307.899994  300.209991  307.709991  307.709991   \n",
       "\n",
       "         Volume  \n",
       "0     137658500  \n",
       "1     158179000  \n",
       "2     232913100  \n",
       "3     169241100  \n",
       "4     172155900  \n",
       "...         ...  \n",
       "2752   36405900  \n",
       "2753   40575300  \n",
       "2754   50155600  \n",
       "2755   39732300  \n",
       "2756   41561200  \n",
       "\n",
       "[2757 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting dates, chronologically\n",
    "df_v.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         datetime64[ns]\n",
       "Open                float64\n",
       "High                float64\n",
       "Low                 float64\n",
       "Close               float64\n",
       "Adj Close           float64\n",
       "Volume                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-04</td>\n",
       "      <td>20.018572</td>\n",
       "      <td>20.597143</td>\n",
       "      <td>20.005714</td>\n",
       "      <td>20.534286</td>\n",
       "      <td>17.776474</td>\n",
       "      <td>137658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>20.758572</td>\n",
       "      <td>20.914286</td>\n",
       "      <td>20.458570</td>\n",
       "      <td>20.667143</td>\n",
       "      <td>17.891493</td>\n",
       "      <td>158179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-08</td>\n",
       "      <td>20.545713</td>\n",
       "      <td>20.604286</td>\n",
       "      <td>19.918571</td>\n",
       "      <td>20.549999</td>\n",
       "      <td>17.790079</td>\n",
       "      <td>232913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-09</td>\n",
       "      <td>20.544285</td>\n",
       "      <td>20.651428</td>\n",
       "      <td>20.078571</td>\n",
       "      <td>20.388571</td>\n",
       "      <td>17.650331</td>\n",
       "      <td>169241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-06-10</td>\n",
       "      <td>20.325714</td>\n",
       "      <td>20.335714</td>\n",
       "      <td>19.757143</td>\n",
       "      <td>20.035715</td>\n",
       "      <td>17.344868</td>\n",
       "      <td>172155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>308.100006</td>\n",
       "      <td>317.049988</td>\n",
       "      <td>307.239990</td>\n",
       "      <td>315.010010</td>\n",
       "      <td>315.010010</td>\n",
       "      <td>36405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>317.829987</td>\n",
       "      <td>319.690002</td>\n",
       "      <td>310.910004</td>\n",
       "      <td>311.410004</td>\n",
       "      <td>311.410004</td>\n",
       "      <td>40575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>312.149994</td>\n",
       "      <td>315.950012</td>\n",
       "      <td>303.209991</td>\n",
       "      <td>307.649994</td>\n",
       "      <td>307.649994</td>\n",
       "      <td>50155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>304.510010</td>\n",
       "      <td>309.790009</td>\n",
       "      <td>301.529999</td>\n",
       "      <td>309.540009</td>\n",
       "      <td>309.540009</td>\n",
       "      <td>39732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>300.350006</td>\n",
       "      <td>307.899994</td>\n",
       "      <td>300.209991</td>\n",
       "      <td>307.709991</td>\n",
       "      <td>307.709991</td>\n",
       "      <td>41561200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2757 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-06-04   20.018572   20.597143   20.005714   20.534286   17.776474   \n",
       "1    2009-06-05   20.758572   20.914286   20.458570   20.667143   17.891493   \n",
       "2    2009-06-08   20.545713   20.604286   19.918571   20.549999   17.790079   \n",
       "3    2009-06-09   20.544285   20.651428   20.078571   20.388571   17.650331   \n",
       "4    2009-06-10   20.325714   20.335714   19.757143   20.035715   17.344868   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2752 2020-05-11  308.100006  317.049988  307.239990  315.010010  315.010010   \n",
       "2753 2020-05-12  317.829987  319.690002  310.910004  311.410004  311.410004   \n",
       "2754 2020-05-13  312.149994  315.950012  303.209991  307.649994  307.649994   \n",
       "2755 2020-05-14  304.510010  309.790009  301.529999  309.540009  309.540009   \n",
       "2756 2020-05-15  300.350006  307.899994  300.209991  307.709991  307.709991   \n",
       "\n",
       "         Volume  \n",
       "0     137658500  \n",
       "1     158179000  \n",
       "2     232913100  \n",
       "3     169241100  \n",
       "4     172155900  \n",
       "...         ...  \n",
       "2752   36405900  \n",
       "2753   40575300  \n",
       "2754   50155600  \n",
       "2755   39732300  \n",
       "2756   41561200  \n",
       "\n",
       "[2757 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment for all the articles with \"Apple\" in the body of an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a file\n",
    "df2 = pd.read_csv('df_AP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>0.143650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>-0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>-0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>-0.096240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1341</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>1342</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1343</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>1344</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1345</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>0.403940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1346 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Date  compound_mean\n",
       "0              0  2011-07-06       0.143650\n",
       "1              1  2011-07-07      -0.284200\n",
       "2              2  2011-07-08      -0.102600\n",
       "3              3  2011-07-11       0.236200\n",
       "4              4  2011-07-12      -0.096240\n",
       "...          ...         ...            ...\n",
       "1341        1341  2017-01-06       0.636900\n",
       "1342        1342  2017-01-08       0.296000\n",
       "1343        1343  2017-01-10       0.024698\n",
       "1344        1344  2017-01-11       0.024698\n",
       "1345        1345  2017-01-12       0.403940\n",
       "\n",
       "[1346 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting column Unnamed\n",
    "df2 = df2.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>0.143650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>-0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>-0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>-0.096240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>0.403940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1346 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  compound_mean\n",
       "0     2011-07-06       0.143650\n",
       "1     2011-07-07      -0.284200\n",
       "2     2011-07-08      -0.102600\n",
       "3     2011-07-11       0.236200\n",
       "4     2011-07-12      -0.096240\n",
       "...          ...            ...\n",
       "1341  2017-01-06       0.636900\n",
       "1342  2017-01-08       0.296000\n",
       "1343  2017-01-10       0.024698\n",
       "1344  2017-01-11       0.024698\n",
       "1345  2017-01-12       0.403940\n",
       "\n",
       "[1346 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              object\n",
       "compound_mean    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column Date type to datetime type\n",
    "df2.Date=pd.to_datetime(df2['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging dataframe with historical data with dataframe with sentiments \n",
    "df3 = pd.merge(df_v,df2,on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>49.849998</td>\n",
       "      <td>50.585712</td>\n",
       "      <td>49.529999</td>\n",
       "      <td>50.251427</td>\n",
       "      <td>43.502525</td>\n",
       "      <td>111156500</td>\n",
       "      <td>0.143650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>50.667141</td>\n",
       "      <td>51.142857</td>\n",
       "      <td>50.571430</td>\n",
       "      <td>51.028572</td>\n",
       "      <td>44.175301</td>\n",
       "      <td>99915900</td>\n",
       "      <td>-0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>50.477142</td>\n",
       "      <td>51.428570</td>\n",
       "      <td>50.314285</td>\n",
       "      <td>51.387142</td>\n",
       "      <td>44.485714</td>\n",
       "      <td>122408300</td>\n",
       "      <td>-0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>50.905716</td>\n",
       "      <td>51.395714</td>\n",
       "      <td>50.402859</td>\n",
       "      <td>50.571430</td>\n",
       "      <td>43.779568</td>\n",
       "      <td>110668600</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>50.504284</td>\n",
       "      <td>51.097141</td>\n",
       "      <td>49.802856</td>\n",
       "      <td>50.535713</td>\n",
       "      <td>43.748634</td>\n",
       "      <td>112902300</td>\n",
       "      <td>-0.096240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>116.860001</td>\n",
       "      <td>115.809998</td>\n",
       "      <td>116.610001</td>\n",
       "      <td>110.829552</td>\n",
       "      <td>22193600</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>116.779999</td>\n",
       "      <td>118.160004</td>\n",
       "      <td>116.470001</td>\n",
       "      <td>117.910004</td>\n",
       "      <td>112.065109</td>\n",
       "      <td>31751900</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>118.769997</td>\n",
       "      <td>119.379997</td>\n",
       "      <td>118.300003</td>\n",
       "      <td>119.110001</td>\n",
       "      <td>113.205620</td>\n",
       "      <td>24462100</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>118.739998</td>\n",
       "      <td>119.930000</td>\n",
       "      <td>118.599998</td>\n",
       "      <td>119.750000</td>\n",
       "      <td>113.813881</td>\n",
       "      <td>27588600</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>118.900002</td>\n",
       "      <td>119.300003</td>\n",
       "      <td>118.209999</td>\n",
       "      <td>119.250000</td>\n",
       "      <td>113.338661</td>\n",
       "      <td>27086200</td>\n",
       "      <td>0.403940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1165 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2011-07-06   49.849998   50.585712   49.529999   50.251427   43.502525   \n",
       "1    2011-07-07   50.667141   51.142857   50.571430   51.028572   44.175301   \n",
       "2    2011-07-08   50.477142   51.428570   50.314285   51.387142   44.485714   \n",
       "3    2011-07-11   50.905716   51.395714   50.402859   50.571430   43.779568   \n",
       "4    2011-07-12   50.504284   51.097141   49.802856   50.535713   43.748634   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1160 2017-01-05  115.919998  116.860001  115.809998  116.610001  110.829552   \n",
       "1161 2017-01-06  116.779999  118.160004  116.470001  117.910004  112.065109   \n",
       "1162 2017-01-10  118.769997  119.379997  118.300003  119.110001  113.205620   \n",
       "1163 2017-01-11  118.739998  119.930000  118.599998  119.750000  113.813881   \n",
       "1164 2017-01-12  118.900002  119.300003  118.209999  119.250000  113.338661   \n",
       "\n",
       "         Volume  compound_mean  \n",
       "0     111156500       0.143650  \n",
       "1      99915900      -0.284200  \n",
       "2     122408300      -0.102600  \n",
       "3     110668600       0.236200  \n",
       "4     112902300      -0.096240  \n",
       "...         ...            ...  \n",
       "1160   22193600       0.458800  \n",
       "1161   31751900       0.636900  \n",
       "1162   24462100       0.024698  \n",
       "1163   27588600       0.024698  \n",
       "1164   27086200       0.403940  \n",
       "\n",
       "[1165 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for prediction of label for the next day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepcopying dataframe, so there would be no need to run everything from the beggining\n",
    "df = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addind label of 1(up) or 0(down) for the price of a next day\n",
    "def add_label(dfi):\n",
    "    idx = len(dfi.columns)\n",
    "    new_col = np.where(dfi['Close'] >= dfi['Close'].shift(1), 1, 0)  \n",
    "    dfi.insert(loc=idx, column='Label', value=new_col)\n",
    "    dfi = dfi.fillna(0)\n",
    "    df['Label'] =  df['Label'].shift(-1, axis = 0)\n",
    "    df.drop(df.index[len(df)-1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>49.849998</td>\n",
       "      <td>50.585712</td>\n",
       "      <td>49.529999</td>\n",
       "      <td>50.251427</td>\n",
       "      <td>43.502525</td>\n",
       "      <td>111156500</td>\n",
       "      <td>0.143650</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>50.667141</td>\n",
       "      <td>51.142857</td>\n",
       "      <td>50.571430</td>\n",
       "      <td>51.028572</td>\n",
       "      <td>44.175301</td>\n",
       "      <td>99915900</td>\n",
       "      <td>-0.284200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>50.477142</td>\n",
       "      <td>51.428570</td>\n",
       "      <td>50.314285</td>\n",
       "      <td>51.387142</td>\n",
       "      <td>44.485714</td>\n",
       "      <td>122408300</td>\n",
       "      <td>-0.102600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>50.905716</td>\n",
       "      <td>51.395714</td>\n",
       "      <td>50.402859</td>\n",
       "      <td>50.571430</td>\n",
       "      <td>43.779568</td>\n",
       "      <td>110668600</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>50.504284</td>\n",
       "      <td>51.097141</td>\n",
       "      <td>49.802856</td>\n",
       "      <td>50.535713</td>\n",
       "      <td>43.748634</td>\n",
       "      <td>112902300</td>\n",
       "      <td>-0.096240</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>115.849998</td>\n",
       "      <td>116.510002</td>\n",
       "      <td>115.750000</td>\n",
       "      <td>116.019997</td>\n",
       "      <td>110.268791</td>\n",
       "      <td>21118100</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>116.860001</td>\n",
       "      <td>115.809998</td>\n",
       "      <td>116.610001</td>\n",
       "      <td>110.829552</td>\n",
       "      <td>22193600</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>116.779999</td>\n",
       "      <td>118.160004</td>\n",
       "      <td>116.470001</td>\n",
       "      <td>117.910004</td>\n",
       "      <td>112.065109</td>\n",
       "      <td>31751900</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>118.769997</td>\n",
       "      <td>119.379997</td>\n",
       "      <td>118.300003</td>\n",
       "      <td>119.110001</td>\n",
       "      <td>113.205620</td>\n",
       "      <td>24462100</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>118.739998</td>\n",
       "      <td>119.930000</td>\n",
       "      <td>118.599998</td>\n",
       "      <td>119.750000</td>\n",
       "      <td>113.813881</td>\n",
       "      <td>27588600</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2011-07-06   49.849998   50.585712   49.529999   50.251427   43.502525   \n",
       "1    2011-07-07   50.667141   51.142857   50.571430   51.028572   44.175301   \n",
       "2    2011-07-08   50.477142   51.428570   50.314285   51.387142   44.485714   \n",
       "3    2011-07-11   50.905716   51.395714   50.402859   50.571430   43.779568   \n",
       "4    2011-07-12   50.504284   51.097141   49.802856   50.535713   43.748634   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1159 2017-01-04  115.849998  116.510002  115.750000  116.019997  110.268791   \n",
       "1160 2017-01-05  115.919998  116.860001  115.809998  116.610001  110.829552   \n",
       "1161 2017-01-06  116.779999  118.160004  116.470001  117.910004  112.065109   \n",
       "1162 2017-01-10  118.769997  119.379997  118.300003  119.110001  113.205620   \n",
       "1163 2017-01-11  118.739998  119.930000  118.599998  119.750000  113.813881   \n",
       "\n",
       "         Volume  compound_mean  Label  \n",
       "0     111156500       0.143650    1.0  \n",
       "1      99915900      -0.284200    1.0  \n",
       "2     122408300      -0.102600    0.0  \n",
       "3     110668600       0.236200    0.0  \n",
       "4     112902300      -0.096240    1.0  \n",
       "...         ...            ...    ...  \n",
       "1159   21118100       0.024698    1.0  \n",
       "1160   22193600       0.458800    1.0  \n",
       "1161   31751900       0.636900    1.0  \n",
       "1162   24462100       0.024698    1.0  \n",
       "1163   27588600       0.024698    0.0  \n",
       "\n",
       "[1164 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             datetime64[ns]\n",
       "Open                    float64\n",
       "High                    float64\n",
       "Low                     float64\n",
       "Close                   float64\n",
       "Adj Close               float64\n",
       "Volume                    int64\n",
       "compound_mean           float64\n",
       "Label                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training and testing datasets\n",
    "X = array[:,1:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardising features, fitting and transforming X\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting Y to data type integer\n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00965776 0.00663629 0.01275638 0.00939164 0.00856365 0.23789287\n",
      " 0.31203994]\n",
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'compound_mean'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X[1])\n",
    "print(df.columns[1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.626e-01 3.989e-01 4.030e-01 4.352e-01 4.028e-01 2.409e-02 1.947e-04]\n",
      "[[0.    0.    0.    0.    0.   ]\n",
      " [0.01  0.007 0.013 0.009 0.009]\n",
      " [0.007 0.01  0.01  0.014 0.013]\n",
      " [0.012 0.01  0.011 0.004 0.004]\n",
      " [0.008 0.006 0.003 0.003 0.003]]\n"
     ]
    }
   ],
   "source": [
    "#choosing best features for the model\n",
    "test = SelectKBest(score_func=chi2, k=5)\n",
    "fit = test.fit(X, Y)\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.01 , 0.007, 0.013, 0.009, 0.009],\n",
       "       [0.007, 0.01 , 0.01 , 0.014, 0.013],\n",
       "       ...,\n",
       "       [0.791, 0.805, 0.82 , 0.818, 0.873],\n",
       "       [0.815, 0.819, 0.842, 0.832, 0.887],\n",
       "       [0.814, 0.826, 0.846, 0.84 , 0.895]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#High, Low, Close, Volume and Adj Close give the most information. Compound mean is the least relevant feature\n",
    "#as is's picked last. We will compare in this case machine learning models with and without compound mean\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 873 samples, validate on 291 samples\n",
      "Epoch 1/100\n",
      "873/873 [==============================] - 1s 1ms/step - loss: 0.6964 - accuracy: 0.4868 - val_loss: 0.6983 - val_accuracy: 0.4467\n",
      "Epoch 2/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6964 - accuracy: 0.4903 - val_loss: 0.6922 - val_accuracy: 0.5533\n",
      "Epoch 3/100\n",
      "873/873 [==============================] - 0s 207us/step - loss: 0.6956 - accuracy: 0.5155 - val_loss: 0.6919 - val_accuracy: 0.5739\n",
      "Epoch 4/100\n",
      "873/873 [==============================] - 0s 211us/step - loss: 0.6929 - accuracy: 0.5155 - val_loss: 0.6929 - val_accuracy: 0.4983\n",
      "Epoch 5/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6957 - accuracy: 0.5040 - val_loss: 0.6917 - val_accuracy: 0.5533\n",
      "Epoch 6/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6965 - accuracy: 0.5178 - val_loss: 0.6927 - val_accuracy: 0.5773\n",
      "Epoch 7/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6912 - val_accuracy: 0.5533\n",
      "Epoch 8/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6968 - accuracy: 0.4811 - val_loss: 0.6907 - val_accuracy: 0.5533\n",
      "Epoch 9/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6934 - accuracy: 0.5097 - val_loss: 0.6923 - val_accuracy: 0.5533\n",
      "Epoch 10/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6944 - accuracy: 0.5029 - val_loss: 0.6922 - val_accuracy: 0.5533\n",
      "Epoch 11/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6931 - accuracy: 0.4914 - val_loss: 0.6911 - val_accuracy: 0.5911\n",
      "Epoch 12/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6953 - accuracy: 0.4926 - val_loss: 0.6914 - val_accuracy: 0.5533\n",
      "Epoch 13/100\n",
      "873/873 [==============================] - 0s 181us/step - loss: 0.6944 - accuracy: 0.4983 - val_loss: 0.6921 - val_accuracy: 0.5533\n",
      "Epoch 14/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6939 - accuracy: 0.5109 - val_loss: 0.6932 - val_accuracy: 0.4777\n",
      "Epoch 15/100\n",
      "873/873 [==============================] - 0s 203us/step - loss: 0.6945 - accuracy: 0.5006 - val_loss: 0.6920 - val_accuracy: 0.5739\n",
      "Epoch 16/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6936 - accuracy: 0.5029 - val_loss: 0.6917 - val_accuracy: 0.5533\n",
      "Epoch 17/100\n",
      "873/873 [==============================] - 0s 188us/step - loss: 0.6968 - accuracy: 0.4490 - val_loss: 0.6930 - val_accuracy: 0.5086\n",
      "Epoch 18/100\n",
      "873/873 [==============================] - 0s 234us/step - loss: 0.6948 - accuracy: 0.4960 - val_loss: 0.6946 - val_accuracy: 0.4467\n",
      "Epoch 19/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6944 - accuracy: 0.4903 - val_loss: 0.6927 - val_accuracy: 0.5533\n",
      "Epoch 20/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6928 - accuracy: 0.5120 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 21/100\n",
      "873/873 [==============================] - 0s 197us/step - loss: 0.6943 - accuracy: 0.4868 - val_loss: 0.6941 - val_accuracy: 0.4467\n",
      "Epoch 22/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6930 - accuracy: 0.4994 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 23/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6943 - accuracy: 0.4891 - val_loss: 0.6918 - val_accuracy: 0.5533\n",
      "Epoch 24/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6929 - accuracy: 0.5052 - val_loss: 0.6924 - val_accuracy: 0.5533\n",
      "Epoch 25/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6940 - accuracy: 0.4971 - val_loss: 0.6946 - val_accuracy: 0.4467\n",
      "Epoch 26/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6969 - accuracy: 0.4903 - val_loss: 0.6930 - val_accuracy: 0.5017\n",
      "Epoch 27/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6930 - accuracy: 0.5052 - val_loss: 0.6922 - val_accuracy: 0.5533\n",
      "Epoch 28/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6940 - val_accuracy: 0.4467\n",
      "Epoch 29/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6924 - accuracy: 0.5109 - val_loss: 0.6955 - val_accuracy: 0.4467\n",
      "Epoch 30/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6939 - accuracy: 0.5097 - val_loss: 0.6952 - val_accuracy: 0.4467\n",
      "Epoch 31/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6926 - accuracy: 0.5235 - val_loss: 0.6914 - val_accuracy: 0.5533\n",
      "Epoch 32/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6930 - val_accuracy: 0.4948\n",
      "Epoch 33/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6915 - accuracy: 0.5223 - val_loss: 0.6928 - val_accuracy: 0.5086\n",
      "Epoch 34/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6927 - accuracy: 0.4948 - val_loss: 0.6946 - val_accuracy: 0.4467\n",
      "Epoch 35/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6925 - accuracy: 0.5166 - val_loss: 0.6928 - val_accuracy: 0.5052\n",
      "Epoch 36/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6929 - accuracy: 0.4960 - val_loss: 0.6917 - val_accuracy: 0.5945\n",
      "Epoch 37/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6949 - accuracy: 0.4891 - val_loss: 0.6916 - val_accuracy: 0.5533\n",
      "Epoch 38/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6941 - accuracy: 0.4868 - val_loss: 0.6922 - val_accuracy: 0.5911\n",
      "Epoch 39/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6947 - accuracy: 0.4914 - val_loss: 0.6915 - val_accuracy: 0.5533\n",
      "Epoch 40/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6946 - accuracy: 0.5212 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 41/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6938 - accuracy: 0.4765 - val_loss: 0.6925 - val_accuracy: 0.5704\n",
      "Epoch 42/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6934 - accuracy: 0.5086 - val_loss: 0.6938 - val_accuracy: 0.4467\n",
      "Epoch 43/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6931 - accuracy: 0.5006 - val_loss: 0.6929 - val_accuracy: 0.5636\n",
      "Epoch 44/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6927 - accuracy: 0.5097 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 45/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6949 - accuracy: 0.4971 - val_loss: 0.6945 - val_accuracy: 0.4467\n",
      "Epoch 46/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6928 - accuracy: 0.5052 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 47/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6942 - val_accuracy: 0.4467\n",
      "Epoch 48/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6930 - accuracy: 0.5189 - val_loss: 0.6935 - val_accuracy: 0.4467\n",
      "Epoch 49/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6948 - accuracy: 0.4800 - val_loss: 0.6925 - val_accuracy: 0.5876\n",
      "Epoch 50/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6943 - accuracy: 0.4788 - val_loss: 0.6937 - val_accuracy: 0.4467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "873/873 [==============================] - 0s 215us/step - loss: 0.6933 - accuracy: 0.5006 - val_loss: 0.6930 - val_accuracy: 0.5292\n",
      "Epoch 52/100\n",
      "873/873 [==============================] - 0s 207us/step - loss: 0.6929 - accuracy: 0.5040 - val_loss: 0.6928 - val_accuracy: 0.5533\n",
      "Epoch 53/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6951 - accuracy: 0.4754 - val_loss: 0.6941 - val_accuracy: 0.4467\n",
      "Epoch 54/100\n",
      "873/873 [==============================] - 0s 176us/step - loss: 0.6935 - accuracy: 0.4926 - val_loss: 0.6935 - val_accuracy: 0.4467\n",
      "Epoch 55/100\n",
      "873/873 [==============================] - 0s 178us/step - loss: 0.6925 - accuracy: 0.5189 - val_loss: 0.6938 - val_accuracy: 0.4467\n",
      "Epoch 56/100\n",
      "873/873 [==============================] - 0s 177us/step - loss: 0.6924 - accuracy: 0.4914 - val_loss: 0.6926 - val_accuracy: 0.5223\n",
      "Epoch 57/100\n",
      "873/873 [==============================] - 0s 180us/step - loss: 0.6939 - accuracy: 0.5017 - val_loss: 0.6916 - val_accuracy: 0.5533\n",
      "Epoch 58/100\n",
      "873/873 [==============================] - 0s 179us/step - loss: 0.6942 - accuracy: 0.4971 - val_loss: 0.6930 - val_accuracy: 0.5052\n",
      "Epoch 59/100\n",
      "873/873 [==============================] - 0s 179us/step - loss: 0.6940 - accuracy: 0.4903 - val_loss: 0.6934 - val_accuracy: 0.4467\n",
      "Epoch 60/100\n",
      "873/873 [==============================] - 0s 179us/step - loss: 0.6932 - accuracy: 0.5120 - val_loss: 0.6942 - val_accuracy: 0.4467\n",
      "Epoch 61/100\n",
      "873/873 [==============================] - 0s 181us/step - loss: 0.6930 - accuracy: 0.5143 - val_loss: 0.6922 - val_accuracy: 0.5533\n",
      "Epoch 62/100\n",
      "873/873 [==============================] - 0s 179us/step - loss: 0.6943 - accuracy: 0.4800 - val_loss: 0.6919 - val_accuracy: 0.5533\n",
      "Epoch 63/100\n",
      "873/873 [==============================] - 0s 179us/step - loss: 0.6939 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.4570\n",
      "Epoch 64/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6929 - accuracy: 0.5166 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 65/100\n",
      "873/873 [==============================] - 0s 177us/step - loss: 0.6943 - accuracy: 0.4971 - val_loss: 0.6919 - val_accuracy: 0.5533\n",
      "Epoch 66/100\n",
      "873/873 [==============================] - 0s 178us/step - loss: 0.6923 - accuracy: 0.5109 - val_loss: 0.6931 - val_accuracy: 0.5120\n",
      "Epoch 67/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6945 - accuracy: 0.4822 - val_loss: 0.6935 - val_accuracy: 0.4467\n",
      "Epoch 68/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6934 - accuracy: 0.5223 - val_loss: 0.6931 - val_accuracy: 0.5086\n",
      "Epoch 69/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6928 - accuracy: 0.5074 - val_loss: 0.6951 - val_accuracy: 0.4467\n",
      "Epoch 70/100\n",
      "873/873 [==============================] - 0s 188us/step - loss: 0.6938 - accuracy: 0.4948 - val_loss: 0.6941 - val_accuracy: 0.4467\n",
      "Epoch 71/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6942 - accuracy: 0.4834 - val_loss: 0.6925 - val_accuracy: 0.5911\n",
      "Epoch 72/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6932 - accuracy: 0.5212 - val_loss: 0.6933 - val_accuracy: 0.4467\n",
      "Epoch 73/100\n",
      "873/873 [==============================] - 0s 180us/step - loss: 0.6927 - accuracy: 0.5212 - val_loss: 0.6928 - val_accuracy: 0.5911\n",
      "Epoch 74/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6922 - accuracy: 0.5269 - val_loss: 0.6935 - val_accuracy: 0.4467\n",
      "Epoch 75/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6932 - accuracy: 0.5155 - val_loss: 0.6927 - val_accuracy: 0.5739\n",
      "Epoch 76/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6925 - accuracy: 0.5143 - val_loss: 0.6938 - val_accuracy: 0.4467\n",
      "Epoch 77/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6925 - accuracy: 0.4948 - val_loss: 0.6929 - val_accuracy: 0.5086\n",
      "Epoch 78/100\n",
      "873/873 [==============================] - 0s 210us/step - loss: 0.6940 - accuracy: 0.5040 - val_loss: 0.6941 - val_accuracy: 0.4467\n",
      "Epoch 79/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6943 - accuracy: 0.4868 - val_loss: 0.6913 - val_accuracy: 0.5533\n",
      "Epoch 80/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6935 - accuracy: 0.4845 - val_loss: 0.6933 - val_accuracy: 0.4502\n",
      "Epoch 81/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6934 - accuracy: 0.5074 - val_loss: 0.6933 - val_accuracy: 0.4605\n",
      "Epoch 82/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6935 - accuracy: 0.4914 - val_loss: 0.6936 - val_accuracy: 0.4467\n",
      "Epoch 83/100\n",
      "873/873 [==============================] - 0s 185us/step - loss: 0.6942 - accuracy: 0.4719 - val_loss: 0.6929 - val_accuracy: 0.5120\n",
      "Epoch 84/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6942 - accuracy: 0.4765 - val_loss: 0.6923 - val_accuracy: 0.5533\n",
      "Epoch 85/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6921 - accuracy: 0.5212 - val_loss: 0.6924 - val_accuracy: 0.5567\n",
      "Epoch 86/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6939 - accuracy: 0.4880 - val_loss: 0.6927 - val_accuracy: 0.5533\n",
      "Epoch 87/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6936 - accuracy: 0.4960 - val_loss: 0.6933 - val_accuracy: 0.4467\n",
      "Epoch 88/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6933 - accuracy: 0.5086 - val_loss: 0.6945 - val_accuracy: 0.4467\n",
      "Epoch 89/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6940 - accuracy: 0.4891 - val_loss: 0.6927 - val_accuracy: 0.5773\n",
      "Epoch 90/100\n",
      "873/873 [==============================] - 0s 230us/step - loss: 0.6939 - accuracy: 0.4834 - val_loss: 0.6934 - val_accuracy: 0.4467\n",
      "Epoch 91/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6938 - val_accuracy: 0.4467\n",
      "Epoch 92/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6946 - accuracy: 0.4628 - val_loss: 0.6948 - val_accuracy: 0.4467\n",
      "Epoch 93/100\n",
      "873/873 [==============================] - 0s 184us/step - loss: 0.6933 - accuracy: 0.4937 - val_loss: 0.6944 - val_accuracy: 0.4467\n",
      "Epoch 94/100\n",
      "873/873 [==============================] - 0s 181us/step - loss: 0.6926 - accuracy: 0.5292 - val_loss: 0.6935 - val_accuracy: 0.4467\n",
      "Epoch 95/100\n",
      "873/873 [==============================] - 0s 230us/step - loss: 0.6945 - accuracy: 0.4731 - val_loss: 0.6943 - val_accuracy: 0.4467\n",
      "Epoch 96/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6940 - accuracy: 0.4937 - val_loss: 0.6940 - val_accuracy: 0.4467\n",
      "Epoch 97/100\n",
      "873/873 [==============================] - 0s 182us/step - loss: 0.6934 - accuracy: 0.5040 - val_loss: 0.6957 - val_accuracy: 0.4467\n",
      "Epoch 98/100\n",
      "873/873 [==============================] - 0s 183us/step - loss: 0.6941 - accuracy: 0.4994 - val_loss: 0.6937 - val_accuracy: 0.4467\n",
      "Epoch 99/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6937 - accuracy: 0.4857 - val_loss: 0.6938 - val_accuracy: 0.4467\n",
      "Epoch 100/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6933 - accuracy: 0.5155 - val_loss: 0.6938 - val_accuracy: 0.4467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#building and training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, Y, test_size=0.25)\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(5,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data= (X_test,y_test),\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/291 [==============================] - 0s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6938442301094737, 0.4467353820800781]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will take also less relevant features. After this we will determine what to use in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing the less relevant feature\n",
    "test = SelectKBest(score_func=chi2, k=7)\n",
    "fit = test.fit(X, Y)\n",
    "np.set_printoptions(precision=3)\n",
    "features = fit.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 873 samples, validate on 291 samples\n",
      "Epoch 1/100\n",
      "873/873 [==============================] - 1s 639us/step - loss: 0.6956 - accuracy: 0.4971 - val_loss: 0.6985 - val_accuracy: 0.4192\n",
      "Epoch 2/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6964 - accuracy: 0.5063 - val_loss: 0.6918 - val_accuracy: 0.5739\n",
      "Epoch 3/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6979 - accuracy: 0.5017 - val_loss: 0.6950 - val_accuracy: 0.4192\n",
      "Epoch 4/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6971 - accuracy: 0.5166 - val_loss: 0.6904 - val_accuracy: 0.5808\n",
      "Epoch 5/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6952 - accuracy: 0.4742 - val_loss: 0.6935 - val_accuracy: 0.5017\n",
      "Epoch 6/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6927 - accuracy: 0.5212 - val_loss: 0.6952 - val_accuracy: 0.4845\n",
      "Epoch 7/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6914 - accuracy: 0.5246 - val_loss: 0.6941 - val_accuracy: 0.4948\n",
      "Epoch 8/100\n",
      "873/873 [==============================] - 0s 204us/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6922 - val_accuracy: 0.5326\n",
      "Epoch 9/100\n",
      "873/873 [==============================] - 0s 206us/step - loss: 0.6969 - accuracy: 0.4845 - val_loss: 0.6964 - val_accuracy: 0.4192\n",
      "Epoch 10/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6927 - accuracy: 0.5086 - val_loss: 0.6946 - val_accuracy: 0.4639\n",
      "Epoch 11/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6945 - accuracy: 0.5109 - val_loss: 0.6965 - val_accuracy: 0.4158\n",
      "Epoch 12/100\n",
      "873/873 [==============================] - 0s 197us/step - loss: 0.6936 - accuracy: 0.5086 - val_loss: 0.6945 - val_accuracy: 0.4811\n",
      "Epoch 13/100\n",
      "873/873 [==============================] - 0s 198us/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.7003 - val_accuracy: 0.4192\n",
      "Epoch 14/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6945 - accuracy: 0.5269 - val_loss: 0.6917 - val_accuracy: 0.5567\n",
      "Epoch 15/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6933 - accuracy: 0.4811 - val_loss: 0.6937 - val_accuracy: 0.5086\n",
      "Epoch 16/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6947 - accuracy: 0.4903 - val_loss: 0.6943 - val_accuracy: 0.4364\n",
      "Epoch 17/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6912 - accuracy: 0.5155 - val_loss: 0.6951 - val_accuracy: 0.4192\n",
      "Epoch 18/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6921 - accuracy: 0.5097 - val_loss: 0.6982 - val_accuracy: 0.4192\n",
      "Epoch 19/100\n",
      "873/873 [==============================] - 0s 202us/step - loss: 0.6965 - accuracy: 0.4765 - val_loss: 0.6925 - val_accuracy: 0.5533\n",
      "Epoch 20/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6915 - accuracy: 0.5200 - val_loss: 0.6932 - val_accuracy: 0.5361\n",
      "Epoch 21/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6924 - accuracy: 0.5143 - val_loss: 0.6980 - val_accuracy: 0.4192\n",
      "Epoch 22/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6942 - accuracy: 0.5155 - val_loss: 0.6969 - val_accuracy: 0.4192\n",
      "Epoch 23/100\n",
      "873/873 [==============================] - 0s 200us/step - loss: 0.6923 - accuracy: 0.4937 - val_loss: 0.6936 - val_accuracy: 0.4742\n",
      "Epoch 24/100\n",
      "873/873 [==============================] - 0s 197us/step - loss: 0.6934 - accuracy: 0.4960 - val_loss: 0.6947 - val_accuracy: 0.4364\n",
      "Epoch 25/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6943 - accuracy: 0.4937 - val_loss: 0.6954 - val_accuracy: 0.4330\n",
      "Epoch 26/100\n",
      "873/873 [==============================] - 0s 209us/step - loss: 0.6927 - accuracy: 0.5349 - val_loss: 0.6929 - val_accuracy: 0.5498\n",
      "Epoch 27/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6934 - accuracy: 0.5052 - val_loss: 0.6944 - val_accuracy: 0.4570\n",
      "Epoch 28/100\n",
      "873/873 [==============================] - 0s 200us/step - loss: 0.6943 - accuracy: 0.5017 - val_loss: 0.6939 - val_accuracy: 0.4536\n",
      "Epoch 29/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6924 - accuracy: 0.4948 - val_loss: 0.6985 - val_accuracy: 0.4192\n",
      "Epoch 30/100\n",
      "873/873 [==============================] - 0s 197us/step - loss: 0.6934 - accuracy: 0.4857 - val_loss: 0.6947 - val_accuracy: 0.4330\n",
      "Epoch 31/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6958 - accuracy: 0.4971 - val_loss: 0.6942 - val_accuracy: 0.4536\n",
      "Epoch 32/100\n",
      "873/873 [==============================] - 0s 200us/step - loss: 0.6929 - accuracy: 0.5200 - val_loss: 0.6925 - val_accuracy: 0.5739\n",
      "Epoch 33/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6935 - accuracy: 0.4651 - val_loss: 0.6945 - val_accuracy: 0.4192\n",
      "Epoch 34/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6927 - accuracy: 0.5040 - val_loss: 0.6970 - val_accuracy: 0.4192\n",
      "Epoch 35/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6929 - accuracy: 0.5189 - val_loss: 0.6984 - val_accuracy: 0.4192\n",
      "Epoch 36/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6921 - accuracy: 0.5109 - val_loss: 0.6949 - val_accuracy: 0.4296\n",
      "Epoch 37/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6927 - accuracy: 0.5074 - val_loss: 0.6970 - val_accuracy: 0.4192\n",
      "Epoch 38/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.6937 - val_accuracy: 0.4502\n",
      "Epoch 39/100\n",
      "873/873 [==============================] - 0s 210us/step - loss: 0.6928 - accuracy: 0.5246 - val_loss: 0.6991 - val_accuracy: 0.4192\n",
      "Epoch 40/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6924 - accuracy: 0.5155 - val_loss: 0.6985 - val_accuracy: 0.4261\n",
      "Epoch 41/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6940 - accuracy: 0.4937 - val_loss: 0.6951 - val_accuracy: 0.4502\n",
      "Epoch 42/100\n",
      "873/873 [==============================] - 0s 188us/step - loss: 0.6927 - accuracy: 0.5006 - val_loss: 0.6957 - val_accuracy: 0.4502\n",
      "Epoch 43/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6907 - accuracy: 0.5235 - val_loss: 0.6974 - val_accuracy: 0.4536\n",
      "Epoch 44/100\n",
      "873/873 [==============================] - 0s 198us/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6977 - val_accuracy: 0.4261\n",
      "Epoch 45/100\n",
      "873/873 [==============================] - 0s 186us/step - loss: 0.6923 - accuracy: 0.4994 - val_loss: 0.6949 - val_accuracy: 0.4502\n",
      "Epoch 46/100\n",
      "873/873 [==============================] - 0s 200us/step - loss: 0.6916 - accuracy: 0.5074 - val_loss: 0.6982 - val_accuracy: 0.4192\n",
      "Epoch 47/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6920 - accuracy: 0.5178 - val_loss: 0.6998 - val_accuracy: 0.4192\n",
      "Epoch 48/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6926 - accuracy: 0.5063 - val_loss: 0.6962 - val_accuracy: 0.4192\n",
      "Epoch 49/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6918 - accuracy: 0.5223 - val_loss: 0.6940 - val_accuracy: 0.4536\n",
      "Epoch 50/100\n",
      "873/873 [==============================] - 0s 200us/step - loss: 0.6899 - accuracy: 0.5281 - val_loss: 0.6984 - val_accuracy: 0.4570\n",
      "Epoch 51/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6903 - accuracy: 0.5281 - val_loss: 0.6924 - val_accuracy: 0.5464\n",
      "Epoch 52/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6926 - accuracy: 0.5200 - val_loss: 0.6944 - val_accuracy: 0.4811\n",
      "Epoch 53/100\n",
      "873/873 [==============================] - 0s 207us/step - loss: 0.6935 - accuracy: 0.5143 - val_loss: 0.6983 - val_accuracy: 0.4261\n",
      "Epoch 54/100\n",
      "873/873 [==============================] - 0s 198us/step - loss: 0.6930 - accuracy: 0.5063 - val_loss: 0.6958 - val_accuracy: 0.4536\n",
      "Epoch 55/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6930 - accuracy: 0.5074 - val_loss: 0.6951 - val_accuracy: 0.4089\n",
      "Epoch 56/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6892 - accuracy: 0.5086 - val_loss: 0.6977 - val_accuracy: 0.4158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6920 - accuracy: 0.4948 - val_loss: 0.6966 - val_accuracy: 0.4536\n",
      "Epoch 58/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6906 - accuracy: 0.5143 - val_loss: 0.6955 - val_accuracy: 0.4433\n",
      "Epoch 59/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6902 - accuracy: 0.5178 - val_loss: 0.6948 - val_accuracy: 0.4536\n",
      "Epoch 60/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6923 - accuracy: 0.5074 - val_loss: 0.6966 - val_accuracy: 0.4192\n",
      "Epoch 61/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6912 - accuracy: 0.5372 - val_loss: 0.6958 - val_accuracy: 0.4467\n",
      "Epoch 62/100\n",
      "873/873 [==============================] - 0s 201us/step - loss: 0.6918 - accuracy: 0.5097 - val_loss: 0.6933 - val_accuracy: 0.4914\n",
      "Epoch 63/100\n",
      "873/873 [==============================] - 0s 207us/step - loss: 0.6945 - accuracy: 0.5304 - val_loss: 0.6997 - val_accuracy: 0.4192\n",
      "Epoch 64/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6918 - accuracy: 0.4983 - val_loss: 0.6941 - val_accuracy: 0.4433\n",
      "Epoch 65/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6931 - accuracy: 0.5086 - val_loss: 0.6934 - val_accuracy: 0.4639\n",
      "Epoch 66/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6926 - accuracy: 0.5040 - val_loss: 0.6943 - val_accuracy: 0.4536\n",
      "Epoch 67/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6915 - accuracy: 0.4960 - val_loss: 0.6960 - val_accuracy: 0.4502\n",
      "Epoch 68/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6940 - val_accuracy: 0.4674\n",
      "Epoch 69/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6905 - accuracy: 0.5029 - val_loss: 0.6975 - val_accuracy: 0.4536\n",
      "Epoch 70/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6925 - accuracy: 0.5109 - val_loss: 0.6932 - val_accuracy: 0.4948\n",
      "Epoch 71/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6911 - accuracy: 0.5155 - val_loss: 0.6975 - val_accuracy: 0.4296\n",
      "Epoch 72/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6927 - accuracy: 0.5052 - val_loss: 0.6967 - val_accuracy: 0.4192\n",
      "Epoch 73/100\n",
      "873/873 [==============================] - 0s 196us/step - loss: 0.6920 - accuracy: 0.5086 - val_loss: 0.6931 - val_accuracy: 0.4948\n",
      "Epoch 74/100\n",
      "873/873 [==============================] - 0s 199us/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6955 - val_accuracy: 0.4296\n",
      "Epoch 75/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6900 - accuracy: 0.5132 - val_loss: 0.6968 - val_accuracy: 0.4192\n",
      "Epoch 76/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6963 - val_accuracy: 0.4605\n",
      "Epoch 77/100\n",
      "873/873 [==============================] - 0s 187us/step - loss: 0.6930 - accuracy: 0.5074 - val_loss: 0.6967 - val_accuracy: 0.4261\n",
      "Epoch 78/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6934 - accuracy: 0.4937 - val_loss: 0.6943 - val_accuracy: 0.4639\n",
      "Epoch 79/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6966 - val_accuracy: 0.4192\n",
      "Epoch 80/100\n",
      "873/873 [==============================] - 0s 198us/step - loss: 0.6932 - accuracy: 0.5132 - val_loss: 0.6951 - val_accuracy: 0.4296\n",
      "Epoch 81/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6901 - accuracy: 0.5029 - val_loss: 0.6973 - val_accuracy: 0.4296\n",
      "Epoch 82/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6917 - accuracy: 0.5074 - val_loss: 0.6962 - val_accuracy: 0.4192\n",
      "Epoch 83/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6912 - accuracy: 0.5258 - val_loss: 0.6952 - val_accuracy: 0.4536\n",
      "Epoch 84/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6914 - accuracy: 0.5097 - val_loss: 0.6978 - val_accuracy: 0.4708\n",
      "Epoch 85/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6876 - accuracy: 0.5533 - val_loss: 0.6974 - val_accuracy: 0.4639\n",
      "Epoch 86/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6944 - accuracy: 0.5292 - val_loss: 0.6967 - val_accuracy: 0.4570\n",
      "Epoch 87/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6924 - accuracy: 0.4914 - val_loss: 0.6962 - val_accuracy: 0.4536\n",
      "Epoch 88/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6940 - accuracy: 0.4926 - val_loss: 0.6959 - val_accuracy: 0.4674\n",
      "Epoch 89/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6900 - accuracy: 0.4994 - val_loss: 0.6955 - val_accuracy: 0.4605\n",
      "Epoch 90/100\n",
      "873/873 [==============================] - 0s 192us/step - loss: 0.6916 - accuracy: 0.4868 - val_loss: 0.6963 - val_accuracy: 0.4261\n",
      "Epoch 91/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6906 - accuracy: 0.5166 - val_loss: 0.6978 - val_accuracy: 0.4570\n",
      "Epoch 92/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6908 - accuracy: 0.5109 - val_loss: 0.6939 - val_accuracy: 0.4536\n",
      "Epoch 93/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6917 - accuracy: 0.4937 - val_loss: 0.6947 - val_accuracy: 0.4742\n",
      "Epoch 94/100\n",
      "873/873 [==============================] - 0s 195us/step - loss: 0.6899 - accuracy: 0.5143 - val_loss: 0.6970 - val_accuracy: 0.4674\n",
      "Epoch 95/100\n",
      "873/873 [==============================] - 0s 194us/step - loss: 0.6911 - accuracy: 0.5200 - val_loss: 0.6973 - val_accuracy: 0.4639\n",
      "Epoch 96/100\n",
      "873/873 [==============================] - 0s 193us/step - loss: 0.6905 - accuracy: 0.5441 - val_loss: 0.6947 - val_accuracy: 0.4536\n",
      "Epoch 97/100\n",
      "873/873 [==============================] - 0s 188us/step - loss: 0.6903 - accuracy: 0.4926 - val_loss: 0.6959 - val_accuracy: 0.4742\n",
      "Epoch 98/100\n",
      "873/873 [==============================] - 0s 190us/step - loss: 0.6900 - accuracy: 0.5074 - val_loss: 0.6934 - val_accuracy: 0.4811\n",
      "Epoch 99/100\n",
      "873/873 [==============================] - 0s 191us/step - loss: 0.6870 - accuracy: 0.5292 - val_loss: 0.6993 - val_accuracy: 0.4467\n",
      "Epoch 100/100\n",
      "873/873 [==============================] - 0s 189us/step - loss: 0.6871 - accuracy: 0.5178 - val_loss: 0.7083 - val_accuracy: 0.4570\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#and building&training a model once more\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, Y, test_size=0.25)\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(7,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data= (X_test,y_test),\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/291 [==============================] - 0s 65us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7083165655840713, 0.45704466104507446]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It seems, that including more feature yields better results - however in LSTM it yields worse esults\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM AND GRU METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepcopying dataframe, so there would be no need to run everything from the beggining\n",
    "normalized_df1 = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new columns\n",
    "normalized_df1 = normalized_df1[['Open','High','Low','Close','compound_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_df(df):\n",
    "    normalized_df=(df-df.mean())/df.std()\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df2 = copy.deepcopy(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function add label: up or down or stable\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using add label\n",
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.775, -1.768, -1.763, -1.757,  0.371,  0.   ],\n",
       "       [-1.739, -1.743, -1.716, -1.722, -1.005,  1.   ],\n",
       "       [-1.747, -1.73 , -1.728, -1.706, -0.421,  1.   ],\n",
       "       ...,\n",
       "       [ 1.311,  1.293,  1.341,  1.328, -0.012,  1.   ],\n",
       "       [ 1.309,  1.317,  1.355,  1.357, -0.012,  1.   ],\n",
       "       [ 1.317,  1.289,  1.337,  1.334,  1.207,  0.   ]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a generator\n",
    "from keras.utils import to_categorical\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=32, step=5):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][-1]\n",
    "        yield samples, to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30\n",
    "step = 10\n",
    "delay = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training, testing and validation sets\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2498 - val_accuracy: 0.5052\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 356ms/step - loss: 0.2507 - accuracy: 0.4531 - val_loss: 0.2497 - val_accuracy: 0.5052\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 358ms/step - loss: 0.2500 - accuracy: 0.5000 - val_loss: 0.2501 - val_accuracy: 0.5052\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 355ms/step - loss: 0.2532 - accuracy: 0.3906 - val_loss: 0.2500 - val_accuracy: 0.5052\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 356ms/step - loss: 0.2510 - accuracy: 0.3750 - val_loss: 0.2500 - val_accuracy: 0.5052\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 360ms/step - loss: 0.2498 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.5365\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 1s 381ms/step - loss: 0.2505 - accuracy: 0.4531 - val_loss: 0.2501 - val_accuracy: 0.5104\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2503 - accuracy: 0.5000 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 1s 358ms/step - loss: 0.2511 - accuracy: 0.4844 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 0.2496 - accuracy: 0.5625 - val_loss: 0.2504 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2503 - val_accuracy: 0.5156\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.2501 - accuracy: 0.4219 - val_loss: 0.2503 - val_accuracy: 0.5104\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.2512 - accuracy: 0.4375 - val_loss: 0.2501 - val_accuracy: 0.5312\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.2508 - accuracy: 0.4531 - val_loss: 0.2500 - val_accuracy: 0.5312\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2493 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.5521\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.2508 - accuracy: 0.4219 - val_loss: 0.2498 - val_accuracy: 0.5417\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 353ms/step - loss: 0.2516 - accuracy: 0.4062 - val_loss: 0.2499 - val_accuracy: 0.5260\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 356ms/step - loss: 0.2499 - accuracy: 0.5000 - val_loss: 0.2499 - val_accuracy: 0.5156\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 360ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2498 - val_accuracy: 0.5260\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.2498 - accuracy: 0.4844 - val_loss: 0.2498 - val_accuracy: 0.5208\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.2497 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.5312\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.2502 - accuracy: 0.4688 - val_loss: 0.2499 - val_accuracy: 0.5208\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.2496 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.5729\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.2503 - accuracy: 0.5000 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.2489 - accuracy: 0.5938 - val_loss: 0.2498 - val_accuracy: 0.5573\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.2508 - accuracy: 0.4219 - val_loss: 0.2500 - val_accuracy: 0.5469\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.2517 - accuracy: 0.4062 - val_loss: 0.2500 - val_accuracy: 0.5260\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.2500 - accuracy: 0.4688 - val_loss: 0.2501 - val_accuracy: 0.5208\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2493 - accuracy: 0.5625 - val_loss: 0.2501 - val_accuracy: 0.5208\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.2500 - accuracy: 0.4688 - val_loss: 0.2501 - val_accuracy: 0.5156\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.2495 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.5208\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.2503 - accuracy: 0.4688 - val_loss: 0.2502 - val_accuracy: 0.5156\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2488 - accuracy: 0.5781 - val_loss: 0.2501 - val_accuracy: 0.5365\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.2497 - accuracy: 0.5000 - val_loss: 0.2501 - val_accuracy: 0.5260\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.2488 - accuracy: 0.5938 - val_loss: 0.2501 - val_accuracy: 0.5417\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.2509 - accuracy: 0.4375 - val_loss: 0.2502 - val_accuracy: 0.5208\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2518 - accuracy: 0.4062 - val_loss: 0.2502 - val_accuracy: 0.5156\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2500 - accuracy: 0.4844 - val_loss: 0.2502 - val_accuracy: 0.5156\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.2486 - accuracy: 0.5781 - val_loss: 0.2502 - val_accuracy: 0.5208\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.2499 - accuracy: 0.4688 - val_loss: 0.2502 - val_accuracy: 0.5156\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2486 - accuracy: 0.5781 - val_loss: 0.2502 - val_accuracy: 0.5208\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 1s 357ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5156\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2466 - accuracy: 0.5938 - val_loss: 0.2502 - val_accuracy: 0.5156\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2491 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.5104\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 1s 354ms/step - loss: 0.2483 - accuracy: 0.5781 - val_loss: 0.2502 - val_accuracy: 0.5208\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.2508 - accuracy: 0.4219 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.2530 - accuracy: 0.4062 - val_loss: 0.2502 - val_accuracy: 0.4948\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.2507 - accuracy: 0.4531 - val_loss: 0.2503 - val_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2485 - accuracy: 0.5938 - val_loss: 0.2502 - val_accuracy: 0.4948\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.4948\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "#creating LSTM model and training it\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=100,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5104166865348816\n",
      "test_loss: 0.24977262318134308\n"
     ]
    }
   ],
   "source": [
    "#evaluating our model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2551 - accuracy: 0.3594 - val_loss: 0.2639 - val_accuracy: 0.5052\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2496 - accuracy: 0.5000 - val_loss: 0.2605 - val_accuracy: 0.5052\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.2514 - accuracy: 0.5000 - val_loss: 0.2639 - val_accuracy: 0.5052\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2638 - val_accuracy: 0.5052\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2494 - accuracy: 0.5469 - val_loss: 0.2659 - val_accuracy: 0.5052\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2655 - val_accuracy: 0.5052\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 0.2538 - accuracy: 0.4531 - val_loss: 0.2653 - val_accuracy: 0.5052\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 0.2393 - accuracy: 0.5938 - val_loss: 0.2681 - val_accuracy: 0.5052\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.2509 - accuracy: 0.4688 - val_loss: 0.2699 - val_accuracy: 0.5052\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 0.2457 - accuracy: 0.5312 - val_loss: 0.2753 - val_accuracy: 0.5052\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - 1s 501ms/step - loss: 0.2496 - accuracy: 0.4375 - val_loss: 0.2781 - val_accuracy: 0.5052\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2500 - accuracy: 0.5469 - val_loss: 0.2764 - val_accuracy: 0.5052\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2404 - accuracy: 0.5000 - val_loss: 0.2763 - val_accuracy: 0.5052\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2457 - accuracy: 0.5938 - val_loss: 0.2762 - val_accuracy: 0.5052\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2489 - accuracy: 0.4844 - val_loss: 0.2754 - val_accuracy: 0.5052\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2533 - accuracy: 0.4375 - val_loss: 0.2767 - val_accuracy: 0.5052\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.2593 - accuracy: 0.5312 - val_loss: 0.2776 - val_accuracy: 0.5052\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2442 - accuracy: 0.5781 - val_loss: 0.2760 - val_accuracy: 0.5052\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2532 - accuracy: 0.4844 - val_loss: 0.2789 - val_accuracy: 0.5052\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2473 - accuracy: 0.5781 - val_loss: 0.2837 - val_accuracy: 0.5052\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2574 - accuracy: 0.4062 - val_loss: 0.2838 - val_accuracy: 0.5052\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.2561 - accuracy: 0.4688 - val_loss: 0.2824 - val_accuracy: 0.5052\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2467 - accuracy: 0.5938 - val_loss: 0.2809 - val_accuracy: 0.5052\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - 1s 381ms/step - loss: 0.2508 - accuracy: 0.5469 - val_loss: 0.2813 - val_accuracy: 0.5052\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2453 - accuracy: 0.6875 - val_loss: 0.2823 - val_accuracy: 0.5052\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2433 - accuracy: 0.6094 - val_loss: 0.2862 - val_accuracy: 0.5052\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.2425 - accuracy: 0.5781 - val_loss: 0.2868 - val_accuracy: 0.5052\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2419 - accuracy: 0.6406 - val_loss: 0.2896 - val_accuracy: 0.5052\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2488 - accuracy: 0.5312 - val_loss: 0.2929 - val_accuracy: 0.5052\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.2504 - accuracy: 0.5312 - val_loss: 0.2948 - val_accuracy: 0.5052\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2539 - accuracy: 0.4844 - val_loss: 0.2999 - val_accuracy: 0.5052\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2504 - accuracy: 0.5156 - val_loss: 0.2958 - val_accuracy: 0.5052\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2466 - accuracy: 0.5625 - val_loss: 0.2958 - val_accuracy: 0.5052\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2464 - accuracy: 0.5312 - val_loss: 0.2970 - val_accuracy: 0.5052\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2410 - accuracy: 0.6719 - val_loss: 0.2984 - val_accuracy: 0.5052\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2473 - accuracy: 0.5469 - val_loss: 0.3008 - val_accuracy: 0.5052\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.3004 - val_accuracy: 0.5052\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2334 - accuracy: 0.6875 - val_loss: 0.3023 - val_accuracy: 0.5052\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2552 - accuracy: 0.4531 - val_loss: 0.3032 - val_accuracy: 0.5052\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2453 - accuracy: 0.5156 - val_loss: 0.3032 - val_accuracy: 0.5052\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2550 - accuracy: 0.4531 - val_loss: 0.3042 - val_accuracy: 0.5052\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2620 - accuracy: 0.4375 - val_loss: 0.3038 - val_accuracy: 0.5052\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2430 - accuracy: 0.6406 - val_loss: 0.3030 - val_accuracy: 0.5052\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.2525 - accuracy: 0.5312 - val_loss: 0.3040 - val_accuracy: 0.5052\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 0.2480 - accuracy: 0.5625 - val_loss: 0.3029 - val_accuracy: 0.5052\n",
      "Epoch 46/250\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2539 - accuracy: 0.4844 - val_loss: 0.3013 - val_accuracy: 0.5052\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2490 - accuracy: 0.5625 - val_loss: 0.3012 - val_accuracy: 0.5052\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2443 - accuracy: 0.5938 - val_loss: 0.3013 - val_accuracy: 0.5052\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2504 - accuracy: 0.5156 - val_loss: 0.3035 - val_accuracy: 0.5052\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2421 - accuracy: 0.6094 - val_loss: 0.3052 - val_accuracy: 0.5052\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2530 - accuracy: 0.4688 - val_loss: 0.3071 - val_accuracy: 0.5052\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2591 - accuracy: 0.4688 - val_loss: 0.3051 - val_accuracy: 0.5052\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "#building and training GRU model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.1))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=250,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.46875\n",
      "test_loss: 0.25039148330688477\n"
     ]
    }
   ],
   "source": [
    "#evaluating GRU model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "normalized_df1 = normalized_df2\n",
    "\n",
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std\n",
    "\n",
    "#adding label: up/down or steady\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying function \n",
    "del normalized_df1['compound_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.775432</td>\n",
       "      <td>-1.767936</td>\n",
       "      <td>-1.763157</td>\n",
       "      <td>-1.757065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.738841</td>\n",
       "      <td>-1.743148</td>\n",
       "      <td>-1.716146</td>\n",
       "      <td>-1.722247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.747349</td>\n",
       "      <td>-1.730437</td>\n",
       "      <td>-1.727754</td>\n",
       "      <td>-1.706183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.728158</td>\n",
       "      <td>-1.731899</td>\n",
       "      <td>-1.723755</td>\n",
       "      <td>-1.742728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.746134</td>\n",
       "      <td>-1.745182</td>\n",
       "      <td>-1.750840</td>\n",
       "      <td>-1.744328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>1.183113</td>\n",
       "      <td>1.180657</td>\n",
       "      <td>1.228761</td>\n",
       "      <td>1.215906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1.221623</td>\n",
       "      <td>1.238496</td>\n",
       "      <td>1.258554</td>\n",
       "      <td>1.274149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>1.310733</td>\n",
       "      <td>1.292774</td>\n",
       "      <td>1.341161</td>\n",
       "      <td>1.327910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>1.309390</td>\n",
       "      <td>1.317244</td>\n",
       "      <td>1.354703</td>\n",
       "      <td>1.356583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>1.316555</td>\n",
       "      <td>1.289215</td>\n",
       "      <td>1.337098</td>\n",
       "      <td>1.334183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1165 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low     Close  Label\n",
       "0    -1.775432 -1.767936 -1.763157 -1.757065      0\n",
       "1    -1.738841 -1.743148 -1.716146 -1.722247      1\n",
       "2    -1.747349 -1.730437 -1.727754 -1.706183      1\n",
       "3    -1.728158 -1.731899 -1.723755 -1.742728      0\n",
       "4    -1.746134 -1.745182 -1.750840 -1.744328      0\n",
       "...        ...       ...       ...       ...    ...\n",
       "1160  1.183113  1.180657  1.228761  1.215906      1\n",
       "1161  1.221623  1.238496  1.258554  1.274149      1\n",
       "1162  1.310733  1.292774  1.341161  1.327910      1\n",
       "1163  1.309390  1.317244  1.354703  1.356583      1\n",
       "1164  1.316555  1.289215  1.337098  1.334183      0\n",
       "\n",
       "[1165 rows x 5 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train, test, validation set\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2500 - accuracy: 0.4844 - val_loss: 0.2500 - val_accuracy: 0.4948\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 0.2459 - accuracy: 0.6875 - val_loss: 0.2496 - val_accuracy: 0.4948\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.2539 - accuracy: 0.4375 - val_loss: 0.2498 - val_accuracy: 0.4948\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2498 - val_accuracy: 0.4948\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 1s 364ms/step - loss: 0.2529 - accuracy: 0.4062 - val_loss: 0.2501 - val_accuracy: 0.5260\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.2532 - accuracy: 0.4375 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 0.2495 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2503 - val_accuracy: 0.5104\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.2504 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5104\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.2398 - accuracy: 0.7031 - val_loss: 0.2500 - val_accuracy: 0.5469\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.2537 - accuracy: 0.4375 - val_loss: 0.2501 - val_accuracy: 0.5208\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2501 - val_accuracy: 0.5312\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 1s 491ms/step - loss: 0.2529 - accuracy: 0.4062 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 2s 786ms/step - loss: 0.2546 - accuracy: 0.4375 - val_loss: 0.2504 - val_accuracy: 0.5052\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 1s 573ms/step - loss: 0.2503 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 0.2498 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 1s 485ms/step - loss: 0.2495 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2493 - accuracy: 0.5625 - val_loss: 0.2504 - val_accuracy: 0.5052\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2489 - accuracy: 0.5156 - val_loss: 0.2504 - val_accuracy: 0.5052\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2378 - accuracy: 0.7031 - val_loss: 0.2501 - val_accuracy: 0.5052\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.2536 - accuracy: 0.4375 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.2531 - accuracy: 0.4219 - val_loss: 0.2504 - val_accuracy: 0.5052\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 1s 492ms/step - loss: 0.2534 - accuracy: 0.4375 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 2s 847ms/step - loss: 0.2508 - accuracy: 0.5156 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 0.2507 - accuracy: 0.5156 - val_loss: 0.2505 - val_accuracy: 0.5052\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 0.2503 - accuracy: 0.5312 - val_loss: 0.2505 - val_accuracy: 0.5052\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2492 - accuracy: 0.5469 - val_loss: 0.2505 - val_accuracy: 0.5052\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2488 - accuracy: 0.5156 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2385 - accuracy: 0.7031 - val_loss: 0.2503 - val_accuracy: 0.5052\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2524 - accuracy: 0.4375 - val_loss: 0.2504 - val_accuracy: 0.5052\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2495 - accuracy: 0.5469 - val_loss: 0.2505 - val_accuracy: 0.5052\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2538 - accuracy: 0.4219 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2557 - accuracy: 0.4375 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2508 - val_accuracy: 0.5052\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.2506 - accuracy: 0.5000 - val_loss: 0.2508 - val_accuracy: 0.5052\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.2496 - accuracy: 0.5625 - val_loss: 0.2508 - val_accuracy: 0.5052\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2508 - val_accuracy: 0.5052\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2386 - accuracy: 0.7031 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 0.2527 - accuracy: 0.4375 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2488 - accuracy: 0.5781 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 1s 382ms/step - loss: 0.2521 - accuracy: 0.4844 - val_loss: 0.2510 - val_accuracy: 0.5052\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 0.2542 - accuracy: 0.4375 - val_loss: 0.2511 - val_accuracy: 0.5052\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 0.2490 - accuracy: 0.5312 - val_loss: 0.2512 - val_accuracy: 0.5052\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 0.2495 - accuracy: 0.5312 - val_loss: 0.2512 - val_accuracy: 0.5052\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 1s 378ms/step - loss: 0.2498 - accuracy: 0.4844 - val_loss: 0.2512 - val_accuracy: 0.5052\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 1s 381ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2511 - val_accuracy: 0.5052\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 0.2493 - accuracy: 0.5156 - val_loss: 0.2513 - val_accuracy: 0.5052\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.2355 - accuracy: 0.7031 - val_loss: 0.2510 - val_accuracy: 0.5052\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "#building and training LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=200,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.53125\n",
      "test_loss: 0.24936044216156006\n"
     ]
    }
   ],
   "source": [
    "#evaluating LSTM model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.2525 - accuracy: 0.4531 - val_loss: 0.2551 - val_accuracy: 0.4896\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.2497 - accuracy: 0.5469 - val_loss: 0.2547 - val_accuracy: 0.4896\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.2490 - accuracy: 0.5781 - val_loss: 0.2512 - val_accuracy: 0.5573\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2596 - accuracy: 0.4375 - val_loss: 0.2502 - val_accuracy: 0.5833\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2444 - accuracy: 0.4688 - val_loss: 0.2488 - val_accuracy: 0.5833\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2476 - accuracy: 0.5781 - val_loss: 0.2490 - val_accuracy: 0.5781\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2545 - accuracy: 0.5000 - val_loss: 0.2464 - val_accuracy: 0.5260\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2583 - accuracy: 0.5156 - val_loss: 0.2471 - val_accuracy: 0.5625\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2529 - accuracy: 0.5156 - val_loss: 0.2480 - val_accuracy: 0.5521\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2504 - accuracy: 0.5469 - val_loss: 0.2472 - val_accuracy: 0.5521\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.2514 - accuracy: 0.5469 - val_loss: 0.2479 - val_accuracy: 0.5677\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2468 - accuracy: 0.5312 - val_loss: 0.2478 - val_accuracy: 0.5677\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2531 - accuracy: 0.4531 - val_loss: 0.2475 - val_accuracy: 0.5573\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2533 - accuracy: 0.3750 - val_loss: 0.2473 - val_accuracy: 0.5469\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2485 - accuracy: 0.5156 - val_loss: 0.2470 - val_accuracy: 0.5469\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2451 - accuracy: 0.6094 - val_loss: 0.2470 - val_accuracy: 0.5573\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2531 - accuracy: 0.4844 - val_loss: 0.2459 - val_accuracy: 0.5104\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2578 - accuracy: 0.4688 - val_loss: 0.2464 - val_accuracy: 0.5104\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2492 - accuracy: 0.5156 - val_loss: 0.2468 - val_accuracy: 0.5104\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.2493 - accuracy: 0.5938 - val_loss: 0.2465 - val_accuracy: 0.5104\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.2539 - accuracy: 0.4062 - val_loss: 0.2465 - val_accuracy: 0.5208\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2502 - accuracy: 0.4844 - val_loss: 0.2466 - val_accuracy: 0.5156\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.2441 - accuracy: 0.5469 - val_loss: 0.2463 - val_accuracy: 0.5052\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2462 - val_accuracy: 0.5104\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2535 - accuracy: 0.3906 - val_loss: 0.2455 - val_accuracy: 0.5052\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2470 - accuracy: 0.5312 - val_loss: 0.2457 - val_accuracy: 0.5156\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2520 - accuracy: 0.3750 - val_loss: 0.2446 - val_accuracy: 0.5052\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2578 - accuracy: 0.4375 - val_loss: 0.2450 - val_accuracy: 0.5052\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2607 - accuracy: 0.3750 - val_loss: 0.2458 - val_accuracy: 0.5052\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2441 - accuracy: 0.6094 - val_loss: 0.2453 - val_accuracy: 0.5052\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2533 - accuracy: 0.4844 - val_loss: 0.2457 - val_accuracy: 0.5052\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 0.2536 - accuracy: 0.4375 - val_loss: 0.2458 - val_accuracy: 0.5104\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2534 - accuracy: 0.4688 - val_loss: 0.2450 - val_accuracy: 0.5052\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.2454 - accuracy: 0.5781 - val_loss: 0.2449 - val_accuracy: 0.5052\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2446 - val_accuracy: 0.5052\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.2475 - accuracy: 0.5469 - val_loss: 0.2448 - val_accuracy: 0.5052\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2517 - accuracy: 0.4531 - val_loss: 0.2433 - val_accuracy: 0.5052\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2555 - accuracy: 0.4219 - val_loss: 0.2439 - val_accuracy: 0.5052\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.2554 - accuracy: 0.5156 - val_loss: 0.2441 - val_accuracy: 0.5052\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2484 - accuracy: 0.5938 - val_loss: 0.2437 - val_accuracy: 0.5052\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2512 - accuracy: 0.4844 - val_loss: 0.2438 - val_accuracy: 0.5052\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2488 - accuracy: 0.5469 - val_loss: 0.2437 - val_accuracy: 0.5052\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2533 - accuracy: 0.5000 - val_loss: 0.2435 - val_accuracy: 0.5052\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2483 - accuracy: 0.4688 - val_loss: 0.2431 - val_accuracy: 0.5052\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2448 - accuracy: 0.5156 - val_loss: 0.2426 - val_accuracy: 0.5052\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2503 - accuracy: 0.5000 - val_loss: 0.2428 - val_accuracy: 0.5052\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2493 - accuracy: 0.4844 - val_loss: 0.2420 - val_accuracy: 0.5052\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2567 - accuracy: 0.3594 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2511 - accuracy: 0.5156 - val_loss: 0.2421 - val_accuracy: 0.5052\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2421 - val_accuracy: 0.5052\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2536 - accuracy: 0.4531 - val_loss: 0.2423 - val_accuracy: 0.5052\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2500 - accuracy: 0.4688 - val_loss: 0.2423 - val_accuracy: 0.5052\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2549 - accuracy: 0.4375 - val_loss: 0.2423 - val_accuracy: 0.5052\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 1s 387ms/step - loss: 0.2500 - accuracy: 0.4688 - val_loss: 0.2424 - val_accuracy: 0.5052\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2522 - accuracy: 0.4219 - val_loss: 0.2420 - val_accuracy: 0.5052\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2475 - accuracy: 0.5938 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 0.2488 - accuracy: 0.5156 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 0.2622 - accuracy: 0.3750 - val_loss: 0.2420 - val_accuracy: 0.5052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2526 - accuracy: 0.5312 - val_loss: 0.2423 - val_accuracy: 0.5052\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 1s 388ms/step - loss: 0.2508 - accuracy: 0.4688 - val_loss: 0.2421 - val_accuracy: 0.5052\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2522 - accuracy: 0.4375 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2502 - accuracy: 0.5312 - val_loss: 0.2423 - val_accuracy: 0.5052\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.2539 - accuracy: 0.4844 - val_loss: 0.2420 - val_accuracy: 0.5052\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2464 - accuracy: 0.5625 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2517 - accuracy: 0.4219 - val_loss: 0.2417 - val_accuracy: 0.5052\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.2498 - accuracy: 0.4531 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2490 - accuracy: 0.5156 - val_loss: 0.2415 - val_accuracy: 0.5052\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.2550 - accuracy: 0.4219 - val_loss: 0.2416 - val_accuracy: 0.5052\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.2536 - accuracy: 0.4375 - val_loss: 0.2416 - val_accuracy: 0.5052\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2482 - accuracy: 0.5156 - val_loss: 0.2415 - val_accuracy: 0.5052\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.2476 - accuracy: 0.5781 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.2545 - accuracy: 0.3438 - val_loss: 0.2415 - val_accuracy: 0.5052\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2484 - accuracy: 0.5312 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.2455 - accuracy: 0.5781 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2429 - accuracy: 0.5156 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.2493 - accuracy: 0.5000 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.2558 - accuracy: 0.4375 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2563 - accuracy: 0.4531 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.2461 - accuracy: 0.5781 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2487 - accuracy: 0.4531 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2505 - accuracy: 0.5312 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2467 - accuracy: 0.5625 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.2437 - accuracy: 0.5781 - val_loss: 0.2412 - val_accuracy: 0.5052\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2424 - accuracy: 0.6094 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.2466 - accuracy: 0.5781 - val_loss: 0.2413 - val_accuracy: 0.5052\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2416 - val_accuracy: 0.5052\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.2642 - accuracy: 0.3438 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.2489 - accuracy: 0.4688 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2433 - accuracy: 0.5625 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.2529 - accuracy: 0.4531 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2501 - accuracy: 0.4531 - val_loss: 0.2414 - val_accuracy: 0.5052\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2467 - accuracy: 0.5312 - val_loss: 0.2417 - val_accuracy: 0.5052\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2472 - accuracy: 0.5938 - val_loss: 0.2416 - val_accuracy: 0.5052\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2467 - accuracy: 0.5469 - val_loss: 0.2417 - val_accuracy: 0.5052\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2509 - accuracy: 0.4844 - val_loss: 0.2417 - val_accuracy: 0.5052\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2477 - accuracy: 0.5000 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2621 - accuracy: 0.3906 - val_loss: 0.2419 - val_accuracy: 0.5052\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2484 - accuracy: 0.5156 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.2494 - accuracy: 0.4688 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.2530 - accuracy: 0.4219 - val_loss: 0.2419 - val_accuracy: 0.5052\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.2520 - accuracy: 0.5156 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2509 - accuracy: 0.5625 - val_loss: 0.2417 - val_accuracy: 0.5052\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.2467 - accuracy: 0.5000 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2420 - val_accuracy: 0.5052\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.2496 - accuracy: 0.4375 - val_loss: 0.2418 - val_accuracy: 0.5052\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2442 - accuracy: 0.5625 - val_loss: 0.2426 - val_accuracy: 0.5052\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2559 - accuracy: 0.4219 - val_loss: 0.2425 - val_accuracy: 0.5052\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.2564 - accuracy: 0.4688 - val_loss: 0.2424 - val_accuracy: 0.5052\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2445 - accuracy: 0.5312 - val_loss: 0.2424 - val_accuracy: 0.5052\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.2479 - accuracy: 0.5781 - val_loss: 0.2424 - val_accuracy: 0.5052\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.2531 - accuracy: 0.4531 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2439 - accuracy: 0.6094 - val_loss: 0.2422 - val_accuracy: 0.5052\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.2428 - accuracy: 0.6250 - val_loss: 0.2425 - val_accuracy: 0.5052\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2465 - accuracy: 0.5000 - val_loss: 0.2431 - val_accuracy: 0.5052\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.2519 - accuracy: 0.4219 - val_loss: 0.2427 - val_accuracy: 0.5052\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.2459 - accuracy: 0.5938 - val_loss: 0.2448 - val_accuracy: 0.5052\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 1s 481ms/step - loss: 0.2583 - accuracy: 0.4531 - val_loss: 0.2444 - val_accuracy: 0.5052\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.2524 - accuracy: 0.5156 - val_loss: 0.2441 - val_accuracy: 0.5052\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 1s 526ms/step - loss: 0.2426 - accuracy: 0.6250 - val_loss: 0.2444 - val_accuracy: 0.5052\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.2517 - accuracy: 0.5156 - val_loss: 0.2440 - val_accuracy: 0.5052\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.2489 - accuracy: 0.5469 - val_loss: 0.2440 - val_accuracy: 0.5052\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2503 - accuracy: 0.5312 - val_loss: 0.2443 - val_accuracy: 0.5052\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2427 - accuracy: 0.6094 - val_loss: 0.2446 - val_accuracy: 0.5052\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.2442 - accuracy: 0.5938 - val_loss: 0.2450 - val_accuracy: 0.5052\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2541 - accuracy: 0.3750 - val_loss: 0.2447 - val_accuracy: 0.5052\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2482 - accuracy: 0.5625 - val_loss: 0.2452 - val_accuracy: 0.5052\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.2579 - accuracy: 0.4531 - val_loss: 0.2444 - val_accuracy: 0.5052\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2541 - accuracy: 0.5000 - val_loss: 0.2440 - val_accuracy: 0.5052\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2503 - accuracy: 0.5625 - val_loss: 0.2438 - val_accuracy: 0.5052\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2453 - accuracy: 0.6094 - val_loss: 0.2439 - val_accuracy: 0.5052\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2493 - accuracy: 0.5781 - val_loss: 0.2439 - val_accuracy: 0.5052\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2443 - val_accuracy: 0.5052\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2459 - accuracy: 0.5938 - val_loss: 0.2448 - val_accuracy: 0.5052\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2387 - accuracy: 0.6250 - val_loss: 0.2462 - val_accuracy: 0.5052\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2539 - accuracy: 0.4219 - val_loss: 0.2455 - val_accuracy: 0.5052\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.2500 - accuracy: 0.5781 - val_loss: 0.2469 - val_accuracy: 0.5052\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.2593 - accuracy: 0.4531 - val_loss: 0.2465 - val_accuracy: 0.5052\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.2557 - accuracy: 0.4219 - val_loss: 0.2461 - val_accuracy: 0.5052\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.2462 - accuracy: 0.5312 - val_loss: 0.2460 - val_accuracy: 0.5052\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.2556 - accuracy: 0.4688 - val_loss: 0.2456 - val_accuracy: 0.5052\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2524 - accuracy: 0.4219 - val_loss: 0.2460 - val_accuracy: 0.5052\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2501 - accuracy: 0.5156 - val_loss: 0.2464 - val_accuracy: 0.5052\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.2428 - accuracy: 0.5781 - val_loss: 0.2463 - val_accuracy: 0.5052\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.2449 - accuracy: 0.5781 - val_loss: 0.2467 - val_accuracy: 0.5052\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.2538 - accuracy: 0.3438 - val_loss: 0.2461 - val_accuracy: 0.5052\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 0.2469 - accuracy: 0.5469 - val_loss: 0.2480 - val_accuracy: 0.5052\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.2595 - accuracy: 0.4375 - val_loss: 0.2476 - val_accuracy: 0.5052\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2521 - accuracy: 0.4375 - val_loss: 0.2471 - val_accuracy: 0.5052\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2452 - accuracy: 0.5938 - val_loss: 0.2472 - val_accuracy: 0.5052\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.2493 - accuracy: 0.4844 - val_loss: 0.2469 - val_accuracy: 0.5052\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2507 - accuracy: 0.5156 - val_loss: 0.2471 - val_accuracy: 0.5052\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2514 - accuracy: 0.5469 - val_loss: 0.2477 - val_accuracy: 0.5052\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2396 - accuracy: 0.6250 - val_loss: 0.2482 - val_accuracy: 0.5052\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2454 - accuracy: 0.5781 - val_loss: 0.2490 - val_accuracy: 0.5052\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.2519 - accuracy: 0.4688 - val_loss: 0.2483 - val_accuracy: 0.5052\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.2435 - accuracy: 0.6094 - val_loss: 0.2506 - val_accuracy: 0.5052\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 1s 484ms/step - loss: 0.2536 - accuracy: 0.4219 - val_loss: 0.2496 - val_accuracy: 0.5052\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.2571 - accuracy: 0.4531 - val_loss: 0.2486 - val_accuracy: 0.5052\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2514 - accuracy: 0.5625 - val_loss: 0.2498 - val_accuracy: 0.5052\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.2506 - accuracy: 0.4375 - val_loss: 0.2501 - val_accuracy: 0.5052\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.2495 - accuracy: 0.5156 - val_loss: 0.2502 - val_accuracy: 0.5052\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.2474 - accuracy: 0.5469 - val_loss: 0.2516 - val_accuracy: 0.5052\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 0.2411 - accuracy: 0.6406 - val_loss: 0.2526 - val_accuracy: 0.5052\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.2402 - accuracy: 0.6094 - val_loss: 0.2548 - val_accuracy: 0.5052\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.2541 - accuracy: 0.4375 - val_loss: 0.2533 - val_accuracy: 0.5052\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.2491 - accuracy: 0.5469 - val_loss: 0.2527 - val_accuracy: 0.5052\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.2625 - accuracy: 0.3906 - val_loss: 0.2520 - val_accuracy: 0.5052\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.2498 - accuracy: 0.4844 - val_loss: 0.2513 - val_accuracy: 0.5052\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2472 - accuracy: 0.5781 - val_loss: 0.2515 - val_accuracy: 0.5052\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.2546 - accuracy: 0.4375 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2507 - val_accuracy: 0.5052\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2496 - accuracy: 0.5625 - val_loss: 0.2518 - val_accuracy: 0.5052\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.2375 - accuracy: 0.6250 - val_loss: 0.2526 - val_accuracy: 0.5052\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.2381 - accuracy: 0.6406 - val_loss: 0.2542 - val_accuracy: 0.5052\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00175: early stopping\n"
     ]
    }
   ],
   "source": [
    "#building and training GRU model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=500,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5\n",
      "test_loss: 0.252469539642334\n"
     ]
    }
   ],
   "source": [
    "#evaluating GRU model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
